{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WPD-DS-Challenge \u00b6 This repository includes the workflow used by the UCL ESAIL team for submissions to the Western Power Distribition Data Science competition. An example charging profile from our latest submission can be seen below Installation \u00b6 You can install the battery optimisation library using: pip install batopt Challenge Details \u00b6 High-level Overview \u00b6 A 6MWh/2.5MW battery is connected to a primary distribution substation and a 5MW solar farm in Devon, southwest England. Design the control of a storage device to support the distribution network to: Maximise the daily evening peak reduction. Using as much solar photovoltaic energy as possible. This will be done for each day for the week following the current challenge date. In other words it is a constrained optimisation/control problem under uncertainty. There will be four assessed weeks as part of this challenge. A recording of the kick-off meeting can also be found here . Battery Charging \u00b6 The aim of this compoennt is to maximise the proportion of the battery's charge that comes from solar PV. The total battery charge can be written as: where is the power drawn to the battery from solar generation on day , period , and is that drawn from the grid. Whenever the battery is charging, it will draw from solar PV as much as possible, and take the remainder from the grid. We can therefore express that for a period , the battery will draw from PV an amount: The remainder is drawn from the grid: The proportion of energy stored in the battery from solar PV on day is expressed as: An equivalent expression gives the proportion of energy stored in the battery that was drawn from the grid, . The scoring function is proportional to . We therefore want to maximise by coinciding our battery charging with the solar PV generation. Note that the minimum score that can be gained from this component is 1 (entirely charged from grid), and the maximum is 3 (entirely charged from solar PV). Battery Discharging \u00b6 We'll start by defining the cost function for the demand forecasting component of the battery discharge optimisation. For each day ( = 1, \u2026 , 7) the peak percentage reduction is calculated using: Where: * is the average power (in MW) over the half hour of day , where would mean the period from midnight to 00:30 AM on the current day, . * is the average power (in MW) over the half hour of day , to minimise the peak demand over the evening period (the half hours = 32 to 42) Our goal is to maximise the peak percentage reduction from 3.30PM to 9PM. Constraints \u00b6 We also have a number of constraints. The first constraint is on the maximum import and export of energy, in this case: Secondly the battery cannot charge beyond its capacity, , (in MWh): The total charge in the battery at the next time step is related to how much is currently in the battery and how much charged within the battery at time , i.e. Finally, the battery must start empty at the start of each day in the test week. I.e. for . Literature \u00b6 The literature used in this work is being tracked using Zotero within the ESAIL group , please add new papers and comment on existing ones. These should hopefully make it a lot easier down the line if we turn the work into a paper. Environment Set-Up \u00b6 The easiest way to set-up your conda environment is with the setup_env.bat script for Windows. Alternatively you can carry out these manual steps from the terminal: > conda env create -f environment.yml > conda activate batopt > ipython kernel install --user --name = batopt Nb-Dev Design Approach \u00b6 What is Nb-Dev? \u00b6 nbdev is a library that allows you to develop a python library in Jupyter Notebooks, putting all your code, tests and documentation in one place. That is: you now have a true literate programming environment, as envisioned by Donald Knuth back in 1983!\" Why use Nb-Dev? \u00b6 It enables notebooks to be used as the origin of both the documentation and the code-base, improving code-readability and fitting more nicely within the standard data-science workflow. The library also provides a several tools to handle common problems such as merge issues with notebooks. How to use Nb-Dev? \u00b6 Most of the complexity around nbdev is in the initial set-up which has already been carried out for this repository, leaving the main learning curve as the special commands used in notebooks for exporting code. The special commands all have a # prefix and are used at the top of a cell. #default_exp <sub-module-name> - the name of the sub-module that the notebook will be outputted to (put in the first cell) #exports - to export all contents in the cell #hide - to remove the cell from the documentation These just describe what to do with the cells though, we have to run another function to carry out this conversion (which is normally added at the end of each notebook): from nbdev.export import notebook2script notebook2script () Potential Further Research Questions \u00b6 How does the relative importance of the discharge and charge models change seasonally? How much harder is it to optimise when the real-time grid carbon intensity is used?","title":"Home"},{"location":"#wpd-ds-challenge","text":"This repository includes the workflow used by the UCL ESAIL team for submissions to the Western Power Distribition Data Science competition. An example charging profile from our latest submission can be seen below","title":"WPD-DS-Challenge"},{"location":"#installation","text":"You can install the battery optimisation library using: pip install batopt","title":"Installation"},{"location":"#challenge-details","text":"","title":"Challenge Details"},{"location":"#high-level-overview","text":"A 6MWh/2.5MW battery is connected to a primary distribution substation and a 5MW solar farm in Devon, southwest England. Design the control of a storage device to support the distribution network to: Maximise the daily evening peak reduction. Using as much solar photovoltaic energy as possible. This will be done for each day for the week following the current challenge date. In other words it is a constrained optimisation/control problem under uncertainty. There will be four assessed weeks as part of this challenge. A recording of the kick-off meeting can also be found here .","title":"High-level Overview"},{"location":"#battery-charging","text":"The aim of this compoennt is to maximise the proportion of the battery's charge that comes from solar PV. The total battery charge can be written as: where is the power drawn to the battery from solar generation on day , period , and is that drawn from the grid. Whenever the battery is charging, it will draw from solar PV as much as possible, and take the remainder from the grid. We can therefore express that for a period , the battery will draw from PV an amount: The remainder is drawn from the grid: The proportion of energy stored in the battery from solar PV on day is expressed as: An equivalent expression gives the proportion of energy stored in the battery that was drawn from the grid, . The scoring function is proportional to . We therefore want to maximise by coinciding our battery charging with the solar PV generation. Note that the minimum score that can be gained from this component is 1 (entirely charged from grid), and the maximum is 3 (entirely charged from solar PV).","title":"Battery Charging"},{"location":"#battery-discharging","text":"We'll start by defining the cost function for the demand forecasting component of the battery discharge optimisation. For each day ( = 1, \u2026 , 7) the peak percentage reduction is calculated using: Where: * is the average power (in MW) over the half hour of day , where would mean the period from midnight to 00:30 AM on the current day, . * is the average power (in MW) over the half hour of day , to minimise the peak demand over the evening period (the half hours = 32 to 42) Our goal is to maximise the peak percentage reduction from 3.30PM to 9PM.","title":"Battery Discharging"},{"location":"#constraints","text":"We also have a number of constraints. The first constraint is on the maximum import and export of energy, in this case: Secondly the battery cannot charge beyond its capacity, , (in MWh): The total charge in the battery at the next time step is related to how much is currently in the battery and how much charged within the battery at time , i.e. Finally, the battery must start empty at the start of each day in the test week. I.e. for .","title":"Constraints"},{"location":"#literature","text":"The literature used in this work is being tracked using Zotero within the ESAIL group , please add new papers and comment on existing ones. These should hopefully make it a lot easier down the line if we turn the work into a paper.","title":"Literature"},{"location":"#environment-set-up","text":"The easiest way to set-up your conda environment is with the setup_env.bat script for Windows. Alternatively you can carry out these manual steps from the terminal: > conda env create -f environment.yml > conda activate batopt > ipython kernel install --user --name = batopt","title":"Environment Set-Up"},{"location":"#nb-dev-design-approach","text":"","title":"Nb-Dev Design Approach"},{"location":"#what-is-nb-dev","text":"nbdev is a library that allows you to develop a python library in Jupyter Notebooks, putting all your code, tests and documentation in one place. That is: you now have a true literate programming environment, as envisioned by Donald Knuth back in 1983!\"","title":"What is Nb-Dev?"},{"location":"#why-use-nb-dev","text":"It enables notebooks to be used as the origin of both the documentation and the code-base, improving code-readability and fitting more nicely within the standard data-science workflow. The library also provides a several tools to handle common problems such as merge issues with notebooks.","title":"Why use Nb-Dev?"},{"location":"#how-to-use-nb-dev","text":"Most of the complexity around nbdev is in the initial set-up which has already been carried out for this repository, leaving the main learning curve as the special commands used in notebooks for exporting code. The special commands all have a # prefix and are used at the top of a cell. #default_exp <sub-module-name> - the name of the sub-module that the notebook will be outputted to (put in the first cell) #exports - to export all contents in the cell #hide - to remove the cell from the documentation These just describe what to do with the cells though, we have to run another function to carry out this conversion (which is normally added at the end of each notebook): from nbdev.export import notebook2script notebook2script ()","title":"How to use Nb-Dev?"},{"location":"#potential-further-research-questions","text":"How does the relative importance of the discharge and charge models change seasonally? How much harder is it to optimise when the real-time grid carbon intensity is used?","title":"Potential Further Research Questions"},{"location":"00-utilities/","text":"Utilities \u00b6 #exports import json import numpy as np import pandas as pd import matplotlib.pyplot as plt import junix from html.parser import HTMLParser from nbdev.export2html import convert_md from joblib import Parallel , delayed from scipy.stats import rankdata from skopt import BayesSearchCV import os import codecs from ipypb import track from warnings import warn from functools import partial from distutils.dir_util import copy_tree from collections.abc import Iterable , Sized from collections import defaultdict import sklearn from sklearn import linear_model from sklearn.metrics import r2_score from sklearn.ensemble import RandomForestRegressor from sklearn.base import is_classifier , clone from sklearn.utils.validation import indexable try : from sklearn.metrics import check_scoring except ImportError : from sklearn.metrics.scorer import check_scoring User Inputs \u00b6 dev_nbs_dir = '.' docs_dir = '../docs' docs_nb_img_dir = f ' { docs_dir } /img/nbs' nb_img_dir = '../img/nbs' Monkey Patching skopt \u00b6 We'll quickly fix the issue skopt has with the latest sklearn version, first we'll fix the BayesSearchCV initialisation #exports def bayes_search_CV_init ( self , estimator , search_spaces , optimizer_kwargs = None , n_iter = 50 , scoring = None , fit_params = None , n_jobs = 1 , n_points = 1 , iid = True , refit = True , cv = None , verbose = 0 , pre_dispatch = '2*n_jobs' , random_state = None , error_score = 'raise' , return_train_score = False ): self . search_spaces = search_spaces self . n_iter = n_iter self . n_points = n_points self . random_state = random_state self . optimizer_kwargs = optimizer_kwargs self . _check_search_space ( self . search_spaces ) self . fit_params = fit_params self . iid = None super ( BayesSearchCV , self ) . __init__ ( estimator = estimator , scoring = scoring , n_jobs = n_jobs , refit = refit , cv = cv , verbose = verbose , pre_dispatch = pre_dispatch , error_score = error_score , return_train_score = return_train_score ) BayesSearchCV . __init__ = bayes_search_CV_init We'll then fix the BayesSearchCV fit method #exports def bayes_search_CV__fit ( self , X , y , groups , parameter_iterable ): \"\"\" Actual fitting, performing the search over parameters. Taken from https://github.com/scikit-learn/scikit-learn/blob/0.18.X .../sklearn/model_selection/_search.py \"\"\" estimator = self . estimator cv = sklearn . model_selection . _validation . check_cv ( self . cv , y , classifier = is_classifier ( estimator )) self . scorer_ = check_scoring ( self . estimator , scoring = self . scoring ) X , y , groups = indexable ( X , y , groups ) n_splits = cv . get_n_splits ( X , y , groups ) if self . verbose > 0 and isinstance ( parameter_iterable , Sized ): n_candidates = len ( parameter_iterable ) print ( \"Fitting {0} folds for each of {1} candidates, totalling\" \" {2} fits\" . format ( n_splits , n_candidates , n_candidates * n_splits )) base_estimator = clone ( self . estimator ) pre_dispatch = self . pre_dispatch cv_iter = list ( cv . split ( X , y , groups )) out = Parallel ( n_jobs = self . n_jobs , verbose = self . verbose , pre_dispatch = pre_dispatch )( delayed ( sklearn . model_selection . _validation . _fit_and_score )( clone ( base_estimator ), X , y , self . scorer_ , train , test , self . verbose , parameters , fit_params = self . fit_params , return_train_score = self . return_train_score , return_n_test_samples = True , return_times = True , return_parameters = True , error_score = self . error_score ) for parameters in parameter_iterable for train , test in cv_iter ) # if one choose to see train score, \"out\" will contain train score info if self . return_train_score : ( train_scores , test_scores , n_test_samples , fit_time , score_time , parameters ) = zip ( * out ) else : from warnings import warn ( fit_failed , test_scores , n_test_samples , fit_time , score_time , parameters ) = zip ( * [ a . values () for a in out ]) candidate_params = parameters [:: n_splits ] n_candidates = len ( candidate_params ) results = dict () def _store ( key_name , array , weights = None , splits = False , rank = False ): \"\"\"A small helper to store the scores/times to the cv_results_\"\"\" array = np . array ( array , dtype = np . float64 ) . reshape ( n_candidates , n_splits ) if splits : for split_i in range ( n_splits ): results [ \"split %d _ %s \" % ( split_i , key_name )] = array [:, split_i ] array_means = np . average ( array , axis = 1 , weights = weights ) results [ 'mean_ %s ' % key_name ] = array_means # Weighted std is not directly available in numpy array_stds = np . sqrt ( np . average (( array - array_means [:, np . newaxis ]) ** 2 , axis = 1 , weights = weights )) results [ 'std_ %s ' % key_name ] = array_stds if rank : results [ \"rank_ %s \" % key_name ] = np . asarray ( rankdata ( - array_means , method = 'min' ), dtype = np . int32 ) # Computed the (weighted) mean and std for test scores alone # NOTE test_sample counts (weights) remain the same for all candidates n_test_samples n_test_samples = np . array ( n_test_samples [: n_splits ], dtype = np . int ) _store ( 'test_score' , test_scores , splits = True , rank = True , weights = n_test_samples if self . iid else None ) if self . return_train_score : _store ( 'train_score' , train_scores , splits = True ) _store ( 'fit_time' , fit_time ) _store ( 'score_time' , score_time ) best_index = np . flatnonzero ( results [ \"rank_test_score\" ] == 1 )[ 0 ] best_parameters = candidate_params [ best_index ] # Use one MaskedArray and mask all the places where the param is not # applicable for that candidate. Use defaultdict as each candidate may # not contain all the params param_results = defaultdict ( partial ( np . ma . array , np . empty ( n_candidates ,), mask = True , dtype = object )) for cand_i , params in enumerate ( candidate_params ): for name , value in params . items (): # An all masked empty array gets created for the key # `\"param_%s\" % name` at the first occurence of `name`. # Setting the value at an index also unmasks that index param_results [ \"param_ %s \" % name ][ cand_i ] = value results . update ( param_results ) # Store a list of param dicts at est_sample_counts = np.array(n_test_samples[:n_splits], key 'params' results [ 'params' ] = candidate_params self . cv_results_ = results self . best_index_ = best_index self . n_splits_ = n_splits if self . refit : # fit the best estimator using the entire dataset # clone first to work around broken estimators best_estimator = clone ( base_estimator ) . set_params ( ** best_parameters ) if y is not None : best_estimator . fit ( X , y , ** self . fit_params ) else : best_estimator . fit ( X , ** self . fit_params ) self . best_estimator_ = best_estimator return self BayesSearchCV . _fit = bayes_search_CV__fit Custom sklearn Models \u00b6 We require access to the dataframe index in order to evaluate the discharge optimisation predictions accurately (as we need to group predictions by date), however, standard sklearn strips them and returns only numpy arrays. We'll create a custom model that preserves the index. #exports def add_series_index ( idx_arg_pos = 0 ): def decorator ( func ): def decorator_wrapper ( * args , ** kwargs ): input_s = args [ idx_arg_pos ] assert isinstance ( input_s , ( pd . Series , pd . DataFrame )) result = pd . Series ( func ( * args , ** kwargs ), index = input_s . index ) return result return decorator_wrapper return decorator class PandasRandomForestRegressor ( RandomForestRegressor ): def __init__ ( self , n_estimators = 100 , * , criterion = 'mse' , max_depth = None , min_samples_split = 2 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = 'auto' , max_leaf_nodes = None , min_impurity_decrease = 0.0 , min_impurity_split = None , bootstrap = True , oob_score = False , n_jobs = None , random_state = None , verbose = 0 , warm_start = False , ccp_alpha = 0.0 , max_samples = None , score_func = None ): super () . __init__ ( n_estimators = n_estimators , criterion = criterion , max_depth = max_depth , min_samples_split = min_samples_split , min_samples_leaf = min_samples_leaf , min_weight_fraction_leaf = min_weight_fraction_leaf , max_features = max_features , max_leaf_nodes = max_leaf_nodes , min_impurity_decrease = min_impurity_decrease , min_impurity_split = min_impurity_split , bootstrap = bootstrap , oob_score = oob_score , n_jobs = n_jobs , random_state = random_state , verbose = verbose , warm_start = warm_start , ccp_alpha = ccp_alpha , max_samples = max_samples ) if score_func is None : self . score_func = r2_score else : self . score_func = score_func @add_series_index ( 1 ) def predict ( self , X ): pred = super () . predict ( X ) return pred def score ( self , X , y , * args , ** kwargs ): y_pred = self . predict ( X ) score = self . score_func ( y , y_pred , * args , ** kwargs ) return score pandas_RF = PandasRandomForestRegressor () pandas_RF Saving the Development Notebooks \u00b6 We'll first convert the notebooks to markdown #exports def convert_file_to_json ( filepath ): with open ( filepath , 'r' , encoding = 'utf8' ) as f : contents = f . read () f . close () return json . loads ( contents ) junix . exporter . convert_file_to_json = convert_file_to_json def encode_file_as_utf8 ( fp ): with codecs . open ( fp , 'r' ) as file : contents = file . read ( 1048576 ) file . close () if not contents : pass else : with codecs . open ( fp , 'w' , 'utf-8' ) as file : file . write ( contents ) def convert_nbs_to_md ( nbs_dir , docs_nb_img_dir , docs_dir ): nb_files = [ f for f in os . listdir ( nbs_dir ) if f [ - 6 :] == '.ipynb' ] for nb_file in track ( nb_files ): nb_fp = f ' { nbs_dir } / { nb_file } ' junix . export_images ( nb_fp , docs_nb_img_dir ) convert_md ( nb_fp , docs_dir , img_path = f ' { docs_nb_img_dir } /' , jekyll = False ) md_fp = docs_dir + '/' + nb_file . replace ( '.ipynb' , '' ) + '.md' encode_file_as_utf8 ( md_fp ) convert_nbs_to_md ( dev_nbs_dir , docs_nb_img_dir , docs_dir ) We'll then parse the HTML tables into markdown #exports class MyHTMLParser ( HTMLParser ): def __init__ ( self ): super () . __init__ () self . tags = [] def handle_starttag ( self , tag , attrs ): self . tags . append ( self . get_starttag_text ()) def handle_endtag ( self , tag ): self . tags . append ( f \"</ { tag } >\" ) get_substring_idxs = lambda string , substring : [ num for num in range ( len ( string ) - len ( substring ) + 1 ) if string [ num : num + len ( substring )] == substring ] def convert_df_to_md ( df ): idx_col = df . columns [ 0 ] df = df . set_index ( idx_col ) if idx_col == 'Unnamed: 0' : df . index . name = '' table_md = df . to_markdown () return table_md def extract_div_to_md_table ( start_idx , end_idx , table_and_div_tags , file_txt ): n_start_divs_before = table_and_div_tags [: start_idx ] . count ( '<div>' ) n_end_divs_before = table_and_div_tags [: end_idx ] . count ( '</div>' ) div_start_idx = get_substring_idxs ( file_txt , '<div>' )[ n_start_divs_before - 1 ] div_end_idx = get_substring_idxs ( file_txt , '</div>' )[ n_end_divs_before ] div_txt = file_txt [ div_start_idx : div_end_idx ] potential_dfs = pd . read_html ( div_txt ) assert len ( potential_dfs ) == 1 , 'Multiple tables were found when there should be only one' df = potential_dfs [ 0 ] md_table = convert_df_to_md ( df ) return div_txt , md_table def extract_div_to_md_tables ( md_fp ): with open ( md_fp , 'r' ) as f : file_txt = f . read () parser = MyHTMLParser () parser . feed ( file_txt ) table_and_div_tags = [ tag for tag in parser . tags if tag in [ '<div>' , '</div>' , '<table border=\"1\" class=\"dataframe\">' , '</table>' ]] table_start_tag_idxs = [ i for i , tag in enumerate ( table_and_div_tags ) if tag == '<table border=\"1\" class=\"dataframe\">' ] table_end_tag_idxs = [ table_start_tag_idx + table_and_div_tags [ table_start_tag_idx :] . index ( '</table>' ) for table_start_tag_idx in table_start_tag_idxs ] div_to_md_tables = [] for start_idx , end_idx in zip ( table_start_tag_idxs , table_end_tag_idxs ): div_txt , md_table = extract_div_to_md_table ( start_idx , end_idx , table_and_div_tags , file_txt ) div_to_md_tables += [( div_txt , md_table )] return div_to_md_tables def clean_md_file_tables ( md_fp ): div_to_md_tables = extract_div_to_md_tables ( md_fp ) with open ( md_fp , 'r' ) as f : md_file_text = f . read () for div_txt , md_txt in div_to_md_tables : md_file_text = md_file_text . replace ( div_txt , md_txt ) with open ( md_fp , 'w' ) as f : f . write ( md_file_text ) return md_fps = [ f ' { docs_dir } / { f } ' for f in os . listdir ( docs_dir ) if f [ - 3 :] == '.md' if f != '00-utilities.md' ] for md_fp in md_fps : div_to_md_tables = clean_md_file_tables ( md_fp ) And finally change the filepaths for any images in the notebooks #exports def clean_md_file_img_fps ( md_fp ): with open ( md_fp , 'r' ) as f : md_file_text = f . read () md_file_text = md_file_text . replace ( '../docs/img/nbs' , 'img/nbs' ) with open ( md_fp , 'w' ) as f : f . write ( md_file_text ) return for md_fp in md_fps : clean_md_file_img_fps ( md_fp ) Plotting \u00b6 AxTransformer enables conversion from data coordinates to tick locations, set_date_ticks allows custom date ranges to be applied to plots (including a seaborn heatmap) #exports class AxTransformer : def __init__ ( self , datetime_vals = False ): self . datetime_vals = datetime_vals self . lr = linear_model . LinearRegression () return def process_tick_vals ( self , tick_vals ): if not isinstance ( tick_vals , Iterable ) or isinstance ( tick_vals , str ): tick_vals = [ tick_vals ] if self . datetime_vals == True : tick_vals = pd . to_datetime ( tick_vals ) . astype ( int ) . values tick_vals = np . array ( tick_vals ) return tick_vals def fit ( self , ax , axis = 'x' ): axis = getattr ( ax , f 'get_ { axis } axis' )() tick_locs = axis . get_ticklocs () tick_vals = self . process_tick_vals ([ label . _text for label in axis . get_ticklabels ()]) self . lr . fit ( tick_vals . reshape ( - 1 , 1 ), tick_locs ) return def transform ( self , tick_vals ): tick_vals = self . process_tick_vals ( tick_vals ) tick_locs = self . lr . predict ( np . array ( tick_vals ) . reshape ( - 1 , 1 )) return tick_locs def set_date_ticks ( ax , start_date , end_date , axis = 'y' , date_format = '%Y-%m- %d ' , ** date_range_kwargs ): dt_rng = pd . date_range ( start_date , end_date , ** date_range_kwargs ) ax_transformer = AxTransformer ( datetime_vals = True ) ax_transformer . fit ( ax , axis = axis ) getattr ( ax , f 'set_ { axis } ticks' )( ax_transformer . transform ( dt_rng )) getattr ( ax , f 'set_ { axis } ticklabels' )( dt_rng . strftime ( date_format )) ax . tick_params ( axis = axis , which = 'both' , bottom = True , top = False , labelbottom = True ) return ax Finally we'll export the relevant code to our batopt module","title":"Utilities"},{"location":"00-utilities/#utilities","text":"#exports import json import numpy as np import pandas as pd import matplotlib.pyplot as plt import junix from html.parser import HTMLParser from nbdev.export2html import convert_md from joblib import Parallel , delayed from scipy.stats import rankdata from skopt import BayesSearchCV import os import codecs from ipypb import track from warnings import warn from functools import partial from distutils.dir_util import copy_tree from collections.abc import Iterable , Sized from collections import defaultdict import sklearn from sklearn import linear_model from sklearn.metrics import r2_score from sklearn.ensemble import RandomForestRegressor from sklearn.base import is_classifier , clone from sklearn.utils.validation import indexable try : from sklearn.metrics import check_scoring except ImportError : from sklearn.metrics.scorer import check_scoring","title":"Utilities"},{"location":"00-utilities/#user-inputs","text":"dev_nbs_dir = '.' docs_dir = '../docs' docs_nb_img_dir = f ' { docs_dir } /img/nbs' nb_img_dir = '../img/nbs'","title":"User Inputs"},{"location":"00-utilities/#monkey-patching-skopt","text":"We'll quickly fix the issue skopt has with the latest sklearn version, first we'll fix the BayesSearchCV initialisation #exports def bayes_search_CV_init ( self , estimator , search_spaces , optimizer_kwargs = None , n_iter = 50 , scoring = None , fit_params = None , n_jobs = 1 , n_points = 1 , iid = True , refit = True , cv = None , verbose = 0 , pre_dispatch = '2*n_jobs' , random_state = None , error_score = 'raise' , return_train_score = False ): self . search_spaces = search_spaces self . n_iter = n_iter self . n_points = n_points self . random_state = random_state self . optimizer_kwargs = optimizer_kwargs self . _check_search_space ( self . search_spaces ) self . fit_params = fit_params self . iid = None super ( BayesSearchCV , self ) . __init__ ( estimator = estimator , scoring = scoring , n_jobs = n_jobs , refit = refit , cv = cv , verbose = verbose , pre_dispatch = pre_dispatch , error_score = error_score , return_train_score = return_train_score ) BayesSearchCV . __init__ = bayes_search_CV_init We'll then fix the BayesSearchCV fit method #exports def bayes_search_CV__fit ( self , X , y , groups , parameter_iterable ): \"\"\" Actual fitting, performing the search over parameters. Taken from https://github.com/scikit-learn/scikit-learn/blob/0.18.X .../sklearn/model_selection/_search.py \"\"\" estimator = self . estimator cv = sklearn . model_selection . _validation . check_cv ( self . cv , y , classifier = is_classifier ( estimator )) self . scorer_ = check_scoring ( self . estimator , scoring = self . scoring ) X , y , groups = indexable ( X , y , groups ) n_splits = cv . get_n_splits ( X , y , groups ) if self . verbose > 0 and isinstance ( parameter_iterable , Sized ): n_candidates = len ( parameter_iterable ) print ( \"Fitting {0} folds for each of {1} candidates, totalling\" \" {2} fits\" . format ( n_splits , n_candidates , n_candidates * n_splits )) base_estimator = clone ( self . estimator ) pre_dispatch = self . pre_dispatch cv_iter = list ( cv . split ( X , y , groups )) out = Parallel ( n_jobs = self . n_jobs , verbose = self . verbose , pre_dispatch = pre_dispatch )( delayed ( sklearn . model_selection . _validation . _fit_and_score )( clone ( base_estimator ), X , y , self . scorer_ , train , test , self . verbose , parameters , fit_params = self . fit_params , return_train_score = self . return_train_score , return_n_test_samples = True , return_times = True , return_parameters = True , error_score = self . error_score ) for parameters in parameter_iterable for train , test in cv_iter ) # if one choose to see train score, \"out\" will contain train score info if self . return_train_score : ( train_scores , test_scores , n_test_samples , fit_time , score_time , parameters ) = zip ( * out ) else : from warnings import warn ( fit_failed , test_scores , n_test_samples , fit_time , score_time , parameters ) = zip ( * [ a . values () for a in out ]) candidate_params = parameters [:: n_splits ] n_candidates = len ( candidate_params ) results = dict () def _store ( key_name , array , weights = None , splits = False , rank = False ): \"\"\"A small helper to store the scores/times to the cv_results_\"\"\" array = np . array ( array , dtype = np . float64 ) . reshape ( n_candidates , n_splits ) if splits : for split_i in range ( n_splits ): results [ \"split %d _ %s \" % ( split_i , key_name )] = array [:, split_i ] array_means = np . average ( array , axis = 1 , weights = weights ) results [ 'mean_ %s ' % key_name ] = array_means # Weighted std is not directly available in numpy array_stds = np . sqrt ( np . average (( array - array_means [:, np . newaxis ]) ** 2 , axis = 1 , weights = weights )) results [ 'std_ %s ' % key_name ] = array_stds if rank : results [ \"rank_ %s \" % key_name ] = np . asarray ( rankdata ( - array_means , method = 'min' ), dtype = np . int32 ) # Computed the (weighted) mean and std for test scores alone # NOTE test_sample counts (weights) remain the same for all candidates n_test_samples n_test_samples = np . array ( n_test_samples [: n_splits ], dtype = np . int ) _store ( 'test_score' , test_scores , splits = True , rank = True , weights = n_test_samples if self . iid else None ) if self . return_train_score : _store ( 'train_score' , train_scores , splits = True ) _store ( 'fit_time' , fit_time ) _store ( 'score_time' , score_time ) best_index = np . flatnonzero ( results [ \"rank_test_score\" ] == 1 )[ 0 ] best_parameters = candidate_params [ best_index ] # Use one MaskedArray and mask all the places where the param is not # applicable for that candidate. Use defaultdict as each candidate may # not contain all the params param_results = defaultdict ( partial ( np . ma . array , np . empty ( n_candidates ,), mask = True , dtype = object )) for cand_i , params in enumerate ( candidate_params ): for name , value in params . items (): # An all masked empty array gets created for the key # `\"param_%s\" % name` at the first occurence of `name`. # Setting the value at an index also unmasks that index param_results [ \"param_ %s \" % name ][ cand_i ] = value results . update ( param_results ) # Store a list of param dicts at est_sample_counts = np.array(n_test_samples[:n_splits], key 'params' results [ 'params' ] = candidate_params self . cv_results_ = results self . best_index_ = best_index self . n_splits_ = n_splits if self . refit : # fit the best estimator using the entire dataset # clone first to work around broken estimators best_estimator = clone ( base_estimator ) . set_params ( ** best_parameters ) if y is not None : best_estimator . fit ( X , y , ** self . fit_params ) else : best_estimator . fit ( X , ** self . fit_params ) self . best_estimator_ = best_estimator return self BayesSearchCV . _fit = bayes_search_CV__fit","title":"Monkey Patching skopt"},{"location":"00-utilities/#custom-sklearn-models","text":"We require access to the dataframe index in order to evaluate the discharge optimisation predictions accurately (as we need to group predictions by date), however, standard sklearn strips them and returns only numpy arrays. We'll create a custom model that preserves the index. #exports def add_series_index ( idx_arg_pos = 0 ): def decorator ( func ): def decorator_wrapper ( * args , ** kwargs ): input_s = args [ idx_arg_pos ] assert isinstance ( input_s , ( pd . Series , pd . DataFrame )) result = pd . Series ( func ( * args , ** kwargs ), index = input_s . index ) return result return decorator_wrapper return decorator class PandasRandomForestRegressor ( RandomForestRegressor ): def __init__ ( self , n_estimators = 100 , * , criterion = 'mse' , max_depth = None , min_samples_split = 2 , min_samples_leaf = 1 , min_weight_fraction_leaf = 0.0 , max_features = 'auto' , max_leaf_nodes = None , min_impurity_decrease = 0.0 , min_impurity_split = None , bootstrap = True , oob_score = False , n_jobs = None , random_state = None , verbose = 0 , warm_start = False , ccp_alpha = 0.0 , max_samples = None , score_func = None ): super () . __init__ ( n_estimators = n_estimators , criterion = criterion , max_depth = max_depth , min_samples_split = min_samples_split , min_samples_leaf = min_samples_leaf , min_weight_fraction_leaf = min_weight_fraction_leaf , max_features = max_features , max_leaf_nodes = max_leaf_nodes , min_impurity_decrease = min_impurity_decrease , min_impurity_split = min_impurity_split , bootstrap = bootstrap , oob_score = oob_score , n_jobs = n_jobs , random_state = random_state , verbose = verbose , warm_start = warm_start , ccp_alpha = ccp_alpha , max_samples = max_samples ) if score_func is None : self . score_func = r2_score else : self . score_func = score_func @add_series_index ( 1 ) def predict ( self , X ): pred = super () . predict ( X ) return pred def score ( self , X , y , * args , ** kwargs ): y_pred = self . predict ( X ) score = self . score_func ( y , y_pred , * args , ** kwargs ) return score pandas_RF = PandasRandomForestRegressor () pandas_RF","title":"Custom sklearn Models"},{"location":"00-utilities/#saving-the-development-notebooks","text":"We'll first convert the notebooks to markdown #exports def convert_file_to_json ( filepath ): with open ( filepath , 'r' , encoding = 'utf8' ) as f : contents = f . read () f . close () return json . loads ( contents ) junix . exporter . convert_file_to_json = convert_file_to_json def encode_file_as_utf8 ( fp ): with codecs . open ( fp , 'r' ) as file : contents = file . read ( 1048576 ) file . close () if not contents : pass else : with codecs . open ( fp , 'w' , 'utf-8' ) as file : file . write ( contents ) def convert_nbs_to_md ( nbs_dir , docs_nb_img_dir , docs_dir ): nb_files = [ f for f in os . listdir ( nbs_dir ) if f [ - 6 :] == '.ipynb' ] for nb_file in track ( nb_files ): nb_fp = f ' { nbs_dir } / { nb_file } ' junix . export_images ( nb_fp , docs_nb_img_dir ) convert_md ( nb_fp , docs_dir , img_path = f ' { docs_nb_img_dir } /' , jekyll = False ) md_fp = docs_dir + '/' + nb_file . replace ( '.ipynb' , '' ) + '.md' encode_file_as_utf8 ( md_fp ) convert_nbs_to_md ( dev_nbs_dir , docs_nb_img_dir , docs_dir ) We'll then parse the HTML tables into markdown #exports class MyHTMLParser ( HTMLParser ): def __init__ ( self ): super () . __init__ () self . tags = [] def handle_starttag ( self , tag , attrs ): self . tags . append ( self . get_starttag_text ()) def handle_endtag ( self , tag ): self . tags . append ( f \"</ { tag } >\" ) get_substring_idxs = lambda string , substring : [ num for num in range ( len ( string ) - len ( substring ) + 1 ) if string [ num : num + len ( substring )] == substring ] def convert_df_to_md ( df ): idx_col = df . columns [ 0 ] df = df . set_index ( idx_col ) if idx_col == 'Unnamed: 0' : df . index . name = '' table_md = df . to_markdown () return table_md def extract_div_to_md_table ( start_idx , end_idx , table_and_div_tags , file_txt ): n_start_divs_before = table_and_div_tags [: start_idx ] . count ( '<div>' ) n_end_divs_before = table_and_div_tags [: end_idx ] . count ( '</div>' ) div_start_idx = get_substring_idxs ( file_txt , '<div>' )[ n_start_divs_before - 1 ] div_end_idx = get_substring_idxs ( file_txt , '</div>' )[ n_end_divs_before ] div_txt = file_txt [ div_start_idx : div_end_idx ] potential_dfs = pd . read_html ( div_txt ) assert len ( potential_dfs ) == 1 , 'Multiple tables were found when there should be only one' df = potential_dfs [ 0 ] md_table = convert_df_to_md ( df ) return div_txt , md_table def extract_div_to_md_tables ( md_fp ): with open ( md_fp , 'r' ) as f : file_txt = f . read () parser = MyHTMLParser () parser . feed ( file_txt ) table_and_div_tags = [ tag for tag in parser . tags if tag in [ '<div>' , '</div>' , '<table border=\"1\" class=\"dataframe\">' , '</table>' ]] table_start_tag_idxs = [ i for i , tag in enumerate ( table_and_div_tags ) if tag == '<table border=\"1\" class=\"dataframe\">' ] table_end_tag_idxs = [ table_start_tag_idx + table_and_div_tags [ table_start_tag_idx :] . index ( '</table>' ) for table_start_tag_idx in table_start_tag_idxs ] div_to_md_tables = [] for start_idx , end_idx in zip ( table_start_tag_idxs , table_end_tag_idxs ): div_txt , md_table = extract_div_to_md_table ( start_idx , end_idx , table_and_div_tags , file_txt ) div_to_md_tables += [( div_txt , md_table )] return div_to_md_tables def clean_md_file_tables ( md_fp ): div_to_md_tables = extract_div_to_md_tables ( md_fp ) with open ( md_fp , 'r' ) as f : md_file_text = f . read () for div_txt , md_txt in div_to_md_tables : md_file_text = md_file_text . replace ( div_txt , md_txt ) with open ( md_fp , 'w' ) as f : f . write ( md_file_text ) return md_fps = [ f ' { docs_dir } / { f } ' for f in os . listdir ( docs_dir ) if f [ - 3 :] == '.md' if f != '00-utilities.md' ] for md_fp in md_fps : div_to_md_tables = clean_md_file_tables ( md_fp ) And finally change the filepaths for any images in the notebooks #exports def clean_md_file_img_fps ( md_fp ): with open ( md_fp , 'r' ) as f : md_file_text = f . read () md_file_text = md_file_text . replace ( '../docs/img/nbs' , 'img/nbs' ) with open ( md_fp , 'w' ) as f : f . write ( md_file_text ) return for md_fp in md_fps : clean_md_file_img_fps ( md_fp )","title":"Saving the Development Notebooks"},{"location":"00-utilities/#plotting","text":"AxTransformer enables conversion from data coordinates to tick locations, set_date_ticks allows custom date ranges to be applied to plots (including a seaborn heatmap) #exports class AxTransformer : def __init__ ( self , datetime_vals = False ): self . datetime_vals = datetime_vals self . lr = linear_model . LinearRegression () return def process_tick_vals ( self , tick_vals ): if not isinstance ( tick_vals , Iterable ) or isinstance ( tick_vals , str ): tick_vals = [ tick_vals ] if self . datetime_vals == True : tick_vals = pd . to_datetime ( tick_vals ) . astype ( int ) . values tick_vals = np . array ( tick_vals ) return tick_vals def fit ( self , ax , axis = 'x' ): axis = getattr ( ax , f 'get_ { axis } axis' )() tick_locs = axis . get_ticklocs () tick_vals = self . process_tick_vals ([ label . _text for label in axis . get_ticklabels ()]) self . lr . fit ( tick_vals . reshape ( - 1 , 1 ), tick_locs ) return def transform ( self , tick_vals ): tick_vals = self . process_tick_vals ( tick_vals ) tick_locs = self . lr . predict ( np . array ( tick_vals ) . reshape ( - 1 , 1 )) return tick_locs def set_date_ticks ( ax , start_date , end_date , axis = 'y' , date_format = '%Y-%m- %d ' , ** date_range_kwargs ): dt_rng = pd . date_range ( start_date , end_date , ** date_range_kwargs ) ax_transformer = AxTransformer ( datetime_vals = True ) ax_transformer . fit ( ax , axis = axis ) getattr ( ax , f 'set_ { axis } ticks' )( ax_transformer . transform ( dt_rng )) getattr ( ax , f 'set_ { axis } ticklabels' )( dt_rng . strftime ( date_format )) ax . tick_params ( axis = axis , which = 'both' , bottom = True , top = False , labelbottom = True ) return ax Finally we'll export the relevant code to our batopt module","title":"Plotting"},{"location":"01-retrieval/","text":"Data Retrieval \u00b6 #exports import json import numpy as np import pandas as pd import requests import FEAutils as hlp import matplotlib.pyplot as plt from IPython.display import JSON User Inputs \u00b6 raw_data_dir = '../data/raw' intermediate_data_dir = '../data/intermediate' Public Holidays \u00b6 We'll start by retrieving a JSON for public holidays available from www.gov.uk. get_holidays_json = lambda holidays_url = 'https://www.gov.uk/bank-holidays.json' : requests . get ( holidays_url ) . json () holidays_json = get_holidays_json () JSON ( holidays_json ) <IPython.core.display.JSON object> We'll quickly save this data #exports def save_latest_raw_holiday_data ( raw_data_dir , holidays_url = 'https://www.gov.uk/bank-holidays.json' ): holidays_json = get_holidays_json ( holidays_url ) with open ( f ' { raw_data_dir } /holidays.json' , 'w' ) as fp : json . dump ( holidays_json , fp ) return save_latest_raw_holiday_data ( intermediate_data_dir ) We'll then convert it into a dataframe #exports def load_holidays_df ( raw_data_dir ): with open ( f ' { raw_data_dir } /holidays.json' , 'r' ) as fp : holidays_json = json . load ( fp ) df_holidays = pd . DataFrame ( holidays_json [ 'england-and-wales' ][ 'events' ]) df_holidays [ 'date' ] = pd . to_datetime ( df_holidays [ 'date' ]) return df_holidays df_holidays = load_holidays_df ( raw_data_dir ) df_holidays . head () title date notes bunting 0 New Year\u2019s Day 2016-01-01 nan True 1 Good Friday 2016-03-25 nan False 2 Easter Monday 2016-03-28 nan True 3 Early May bank holiday 2016-05-02 nan True 4 Spring bank holiday 2016-05-30 nan True We'll now create a half-hourly time-series where the prescence of a public holiday is given a value of 1 #exports def holidays_df_to_s ( df_holidays ): holidays_dt_range = pd . date_range ( df_holidays [ 'date' ] . min (), df_holidays [ 'date' ] . max (), freq = '30T' , tz = 'UTC' ) s_holidays = pd . Series ( np . isin ( holidays_dt_range . date , df_holidays [ 'date' ] . dt . date ), index = holidays_dt_range ) . astype ( int ) s_holidays . index . name = 'datetime' s_holidays . name = 'holiday' return s_holidays s_holidays = holidays_df_to_s ( df_holidays ) s_holidays . head () datetime 2016-01-01 00:00:00+00:00 1 2016-01-01 00:30:00+00:00 1 2016-01-01 01:00:00+00:00 1 2016-01-01 01:30:00+00:00 1 2016-01-01 02:00:00+00:00 1 Freq: 30T, Name: holiday, dtype: int32 We'll quickly plot the results fig , ax = plt . subplots ( dpi = 150 ) s_holidays [ '2016' ] . plot () hlp . hide_spines ( ax , positions = [ 'top' , 'bottom' , 'left' , 'right' ]) ax . set_yticks ([]) ax . set_ylim ( 0.1 , 0.9 ) (0.1, 0.9) We'll create a wrapper for combining these steps #exports def load_holidays_s ( raw_data_dir ): df_holidays = load_holidays_df ( raw_data_dir ) s_holidays = holidays_df_to_s ( df_holidays ) return s_holidays s_holidays = load_holidays_s ( raw_data_dir ) s_holidays . head () datetime 2016-01-01 00:00:00+00:00 1 2016-01-01 00:30:00+00:00 1 2016-01-01 01:00:00+00:00 1 2016-01-01 01:30:00+00:00 1 2016-01-01 02:00:00+00:00 1 Freq: 30T, Name: holiday, dtype: int32 And also save the data to a csv s_holidays . to_csv ( f ' { intermediate_data_dir } /holidays.csv' ) Finally we'll export the relevant code to our batopt module","title":"Retrieval"},{"location":"01-retrieval/#data-retrieval","text":"#exports import json import numpy as np import pandas as pd import requests import FEAutils as hlp import matplotlib.pyplot as plt from IPython.display import JSON","title":"Data Retrieval"},{"location":"01-retrieval/#user-inputs","text":"raw_data_dir = '../data/raw' intermediate_data_dir = '../data/intermediate'","title":"User Inputs"},{"location":"01-retrieval/#public-holidays","text":"We'll start by retrieving a JSON for public holidays available from www.gov.uk. get_holidays_json = lambda holidays_url = 'https://www.gov.uk/bank-holidays.json' : requests . get ( holidays_url ) . json () holidays_json = get_holidays_json () JSON ( holidays_json ) <IPython.core.display.JSON object> We'll quickly save this data #exports def save_latest_raw_holiday_data ( raw_data_dir , holidays_url = 'https://www.gov.uk/bank-holidays.json' ): holidays_json = get_holidays_json ( holidays_url ) with open ( f ' { raw_data_dir } /holidays.json' , 'w' ) as fp : json . dump ( holidays_json , fp ) return save_latest_raw_holiday_data ( intermediate_data_dir ) We'll then convert it into a dataframe #exports def load_holidays_df ( raw_data_dir ): with open ( f ' { raw_data_dir } /holidays.json' , 'r' ) as fp : holidays_json = json . load ( fp ) df_holidays = pd . DataFrame ( holidays_json [ 'england-and-wales' ][ 'events' ]) df_holidays [ 'date' ] = pd . to_datetime ( df_holidays [ 'date' ]) return df_holidays df_holidays = load_holidays_df ( raw_data_dir ) df_holidays . head () title date notes bunting 0 New Year\u2019s Day 2016-01-01 nan True 1 Good Friday 2016-03-25 nan False 2 Easter Monday 2016-03-28 nan True 3 Early May bank holiday 2016-05-02 nan True 4 Spring bank holiday 2016-05-30 nan True We'll now create a half-hourly time-series where the prescence of a public holiday is given a value of 1 #exports def holidays_df_to_s ( df_holidays ): holidays_dt_range = pd . date_range ( df_holidays [ 'date' ] . min (), df_holidays [ 'date' ] . max (), freq = '30T' , tz = 'UTC' ) s_holidays = pd . Series ( np . isin ( holidays_dt_range . date , df_holidays [ 'date' ] . dt . date ), index = holidays_dt_range ) . astype ( int ) s_holidays . index . name = 'datetime' s_holidays . name = 'holiday' return s_holidays s_holidays = holidays_df_to_s ( df_holidays ) s_holidays . head () datetime 2016-01-01 00:00:00+00:00 1 2016-01-01 00:30:00+00:00 1 2016-01-01 01:00:00+00:00 1 2016-01-01 01:30:00+00:00 1 2016-01-01 02:00:00+00:00 1 Freq: 30T, Name: holiday, dtype: int32 We'll quickly plot the results fig , ax = plt . subplots ( dpi = 150 ) s_holidays [ '2016' ] . plot () hlp . hide_spines ( ax , positions = [ 'top' , 'bottom' , 'left' , 'right' ]) ax . set_yticks ([]) ax . set_ylim ( 0.1 , 0.9 ) (0.1, 0.9) We'll create a wrapper for combining these steps #exports def load_holidays_s ( raw_data_dir ): df_holidays = load_holidays_df ( raw_data_dir ) s_holidays = holidays_df_to_s ( df_holidays ) return s_holidays s_holidays = load_holidays_s ( raw_data_dir ) s_holidays . head () datetime 2016-01-01 00:00:00+00:00 1 2016-01-01 00:30:00+00:00 1 2016-01-01 01:00:00+00:00 1 2016-01-01 01:30:00+00:00 1 2016-01-01 02:00:00+00:00 1 Freq: 30T, Name: holiday, dtype: int32 And also save the data to a csv s_holidays . to_csv ( f ' { intermediate_data_dir } /holidays.csv' ) Finally we'll export the relevant code to our batopt module","title":"Public Holidays"},{"location":"02-cleaning/","text":"Data Cleaning \u00b6 #exports import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.model_selection import KFold from sklearn.metrics import mean_absolute_error , mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor import os import glob from ipypb import track from batopt import utils , retrieval from IPython.display import JSON User Inputs \u00b6 raw_data_dir = '../data/raw' cache_data_dir = '../data/nb-cache' Loading the Raw Data \u00b6 We'll start by loading in the demand data, first we have to determine the latest training set that is available for us to work with #exports def identify_latest_set_num ( data_dir ): set_num = max ([ int ( f . split ( '_set' )[ 1 ] . replace ( '.csv' , '' )) for f in os . listdir ( data_dir ) if 'set' in f ]) return set_num set_num = identify_latest_set_num ( raw_data_dir ) set_num 4 We'll then load in and clean the datetime index of the dataset #exports def reindex_df_dt_idx ( df , freq = '30T' ): full_dt_idx = pd . date_range ( df . index . min (), df . index . max (), freq = freq ) df = df . reindex ( full_dt_idx ) return df def load_training_dataset ( raw_data_dir : str , dataset_name : str = 'demand' , set_num = None , parse_dt_idx : bool = True , dt_idx_freq : str = '30T' ) -> pd . DataFrame : if set_num is None : set_num = identify_latest_set_num ( raw_data_dir ) allowed_datasets = [ 'demand' , 'pv' , 'weather' ] assert dataset_name in allowed_datasets , f \"`dataset_name` must be one of: { ', ' . join ( allowed_datasets ) } - not { dataset_name } \" df = pd . read_csv ( glob . glob ( f ' { raw_data_dir } / { dataset_name } *set { set_num } .csv' )[ 0 ] . replace ( ' \\\\ ' , '/' )) if parse_dt_idx == True : assert 'datetime' in df . columns , 'if `parse_dt_idx` is True then `datetime` must be a column in the dataset' df [ 'datetime' ] = pd . to_datetime ( df [ 'datetime' ], utc = True ) df = df . set_index ( 'datetime' ) . pipe ( reindex_df_dt_idx , freq = dt_idx_freq ) . sort_index ( axis = 1 ) df . index . name = 'datetime' return df df_demand = load_training_dataset ( raw_data_dir , 'demand' ) df_demand . head () ('Unnamed: 0_level_0', 'datetime') ('demand_MW', 'Unnamed: 1_level_1') 2017-11-03 00:00:00+00:00 2.19 2017-11-03 00:30:00+00:00 2.14 2017-11-03 01:00:00+00:00 2.01 2017-11-03 01:30:00+00:00 1.87 2017-11-03 02:00:00+00:00 1.86 Then the pv df_pv = load_training_dataset ( raw_data_dir , 'pv' ) df_pv . head () ('Unnamed: 0_level_0', 'datetime') ('irradiance_Wm-2', 'Unnamed: 1_level_1') ('panel_temp_C', 'Unnamed: 2_level_1') ('pv_power_mw', 'Unnamed: 3_level_1') 2017-11-03 00:00:00+00:00 0 7.05 0 2017-11-03 00:30:00+00:00 0 7.38 0 2017-11-03 01:00:00+00:00 0 7.7 0 2017-11-03 01:30:00+00:00 0 7.48 0 2017-11-03 02:00:00+00:00 0 7.2 0 And finally the weather df_weather = load_training_dataset ( raw_data_dir , 'weather' , dt_idx_freq = 'H' ) df_weather . head ( 3 ) ('Unnamed: 0_level_0', 'datetime') ('solar_location1', 'Unnamed: 1_level_1') ('solar_location2', 'Unnamed: 2_level_1') ('solar_location3', 'Unnamed: 3_level_1') ('solar_location4', 'Unnamed: 4_level_1') ('solar_location5', 'Unnamed: 5_level_1') ('solar_location6', 'Unnamed: 6_level_1') ('temp_location1', 'Unnamed: 7_level_1') ('temp_location2', 'Unnamed: 8_level_1') ('temp_location3', 'Unnamed: 9_level_1') ('temp_location4', 'Unnamed: 10_level_1') ('temp_location5', 'Unnamed: 11_level_1') ('temp_location6', 'Unnamed: 12_level_1') 2015-01-01 00:00:00+00:00 0 0 0 0 0 0 9.75 9.65 8.83 7.58 11.62 11.22 2015-01-01 01:00:00+00:00 0 0 0 0 0 0 9.91 9.76 8.9 7.62 11.65 11.32 2015-01-01 02:00:00+00:00 0 0 0 0 0 0 9.99 9.8 9.1 7.61 11.65 11.3 We'll also create a function that reads all of the datasets in at once and then combines them #exports def combine_training_datasets ( raw_data_dir , set_num = None ): # Loading provided training datasets single_datasets = dict () dataset_names = [ 'demand' , 'pv' , 'weather' ] for dataset_name in dataset_names : single_datasets [ dataset_name ] = load_training_dataset ( raw_data_dir , dataset_name , set_num = set_num ) # Constructing date range min_dt = min ([ df . index . min () for df in single_datasets . values ()]) max_dt = max ([ df . index . max () for df in single_datasets . values ()]) + pd . Timedelta ( minutes = 30 ) dt_rng = pd . date_range ( min_dt , max_dt , freq = '30T' ) # Constructing combined dataframe df_combined = pd . DataFrame ( index = dt_rng , columns = dataset_names ) for dataset_name in dataset_names : df_single_dataset = single_datasets [ dataset_name ] cols_to_be_overwritten = set ( df_combined . columns ) - ( set ( df_combined . columns ) - set ( df_single_dataset . columns )) assert len ( cols_to_be_overwritten ) == 0 , f \"The following columns exist in multiple datasets meaning data would be overwritten: { ', ' . join ( cols_to_be_overwritten ) } \" df_combined [ df_single_dataset . columns ] = df_single_dataset df_combined = df_combined . sort_index () # Adding holiday dates s_holidays = retrieval . load_holidays_s ( raw_data_dir ) s_cropped_holidays = s_holidays [ max ( df_combined . index . min (), s_holidays . index . min ()): min ( df_combined . index . max (), s_holidays . index . max ())] df_combined . loc [ s_cropped_holidays . index , 'holidays' ] = s_cropped_holidays return df_combined df_combined = combine_training_datasets ( raw_data_dir ) df_combined . head ( 3 ) demand pv weather demand_MW irradiance_Wm-2 panel_temp_C pv_power_mw solar_location1 solar_location2 solar_location3 solar_location4 solar_location5 solar_location6 temp_location1 temp_location2 temp_location3 temp_location4 temp_location5 temp_location6 holidays 2015-01-01 00:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.75 9.65 8.83 7.58 11.62 11.22 nan 2015-01-01 00:30:00+00:00 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 2015-01-01 01:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.91 9.76 8.9 7.62 11.65 11.32 nan Identifying Missing Values \u00b6 We'll quickly inspect the datasets and check their coverage over the full date range when aggregated by dataset #exports def identify_df_dt_entries ( df_demand , df_pv , df_weather ): min_dt = min ( df_demand . index . min (), df_pv . index . min (), df_weather . index . min ()) max_dt = max ( df_demand . index . max (), df_pv . index . max (), df_weather . index . max ()) dt_rng = pd . date_range ( min_dt , max_dt , freq = '30T' ) df_nulls = pd . DataFrame ( index = dt_rng ) df_nulls [ 'demand' ] = df_demand . reindex ( dt_rng ) . isnull () . mean ( axis = 1 ) . astype ( int ) df_nulls [ 'pv' ] = df_pv . reindex ( dt_rng ) . isnull () . mean ( axis = 1 ) . astype ( int ) df_nulls [ 'weather' ] = df_weather . reindex ( dt_rng ) . ffill ( limit = 1 ) . isnull () . mean ( axis = 1 ) . astype ( int ) df_entries = 1 - df_nulls return df_entries df_entries = identify_df_dt_entries ( df_demand , df_pv , df_weather ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) sns . heatmap ( df_entries . T , ax = ax , cmap = plt . cm . binary ) utils . set_date_ticks ( ax , df_entries . index . min () . strftime ( '%Y-%m- %d ' ), df_entries . index . max () . strftime ( '%Y-%m- %d ' ), axis = 'x' , freq = 'Qs' , date_format = '%b %y' ) <AxesSubplot:> We'll also determine the null percentage in each individual column df_demand . isnull () . mean () demand_MW 0.0 dtype: float64 We can see that all of the PV data columns are missing some data df_pv . isnull () . mean () irradiance_Wm-2 0.001863 panel_temp_C 0.001991 pv_power_mw 0.000771 dtype: float64 Locations 1 and 2 are also missing some solar data, with 4 missing temperature data df_weather . isnull () . mean () solar_location1 0.001487 solar_location2 0.000992 solar_location3 0.000000 solar_location4 0.000000 solar_location5 0.000000 solar_location6 0.000000 temp_location1 0.000000 temp_location2 0.000000 temp_location3 0.000000 temp_location4 0.000992 temp_location5 0.000000 temp_location6 0.000000 dtype: float64 Handling Missing Values \u00b6 We'll start by interpolating the missing PV data, first checking the number of variables that have null values for each time period s_pv_num_null_vals = df_pv . isnull () . sum ( axis = 1 ) . replace ( 0 , np . nan ) . dropna () . astype ( int ) s_pv_num_null_vals . value_counts () 1 103 3 24 dtype: int64 pv_power_mw and irradiance_Wm-2 have the same average number of null values, there are also no time-periods where only 2 variables have null values - it's therefore likely that power and irradiance always have null periods at the same time which makes it harder to interpolate their values. We'll quickly check this hypothesis: ( df_pv [ 'pv_power_mw' ] . isnull () == df_pv [ 'irradiance_Wm-2' ] . isnull ()) . mean () == 1 False It appears as though the pv_power_mw and irradiance_Wm-2 missing values are a single time-block that coincides with a larger set of missing values within panel_temp_C . df_pv [ df_pv [ 'pv_power_mw' ] . isnull ()] ('Unnamed: 0_level_0', 'datetime') ('irradiance_Wm-2', 'Unnamed: 1_level_1') ('panel_temp_C', 'Unnamed: 2_level_1') ('pv_power_mw', 'Unnamed: 3_level_1') 2018-03-04 07:00:00+00:00 nan nan nan 2018-03-04 07:30:00+00:00 nan nan nan 2018-03-04 08:00:00+00:00 nan nan nan 2018-03-04 08:30:00+00:00 nan nan nan 2018-03-04 09:00:00+00:00 nan nan nan 2018-03-04 09:30:00+00:00 nan nan nan 2018-03-04 10:00:00+00:00 nan nan nan 2018-03-04 10:30:00+00:00 nan nan nan 2018-03-04 11:00:00+00:00 nan nan nan 2018-03-04 11:30:00+00:00 nan nan nan 2018-03-04 12:00:00+00:00 nan nan nan 2018-03-04 12:30:00+00:00 nan nan nan 2018-03-04 13:00:00+00:00 nan nan nan 2018-03-04 13:30:00+00:00 nan nan nan 2018-03-04 14:00:00+00:00 nan nan nan 2018-03-04 15:00:00+00:00 nan nan nan 2018-03-04 15:30:00+00:00 nan nan nan 2018-03-04 16:00:00+00:00 nan nan nan 2018-03-04 16:30:00+00:00 nan nan nan 2018-03-04 17:00:00+00:00 nan nan nan 2019-07-19 14:00:00+00:00 nan nan nan 2019-07-19 14:30:00+00:00 nan nan nan 2019-07-19 15:00:00+00:00 nan nan nan 2019-07-19 15:30:00+00:00 nan nan nan Looking at the panel_temp_C data we can see there are 3 time-blocks where obervations are missing df_pv [ 'panel_temp_C' ] . isnull () . astype ( int ) . plot () <AxesSubplot:xlabel='datetime'> One option might to be replace the missing temperature values with the temperatures observed at the surrounding weather grid locations, we'll start by constructing a dataframe that includes all of the temperature data as well as the average rolling temperature for each weather data location. #exports def construct_df_temp_features ( df_weather , df_pv ): df_weather = df_weather . reindex ( pd . date_range ( df_weather . index . min (), df_weather . index . max (), freq = '30T' )) . ffill ( limit = 1 ) temp_loc_cols = df_weather . columns [ df_weather . columns . str . contains ( 'temp' )] df_temp_features = ( df_weather . copy () [ temp_loc_cols ] . assign ( site_temp = df_pv [ 'panel_temp_C' ]) ) df_temp_features [[ col + '_rolling' for col in temp_loc_cols ]] = df_temp_features . rolling ( 3 ) . mean ()[ temp_loc_cols ] df_temp_features = df_temp_features . sort_index ( axis = 1 ) return df_temp_features df_temp_features = construct_df_temp_features ( df_weather , df_pv ) . dropna () df_temp_features . head () site_temp temp_location1 temp_location1_rolling temp_location2 temp_location2_rolling temp_location3 temp_location3_rolling temp_location4 temp_location4_rolling temp_location5 temp_location5_rolling temp_location6 temp_location6_rolling 2017-11-03 00:00:00+00:00 7.05 8.56 8.62667 9.64 9.66 7.46 7.78667 6.68 6.93333 13.09 13.0233 13.2 13.1133 2017-11-03 00:30:00+00:00 7.38 8.56 8.59333 9.64 9.65 7.46 7.62333 6.68 6.80667 13.09 13.0567 13.2 13.1567 2017-11-03 01:00:00+00:00 7.7 8.69 8.60333 9.71 9.66333 7.14 7.35333 6.27 6.54333 13.21 13.13 13.32 13.24 2017-11-03 01:30:00+00:00 7.48 8.69 8.64667 9.71 9.68667 7.14 7.24667 6.27 6.40667 13.21 13.17 13.32 13.28 2017-11-03 02:00:00+00:00 7.2 8.74 8.70667 9.73 9.71667 6.86 7.04667 5.91 6.15 13.3 13.24 13.36 13.3333 We'll now check the correlation sns . heatmap ( df_temp_features . corr ()) <AxesSubplot:> The correlation drops off quickly when it gets to the site temperature, looking at the full distributions we can see that the site measurements get far higher. This is because the panel is absorbing heat that raises its temperature above that of the surrounding area, again making it more difficult to simply fill in with the nearby temperature measurements. sns . histplot ( df_temp_features [ 'site_temp' ], color = 'C0' , label = 'Panel' ) sns . histplot ( df_temp_features . drop ( 'site_temp' , axis = 1 ) . min ( axis = 1 ), color = 'C1' , label = 'MERRA Min' ) sns . histplot ( df_temp_features . drop ( 'site_temp' , axis = 1 ) . max ( axis = 1 ), color = 'C2' , label = 'MERRA Max' ) plt . legend ( frameon = False ) C:\\Users\\Ayrto\\anaconda3\\envs\\batopt\\lib\\site-packages\\seaborn\\distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) C:\\Users\\Ayrto\\anaconda3\\envs\\batopt\\lib\\site-packages\\seaborn\\distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) C:\\Users\\Ayrto\\anaconda3\\envs\\batopt\\lib\\site-packages\\seaborn\\distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) <matplotlib.legend.Legend at 0x1fa8d979310> # Could use an RF to estimate the panel temp based on the weather grid temps? # Potential features: current average surrounding temp, average surrounding temp over the last 3 hours #exports def split_X_y_data ( df , target_col = 'site_temp' ): df = df . dropna () X_cols = df . drop ( target_col , axis = 1 ) . columns X = df [ X_cols ] . values y = df [ target_col ] . values return X , y def split_X_y_data_with_index ( df , target_col = 'site_temp' ): df = df . dropna () X_cols = df . drop ( target_col , axis = 1 ) . columns X = df [ X_cols ] . values y = df [ target_col ] . values index = df . index return X , y , index X , y = split_X_y_data ( df_temp_features ) X . shape , y . shape ((37024, 12), (37024,)) #exports def generate_kfold_preds ( X , y , model = LinearRegression (), kfold_kwargs = { 'n_splits' : 5 , 'shuffle' : True }, index = None ): kfold = KFold ( ** kfold_kwargs ) df_pred = pd . DataFrame ( columns = [ 'pred' , 'true' ], index = np . arange ( X . shape [ 0 ])) for train_idxs , test_idxs in kfold . split ( X ): X_train , y_train = X [ train_idxs ], y [ train_idxs ] X_test , y_test = X [ test_idxs ], y [ test_idxs ] model . fit ( X_train , y_train ) df_pred . loc [ test_idxs , 'true' ] = y_test df_pred . loc [ test_idxs , 'pred' ] = model . predict ( X_test ) df_pred = df_pred . sort_index () if index is not None : assert len ( index ) == df_pred . shape [ 0 ], 'The passed index must be the same length as X and y' df_pred . index = index return df_pred df_pred = generate_kfold_preds ( X , y ) df_pred . head () pred true 0 4.75986 7.05 1 5.00606 7.38 2 5.40829 7.7 3 5.49439 7.48 4 5.27108 7.2 #exports def evaluate_models ( X , y , models , post_pred_proc_func = None , index = None ): model_scores = dict () for model_name , model in track ( models . items ()): df_pred = generate_kfold_preds ( X , y , model , index = index ) if post_pred_proc_func is not None : df_pred [ 'pred' ] = post_pred_proc_func ( df_pred [ 'pred' ]) model_scores [ model_name ] = { 'mae' : mean_absolute_error ( df_pred [ 'true' ], df_pred [ 'pred' ]), 'rmse' : np . sqrt ( mean_squared_error ( df_pred [ 'true' ], df_pred [ 'pred' ])) } df_model_scores = pd . DataFrame ( model_scores ) df_model_scores . index . name = 'metric' df_model_scores . columns . name = 'model' return df_model_scores models = { 'std_linear' : LinearRegression (), 'random_forest' : RandomForestRegressor (), 'boosted' : GradientBoostingRegressor () } rerun_panel_temp_model = False model_scores_filename = 'panel_temp_interp_model_results.csv' if ( rerun_panel_temp_model == True ) or ( model_scores_filename not in os . listdir ( cache_data_dir )): df_model_scores = evaluate_models ( X , y , models ) df_model_scores . to_csv ( f ' { cache_data_dir } / { model_scores_filename } ' ) else : df_model_scores = pd . read_csv ( f ' { cache_data_dir } / { model_scores_filename } ' , index_col = 'metric' ) df_model_scores ('Unnamed: 0_level_0', 'metric') ('std_linear', 'Unnamed: 1_level_1') ('random_forest', 'Unnamed: 2_level_1') ('boosted', 'Unnamed: 3_level_1') mae 2.81922 1.68451 2.58143 rmse 3.78674 2.69334 3.73415 top_model = df_model_scores . T [ 'rmse' ] . idxmin () df_pred = generate_kfold_preds ( X , y , models [ top_model ]) df_pred . head () pred true 0 6.0645 7.05 1 6.1691 7.38 2 7.5151 7.7 3 7.7232 7.48 4 7.1748 7.2 s_residuals = df_pred . diff ( 1 , axis = 1 ) . dropna ( axis = 1 ) . iloc [:, 0 ] s_residuals . plot ( linewidth = 0.3 ) <AxesSubplot:> plt . scatter ( df_pred [ 'true' ], df_pred [ 'pred' ], s = 1 ) plt . xlabel ( 'Obervation' ) plt . ylabel ( 'Prediction' ) Text(0, 0.5, 'Prediction') #exports def interpolate_missing_panel_temps ( df_pv , df_weather , model = RandomForestRegressor ()): missing_panel_temp_dts = df_pv . index [ df_pv [ 'panel_temp_C' ] . isnull ()] if len ( missing_panel_temp_dts ) == 0 : # i.e. no missing values return df_pv df_temp_features = construct_df_temp_features ( df_weather , df_pv ) missing_dt_X = df_temp_features . loc [ missing_panel_temp_dts ] . drop ( 'site_temp' , axis = 1 ) . values X , y = split_X_y_data ( df_temp_features , 'site_temp' ) model . fit ( X , y ) df_pv . loc [ missing_panel_temp_dts , 'panel_temp_C' ] = model . predict ( missing_dt_X ) assert df_pv [ 'panel_temp_C' ] . isnull () . sum () == 0 , 'There are still null values for the PV panel temperature' return df_pv df_pv = interpolate_missing_panel_temps ( df_pv , df_weather ) df_pv . isnull () . mean () irradiance_Wm-2 0.002016 panel_temp_C 0.000000 pv_power_mw 0.000645 dtype: float64 #exports def construct_df_irradiance_features ( df_weather , df_pv ): df_weather = df_weather . reindex ( pd . date_range ( df_weather . index . min (), df_weather . index . max (), freq = '30T' )) . ffill ( limit = 1 ) temp_loc_cols = df_weather . columns [ df_weather . columns . str . contains ( 'solar' )] df_irradiance_features = ( df_weather . copy () [ temp_loc_cols ] . assign ( site_solar = df_pv [ 'irradiance_Wm-2' ]) . pipe ( lambda df : df . assign ( hour = df . index . hour + df . index . minute / 60 )) ) df_irradiance_features = df_irradiance_features . sort_index ( axis = 1 ) return df_irradiance_features df_irradiance_features = construct_df_irradiance_features ( df_weather , df_pv ) df_irradiance_features . head () hour site_solar solar_location1 solar_location2 solar_location3 solar_location4 solar_location5 solar_location6 2015-01-01 00:00:00+00:00 0 nan 0 0 0 0 0 0 2015-01-01 00:30:00+00:00 0.5 nan 0 0 0 0 0 0 2015-01-01 01:00:00+00:00 1 nan 0 0 0 0 0 0 2015-01-01 01:30:00+00:00 1.5 nan 0 0 0 0 0 0 2015-01-01 02:00:00+00:00 2 nan 0 0 0 0 0 0 models = { 'std_linear' : LinearRegression (), 'random_forest' : RandomForestRegressor (), 'boosted' : GradientBoostingRegressor () } rerun_site_irradiance_model = False model_scores_filename = 'site_irradiance_interp_model_results.csv' X , y = split_X_y_data ( df_irradiance_features , 'site_solar' ) if ( rerun_site_irradiance_model == True ) or ( model_scores_filename not in os . listdir ( cache_data_dir )): df_model_scores = evaluate_models ( X , y , models ) df_model_scores . to_csv ( f ' { cache_data_dir } / { model_scores_filename } ' ) else : df_model_scores = pd . read_csv ( f ' { cache_data_dir } / { model_scores_filename } ' , index_col = 'metric' ) df_model_scores ('Unnamed: 0_level_0', 'metric') ('std_linear', 'Unnamed: 1_level_1') ('random_forest', 'Unnamed: 2_level_1') ('boosted', 'Unnamed: 3_level_1') mae 57.4977 37.5087 49.6956 rmse 110.546 78.8525 99.1964 top_model = df_model_scores . T [ 'rmse' ] . idxmin () df_pred = generate_kfold_preds ( X , y , models [ top_model ]) df_pred . head () pred true 0 0.000657 0 1 0.00044 0 2 0.000239 0 3 0.001181 0 4 0.001369 0 plt . scatter ( df_pred [ 'true' ], df_pred [ 'pred' ], s = 1 ) plt . xlabel ( 'Obervation' ) plt . ylabel ( 'Prediction' ) Text(0, 0.5, 'Prediction') #exports def interpolate_missing_site_irradiance ( df_pv , df_weather , model = RandomForestRegressor ()): missing_site_irradiance_dts = df_pv . index [ df_pv [ 'irradiance_Wm-2' ] . isnull ()] if len ( missing_site_irradiance_dts ) == 0 : # i.e. no missing values return df_pv df_irradiance_features = construct_df_irradiance_features ( df_weather , df_pv ) missing_dt_X = df_irradiance_features . loc [ missing_site_irradiance_dts ] . drop ( 'site_solar' , axis = 1 ) . values X , y = split_X_y_data ( df_irradiance_features , 'site_solar' ) model . fit ( X , y ) df_pv . loc [ missing_site_irradiance_dts , 'irradiance_Wm-2' ] = model . predict ( missing_dt_X ) assert df_pv [ 'irradiance_Wm-2' ] . isnull () . sum () == 0 , 'There are still null values for the solar site irradiance' return df_pv df_pv = interpolate_missing_site_irradiance ( df_pv , df_weather ) df_pv . isnull () . mean () irradiance_Wm-2 0.000000 panel_temp_C 0.000000 pv_power_mw 0.000645 dtype: float64 Now that we have the irradiance and temperature we're ready to start filling in the missing values for power output, again using the same regression interpolation method #exports def construct_df_power_features ( df_pv ): df_power_features = ( df_pv . pipe ( lambda df : df . assign ( hour = df . index . hour + df . index . minute / 60 )) . sort_index ( axis = 1 ) ) return df_power_features df_power_features = construct_df_power_features ( df_pv ) df_power_features . head () ('Unnamed: 0_level_0', 'datetime') ('hour', 'Unnamed: 1_level_1') ('irradiance_Wm-2', 'Unnamed: 2_level_1') ('panel_temp_C', 'Unnamed: 3_level_1') ('pv_power_mw', 'Unnamed: 4_level_1') 2017-11-03 00:00:00+00:00 0 0 7.05 0 2017-11-03 00:30:00+00:00 0.5 0 7.38 0 2017-11-03 01:00:00+00:00 1 0 7.7 0 2017-11-03 01:30:00+00:00 1.5 0 7.48 0 2017-11-03 02:00:00+00:00 2 0 7.2 0 models = { 'std_linear' : LinearRegression (), 'random_forest' : RandomForestRegressor (), 'boosted' : GradientBoostingRegressor () } rerun_site_power_model = False model_scores_filename = 'site_power_interp_model_results.csv' X , y , dates = split_X_y_data_with_index ( df_power_features , 'pv_power_mw' ) if ( rerun_site_power_model == True ) or ( model_scores_filename not in os . listdir ( cache_data_dir )): df_model_scores = evaluate_models ( X , y , models ) df_model_scores . to_csv ( f ' { cache_data_dir } / { model_scores_filename } ' ) else : df_model_scores = pd . read_csv ( f ' { cache_data_dir } / { model_scores_filename } ' , index_col = 'metric' ) df_model_scores ('Unnamed: 0_level_0', 'metric') ('std_linear', 'Unnamed: 1_level_1') ('random_forest', 'Unnamed: 2_level_1') ('boosted', 'Unnamed: 3_level_1') mae 0.061519 0.041122 0.043927 rmse 0.14598 0.135822 0.133212 top_model = df_model_scores . T [ 'rmse' ] . idxmin () df_pred = generate_kfold_preds ( X , y , models [ top_model ]) df_pred . head () pred true 0 -0.000829 0 1 -0.000634 0 2 -0.000178 0 3 0.000163 0 4 -0.001097 0 plt . scatter ( df_pred [ 'true' ], df_pred [ 'pred' ], s = 1 ) plt . xlabel ( 'Obervation' ) plt . ylabel ( 'Prediction' ) Text(0, 0.5, 'Prediction') Anomalous data points in PV data \u00b6 The PV data shows a number of points where the observed value is 0 but the prediction is much higher. First let's try and identify them (setting the tolerance to be lower will capture more values as anomalous). def identify_anomalies_pv ( df_pred , tolerance = 0.1 ): foo = df_pred . copy () foo [ 'difference' ] = foo . pred - foo . true foo = foo [( foo . difference > tolerance ) & ( foo . true == 0 )] return foo . index anomalous_dates = dates [ identify_anomalies_pv ( df_pred )] anomalous_df = df_power_features [ df_power_features . index . isin ( anomalous_dates )] plt . hist ( anomalous_df . hour ) # Check this histogram to eyeball if any unreasonable anomalous values are caught by the tolerance (e.g. late at night) (array([ 3., 9., 24., 22., 34., 22., 30., 12., 9., 3.]), array([ 6. , 7.25, 8.5 , 9.75, 11. , 12.25, 13.5 , 14.75, 16. , 17.25, 18.5 ]), <BarContainer object of 10 artists>) Replace these values in df_power_features . df_power_features_clean = df_power_features . copy () df_power_features_clean . loc [ df_power_features_clean . index . isin ( anomalous_dates ), 'pv_power_mw' ] = np . nan Rerun the previous model fitting and check the pred vs. actual graph. models = { 'std_linear' : LinearRegression (), 'random_forest' : RandomForestRegressor (), 'boosted' : GradientBoostingRegressor () } rerun_site_power_model = False model_scores_filename = 'site_power_interp_clean_model_results.csv' X , y , dates = split_X_y_data_with_index ( df_power_features_clean , 'pv_power_mw' ) if ( rerun_site_power_model == True ) or ( model_scores_filename not in os . listdir ( cache_data_dir )): df_model_scores = evaluate_models ( X , y , models ) df_model_scores . to_csv ( f ' { cache_data_dir } / { model_scores_filename } ' ) else : df_model_scores = pd . read_csv ( f ' { cache_data_dir } / { model_scores_filename } ' , index_col = 'metric' ) top_model = df_model_scores . T [ 'rmse' ] . idxmin () df_pred = generate_kfold_preds ( X , y , models [ top_model ]) plt . scatter ( df_pred [ 'true' ], df_pred [ 'pred' ], s = 1 ) plt . xlabel ( 'Obervation' ) plt . ylabel ( 'Prediction' ) Text(0, 0.5, 'Prediction') The above graph looks to be a cleaner with tolerance at 0.1. It looks like there might still be some which aren't though. Consider lowering the tolerance. #exports def pv_anomalies_to_nan ( df_pv , model = GradientBoostingRegressor (), tolerance = 0.1 ): \"\"\" Run this function to identify places where predicted values for pv_power_mw are much larger than true values and where the true value is 0 (we expect these are anomalies) and make these values nan. \"\"\" df_power_features = construct_df_power_features ( df_pv ) X , y , dates = split_X_y_data_with_index ( df_power_features , 'pv_power_mw' ) df_pred = generate_kfold_preds ( X , y , model ) df_pred [ 'difference' ] = df_pred . pred - df_pred . true df_pred [ 'datetime' ] = dates df_pred = df_pred . set_index ( 'datetime' ) anomalous_idx = df_pred [( df_pred . difference > tolerance ) & ( df_pred . true == 0 )] . index df_pv . loc [ df_pv . index . isin ( anomalous_idx ), 'pv_power_mw' ] = np . nan return df_pv df_pv = pv_anomalies_to_nan ( df_pv ) #exports def interpolate_missing_site_power ( df_pv , model = RandomForestRegressor ()): missing_site_power_dts = df_pv . index [ df_pv [ 'pv_power_mw' ] . isnull ()] if len ( missing_site_power_dts ) == 0 : # i.e. no missing values return df_pv df_power_features = construct_df_power_features ( df_pv ) missing_dt_X = df_power_features . loc [ missing_site_power_dts ] . drop ( 'pv_power_mw' , axis = 1 ) . values X , y = split_X_y_data ( df_power_features , 'pv_power_mw' ) model . fit ( X , y ) df_pv . loc [ missing_site_power_dts , 'pv_power_mw' ] = model . predict ( missing_dt_X ) assert df_pv [ 'pv_power_mw' ] . isnull () . sum () == 0 , 'There are still null values for the solar site power' return df_pv df_pv = interpolate_missing_site_power ( df_pv ) df_pv . isnull () . mean () irradiance_Wm-2 0.0 panel_temp_C 0.0 pv_power_mw 0.0 dtype: float64 #exports def interpolate_missing_weather_solar ( df_pv , df_weather , weather_col = 'solar_location2' , model = RandomForestRegressor ()): missing_weather_solar_dts = df_weather . index [ df_weather [ weather_col ] . isnull ()] if len ( missing_weather_solar_dts ) == 0 : # i.e. no missing values return df_pv df_irradiance_features = construct_df_irradiance_features ( df_weather , df_pv ) . drop ( 'site_solar' , axis = 1 ) missing_dt_X = df_irradiance_features . loc [ missing_weather_solar_dts ] . drop ( weather_col , axis = 1 ) . values X , y = split_X_y_data ( df_irradiance_features , weather_col ) model . fit ( X , y ) df_weather . loc [ missing_weather_solar_dts , weather_col ] = model . predict ( missing_dt_X ) assert df_weather [ weather_col ] . isnull () . sum () == 0 , 'There are still null values for the weather dataset solar observations' return df_weather df_weather = interpolate_missing_weather_solar ( df_pv , df_weather , model = LinearRegression ()) df_weather . isnull () . mean () solar_location1 0.0000 solar_location2 0.0000 solar_location3 0.0000 solar_location4 0.0000 solar_location5 0.0000 solar_location6 0.0000 temp_location1 0.0000 temp_location2 0.0000 temp_location3 0.0000 temp_location4 0.0011 temp_location5 0.0000 temp_location6 0.0000 dtype: float64 Interpolate Missing Temp Data \u00b6 foo = interpolate_missing_temps ( df_weather , 'temp_location4' , model = LinearRegression ()) foo . isnull () . mean () solar_location1 0.0 solar_location2 0.0 solar_location3 0.0 solar_location4 0.0 solar_location5 0.0 solar_location6 0.0 temp_location1 0.0 temp_location2 0.0 temp_location3 0.0 temp_location4 0.0 temp_location5 0.0 temp_location6 0.0 dtype: float64 Finally we'll export the relevant code to our batopt module","title":"Cleaning"},{"location":"02-cleaning/#data-cleaning","text":"#exports import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.model_selection import KFold from sklearn.metrics import mean_absolute_error , mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor import os import glob from ipypb import track from batopt import utils , retrieval from IPython.display import JSON","title":"Data Cleaning"},{"location":"02-cleaning/#user-inputs","text":"raw_data_dir = '../data/raw' cache_data_dir = '../data/nb-cache'","title":"User Inputs"},{"location":"02-cleaning/#loading-the-raw-data","text":"We'll start by loading in the demand data, first we have to determine the latest training set that is available for us to work with #exports def identify_latest_set_num ( data_dir ): set_num = max ([ int ( f . split ( '_set' )[ 1 ] . replace ( '.csv' , '' )) for f in os . listdir ( data_dir ) if 'set' in f ]) return set_num set_num = identify_latest_set_num ( raw_data_dir ) set_num 4 We'll then load in and clean the datetime index of the dataset #exports def reindex_df_dt_idx ( df , freq = '30T' ): full_dt_idx = pd . date_range ( df . index . min (), df . index . max (), freq = freq ) df = df . reindex ( full_dt_idx ) return df def load_training_dataset ( raw_data_dir : str , dataset_name : str = 'demand' , set_num = None , parse_dt_idx : bool = True , dt_idx_freq : str = '30T' ) -> pd . DataFrame : if set_num is None : set_num = identify_latest_set_num ( raw_data_dir ) allowed_datasets = [ 'demand' , 'pv' , 'weather' ] assert dataset_name in allowed_datasets , f \"`dataset_name` must be one of: { ', ' . join ( allowed_datasets ) } - not { dataset_name } \" df = pd . read_csv ( glob . glob ( f ' { raw_data_dir } / { dataset_name } *set { set_num } .csv' )[ 0 ] . replace ( ' \\\\ ' , '/' )) if parse_dt_idx == True : assert 'datetime' in df . columns , 'if `parse_dt_idx` is True then `datetime` must be a column in the dataset' df [ 'datetime' ] = pd . to_datetime ( df [ 'datetime' ], utc = True ) df = df . set_index ( 'datetime' ) . pipe ( reindex_df_dt_idx , freq = dt_idx_freq ) . sort_index ( axis = 1 ) df . index . name = 'datetime' return df df_demand = load_training_dataset ( raw_data_dir , 'demand' ) df_demand . head () ('Unnamed: 0_level_0', 'datetime') ('demand_MW', 'Unnamed: 1_level_1') 2017-11-03 00:00:00+00:00 2.19 2017-11-03 00:30:00+00:00 2.14 2017-11-03 01:00:00+00:00 2.01 2017-11-03 01:30:00+00:00 1.87 2017-11-03 02:00:00+00:00 1.86 Then the pv df_pv = load_training_dataset ( raw_data_dir , 'pv' ) df_pv . head () ('Unnamed: 0_level_0', 'datetime') ('irradiance_Wm-2', 'Unnamed: 1_level_1') ('panel_temp_C', 'Unnamed: 2_level_1') ('pv_power_mw', 'Unnamed: 3_level_1') 2017-11-03 00:00:00+00:00 0 7.05 0 2017-11-03 00:30:00+00:00 0 7.38 0 2017-11-03 01:00:00+00:00 0 7.7 0 2017-11-03 01:30:00+00:00 0 7.48 0 2017-11-03 02:00:00+00:00 0 7.2 0 And finally the weather df_weather = load_training_dataset ( raw_data_dir , 'weather' , dt_idx_freq = 'H' ) df_weather . head ( 3 ) ('Unnamed: 0_level_0', 'datetime') ('solar_location1', 'Unnamed: 1_level_1') ('solar_location2', 'Unnamed: 2_level_1') ('solar_location3', 'Unnamed: 3_level_1') ('solar_location4', 'Unnamed: 4_level_1') ('solar_location5', 'Unnamed: 5_level_1') ('solar_location6', 'Unnamed: 6_level_1') ('temp_location1', 'Unnamed: 7_level_1') ('temp_location2', 'Unnamed: 8_level_1') ('temp_location3', 'Unnamed: 9_level_1') ('temp_location4', 'Unnamed: 10_level_1') ('temp_location5', 'Unnamed: 11_level_1') ('temp_location6', 'Unnamed: 12_level_1') 2015-01-01 00:00:00+00:00 0 0 0 0 0 0 9.75 9.65 8.83 7.58 11.62 11.22 2015-01-01 01:00:00+00:00 0 0 0 0 0 0 9.91 9.76 8.9 7.62 11.65 11.32 2015-01-01 02:00:00+00:00 0 0 0 0 0 0 9.99 9.8 9.1 7.61 11.65 11.3 We'll also create a function that reads all of the datasets in at once and then combines them #exports def combine_training_datasets ( raw_data_dir , set_num = None ): # Loading provided training datasets single_datasets = dict () dataset_names = [ 'demand' , 'pv' , 'weather' ] for dataset_name in dataset_names : single_datasets [ dataset_name ] = load_training_dataset ( raw_data_dir , dataset_name , set_num = set_num ) # Constructing date range min_dt = min ([ df . index . min () for df in single_datasets . values ()]) max_dt = max ([ df . index . max () for df in single_datasets . values ()]) + pd . Timedelta ( minutes = 30 ) dt_rng = pd . date_range ( min_dt , max_dt , freq = '30T' ) # Constructing combined dataframe df_combined = pd . DataFrame ( index = dt_rng , columns = dataset_names ) for dataset_name in dataset_names : df_single_dataset = single_datasets [ dataset_name ] cols_to_be_overwritten = set ( df_combined . columns ) - ( set ( df_combined . columns ) - set ( df_single_dataset . columns )) assert len ( cols_to_be_overwritten ) == 0 , f \"The following columns exist in multiple datasets meaning data would be overwritten: { ', ' . join ( cols_to_be_overwritten ) } \" df_combined [ df_single_dataset . columns ] = df_single_dataset df_combined = df_combined . sort_index () # Adding holiday dates s_holidays = retrieval . load_holidays_s ( raw_data_dir ) s_cropped_holidays = s_holidays [ max ( df_combined . index . min (), s_holidays . index . min ()): min ( df_combined . index . max (), s_holidays . index . max ())] df_combined . loc [ s_cropped_holidays . index , 'holidays' ] = s_cropped_holidays return df_combined df_combined = combine_training_datasets ( raw_data_dir ) df_combined . head ( 3 ) demand pv weather demand_MW irradiance_Wm-2 panel_temp_C pv_power_mw solar_location1 solar_location2 solar_location3 solar_location4 solar_location5 solar_location6 temp_location1 temp_location2 temp_location3 temp_location4 temp_location5 temp_location6 holidays 2015-01-01 00:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.75 9.65 8.83 7.58 11.62 11.22 nan 2015-01-01 00:30:00+00:00 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 2015-01-01 01:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.91 9.76 8.9 7.62 11.65 11.32 nan","title":"Loading the Raw Data"},{"location":"02-cleaning/#identifying-missing-values","text":"We'll quickly inspect the datasets and check their coverage over the full date range when aggregated by dataset #exports def identify_df_dt_entries ( df_demand , df_pv , df_weather ): min_dt = min ( df_demand . index . min (), df_pv . index . min (), df_weather . index . min ()) max_dt = max ( df_demand . index . max (), df_pv . index . max (), df_weather . index . max ()) dt_rng = pd . date_range ( min_dt , max_dt , freq = '30T' ) df_nulls = pd . DataFrame ( index = dt_rng ) df_nulls [ 'demand' ] = df_demand . reindex ( dt_rng ) . isnull () . mean ( axis = 1 ) . astype ( int ) df_nulls [ 'pv' ] = df_pv . reindex ( dt_rng ) . isnull () . mean ( axis = 1 ) . astype ( int ) df_nulls [ 'weather' ] = df_weather . reindex ( dt_rng ) . ffill ( limit = 1 ) . isnull () . mean ( axis = 1 ) . astype ( int ) df_entries = 1 - df_nulls return df_entries df_entries = identify_df_dt_entries ( df_demand , df_pv , df_weather ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) sns . heatmap ( df_entries . T , ax = ax , cmap = plt . cm . binary ) utils . set_date_ticks ( ax , df_entries . index . min () . strftime ( '%Y-%m- %d ' ), df_entries . index . max () . strftime ( '%Y-%m- %d ' ), axis = 'x' , freq = 'Qs' , date_format = '%b %y' ) <AxesSubplot:> We'll also determine the null percentage in each individual column df_demand . isnull () . mean () demand_MW 0.0 dtype: float64 We can see that all of the PV data columns are missing some data df_pv . isnull () . mean () irradiance_Wm-2 0.001863 panel_temp_C 0.001991 pv_power_mw 0.000771 dtype: float64 Locations 1 and 2 are also missing some solar data, with 4 missing temperature data df_weather . isnull () . mean () solar_location1 0.001487 solar_location2 0.000992 solar_location3 0.000000 solar_location4 0.000000 solar_location5 0.000000 solar_location6 0.000000 temp_location1 0.000000 temp_location2 0.000000 temp_location3 0.000000 temp_location4 0.000992 temp_location5 0.000000 temp_location6 0.000000 dtype: float64","title":"Identifying Missing Values"},{"location":"02-cleaning/#handling-missing-values","text":"We'll start by interpolating the missing PV data, first checking the number of variables that have null values for each time period s_pv_num_null_vals = df_pv . isnull () . sum ( axis = 1 ) . replace ( 0 , np . nan ) . dropna () . astype ( int ) s_pv_num_null_vals . value_counts () 1 103 3 24 dtype: int64 pv_power_mw and irradiance_Wm-2 have the same average number of null values, there are also no time-periods where only 2 variables have null values - it's therefore likely that power and irradiance always have null periods at the same time which makes it harder to interpolate their values. We'll quickly check this hypothesis: ( df_pv [ 'pv_power_mw' ] . isnull () == df_pv [ 'irradiance_Wm-2' ] . isnull ()) . mean () == 1 False It appears as though the pv_power_mw and irradiance_Wm-2 missing values are a single time-block that coincides with a larger set of missing values within panel_temp_C . df_pv [ df_pv [ 'pv_power_mw' ] . isnull ()] ('Unnamed: 0_level_0', 'datetime') ('irradiance_Wm-2', 'Unnamed: 1_level_1') ('panel_temp_C', 'Unnamed: 2_level_1') ('pv_power_mw', 'Unnamed: 3_level_1') 2018-03-04 07:00:00+00:00 nan nan nan 2018-03-04 07:30:00+00:00 nan nan nan 2018-03-04 08:00:00+00:00 nan nan nan 2018-03-04 08:30:00+00:00 nan nan nan 2018-03-04 09:00:00+00:00 nan nan nan 2018-03-04 09:30:00+00:00 nan nan nan 2018-03-04 10:00:00+00:00 nan nan nan 2018-03-04 10:30:00+00:00 nan nan nan 2018-03-04 11:00:00+00:00 nan nan nan 2018-03-04 11:30:00+00:00 nan nan nan 2018-03-04 12:00:00+00:00 nan nan nan 2018-03-04 12:30:00+00:00 nan nan nan 2018-03-04 13:00:00+00:00 nan nan nan 2018-03-04 13:30:00+00:00 nan nan nan 2018-03-04 14:00:00+00:00 nan nan nan 2018-03-04 15:00:00+00:00 nan nan nan 2018-03-04 15:30:00+00:00 nan nan nan 2018-03-04 16:00:00+00:00 nan nan nan 2018-03-04 16:30:00+00:00 nan nan nan 2018-03-04 17:00:00+00:00 nan nan nan 2019-07-19 14:00:00+00:00 nan nan nan 2019-07-19 14:30:00+00:00 nan nan nan 2019-07-19 15:00:00+00:00 nan nan nan 2019-07-19 15:30:00+00:00 nan nan nan Looking at the panel_temp_C data we can see there are 3 time-blocks where obervations are missing df_pv [ 'panel_temp_C' ] . isnull () . astype ( int ) . plot () <AxesSubplot:xlabel='datetime'> One option might to be replace the missing temperature values with the temperatures observed at the surrounding weather grid locations, we'll start by constructing a dataframe that includes all of the temperature data as well as the average rolling temperature for each weather data location. #exports def construct_df_temp_features ( df_weather , df_pv ): df_weather = df_weather . reindex ( pd . date_range ( df_weather . index . min (), df_weather . index . max (), freq = '30T' )) . ffill ( limit = 1 ) temp_loc_cols = df_weather . columns [ df_weather . columns . str . contains ( 'temp' )] df_temp_features = ( df_weather . copy () [ temp_loc_cols ] . assign ( site_temp = df_pv [ 'panel_temp_C' ]) ) df_temp_features [[ col + '_rolling' for col in temp_loc_cols ]] = df_temp_features . rolling ( 3 ) . mean ()[ temp_loc_cols ] df_temp_features = df_temp_features . sort_index ( axis = 1 ) return df_temp_features df_temp_features = construct_df_temp_features ( df_weather , df_pv ) . dropna () df_temp_features . head () site_temp temp_location1 temp_location1_rolling temp_location2 temp_location2_rolling temp_location3 temp_location3_rolling temp_location4 temp_location4_rolling temp_location5 temp_location5_rolling temp_location6 temp_location6_rolling 2017-11-03 00:00:00+00:00 7.05 8.56 8.62667 9.64 9.66 7.46 7.78667 6.68 6.93333 13.09 13.0233 13.2 13.1133 2017-11-03 00:30:00+00:00 7.38 8.56 8.59333 9.64 9.65 7.46 7.62333 6.68 6.80667 13.09 13.0567 13.2 13.1567 2017-11-03 01:00:00+00:00 7.7 8.69 8.60333 9.71 9.66333 7.14 7.35333 6.27 6.54333 13.21 13.13 13.32 13.24 2017-11-03 01:30:00+00:00 7.48 8.69 8.64667 9.71 9.68667 7.14 7.24667 6.27 6.40667 13.21 13.17 13.32 13.28 2017-11-03 02:00:00+00:00 7.2 8.74 8.70667 9.73 9.71667 6.86 7.04667 5.91 6.15 13.3 13.24 13.36 13.3333 We'll now check the correlation sns . heatmap ( df_temp_features . corr ()) <AxesSubplot:> The correlation drops off quickly when it gets to the site temperature, looking at the full distributions we can see that the site measurements get far higher. This is because the panel is absorbing heat that raises its temperature above that of the surrounding area, again making it more difficult to simply fill in with the nearby temperature measurements. sns . histplot ( df_temp_features [ 'site_temp' ], color = 'C0' , label = 'Panel' ) sns . histplot ( df_temp_features . drop ( 'site_temp' , axis = 1 ) . min ( axis = 1 ), color = 'C1' , label = 'MERRA Min' ) sns . histplot ( df_temp_features . drop ( 'site_temp' , axis = 1 ) . max ( axis = 1 ), color = 'C2' , label = 'MERRA Max' ) plt . legend ( frameon = False ) C:\\Users\\Ayrto\\anaconda3\\envs\\batopt\\lib\\site-packages\\seaborn\\distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) C:\\Users\\Ayrto\\anaconda3\\envs\\batopt\\lib\\site-packages\\seaborn\\distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) C:\\Users\\Ayrto\\anaconda3\\envs\\batopt\\lib\\site-packages\\seaborn\\distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) <matplotlib.legend.Legend at 0x1fa8d979310> # Could use an RF to estimate the panel temp based on the weather grid temps? # Potential features: current average surrounding temp, average surrounding temp over the last 3 hours #exports def split_X_y_data ( df , target_col = 'site_temp' ): df = df . dropna () X_cols = df . drop ( target_col , axis = 1 ) . columns X = df [ X_cols ] . values y = df [ target_col ] . values return X , y def split_X_y_data_with_index ( df , target_col = 'site_temp' ): df = df . dropna () X_cols = df . drop ( target_col , axis = 1 ) . columns X = df [ X_cols ] . values y = df [ target_col ] . values index = df . index return X , y , index X , y = split_X_y_data ( df_temp_features ) X . shape , y . shape ((37024, 12), (37024,)) #exports def generate_kfold_preds ( X , y , model = LinearRegression (), kfold_kwargs = { 'n_splits' : 5 , 'shuffle' : True }, index = None ): kfold = KFold ( ** kfold_kwargs ) df_pred = pd . DataFrame ( columns = [ 'pred' , 'true' ], index = np . arange ( X . shape [ 0 ])) for train_idxs , test_idxs in kfold . split ( X ): X_train , y_train = X [ train_idxs ], y [ train_idxs ] X_test , y_test = X [ test_idxs ], y [ test_idxs ] model . fit ( X_train , y_train ) df_pred . loc [ test_idxs , 'true' ] = y_test df_pred . loc [ test_idxs , 'pred' ] = model . predict ( X_test ) df_pred = df_pred . sort_index () if index is not None : assert len ( index ) == df_pred . shape [ 0 ], 'The passed index must be the same length as X and y' df_pred . index = index return df_pred df_pred = generate_kfold_preds ( X , y ) df_pred . head () pred true 0 4.75986 7.05 1 5.00606 7.38 2 5.40829 7.7 3 5.49439 7.48 4 5.27108 7.2 #exports def evaluate_models ( X , y , models , post_pred_proc_func = None , index = None ): model_scores = dict () for model_name , model in track ( models . items ()): df_pred = generate_kfold_preds ( X , y , model , index = index ) if post_pred_proc_func is not None : df_pred [ 'pred' ] = post_pred_proc_func ( df_pred [ 'pred' ]) model_scores [ model_name ] = { 'mae' : mean_absolute_error ( df_pred [ 'true' ], df_pred [ 'pred' ]), 'rmse' : np . sqrt ( mean_squared_error ( df_pred [ 'true' ], df_pred [ 'pred' ])) } df_model_scores = pd . DataFrame ( model_scores ) df_model_scores . index . name = 'metric' df_model_scores . columns . name = 'model' return df_model_scores models = { 'std_linear' : LinearRegression (), 'random_forest' : RandomForestRegressor (), 'boosted' : GradientBoostingRegressor () } rerun_panel_temp_model = False model_scores_filename = 'panel_temp_interp_model_results.csv' if ( rerun_panel_temp_model == True ) or ( model_scores_filename not in os . listdir ( cache_data_dir )): df_model_scores = evaluate_models ( X , y , models ) df_model_scores . to_csv ( f ' { cache_data_dir } / { model_scores_filename } ' ) else : df_model_scores = pd . read_csv ( f ' { cache_data_dir } / { model_scores_filename } ' , index_col = 'metric' ) df_model_scores ('Unnamed: 0_level_0', 'metric') ('std_linear', 'Unnamed: 1_level_1') ('random_forest', 'Unnamed: 2_level_1') ('boosted', 'Unnamed: 3_level_1') mae 2.81922 1.68451 2.58143 rmse 3.78674 2.69334 3.73415 top_model = df_model_scores . T [ 'rmse' ] . idxmin () df_pred = generate_kfold_preds ( X , y , models [ top_model ]) df_pred . head () pred true 0 6.0645 7.05 1 6.1691 7.38 2 7.5151 7.7 3 7.7232 7.48 4 7.1748 7.2 s_residuals = df_pred . diff ( 1 , axis = 1 ) . dropna ( axis = 1 ) . iloc [:, 0 ] s_residuals . plot ( linewidth = 0.3 ) <AxesSubplot:> plt . scatter ( df_pred [ 'true' ], df_pred [ 'pred' ], s = 1 ) plt . xlabel ( 'Obervation' ) plt . ylabel ( 'Prediction' ) Text(0, 0.5, 'Prediction') #exports def interpolate_missing_panel_temps ( df_pv , df_weather , model = RandomForestRegressor ()): missing_panel_temp_dts = df_pv . index [ df_pv [ 'panel_temp_C' ] . isnull ()] if len ( missing_panel_temp_dts ) == 0 : # i.e. no missing values return df_pv df_temp_features = construct_df_temp_features ( df_weather , df_pv ) missing_dt_X = df_temp_features . loc [ missing_panel_temp_dts ] . drop ( 'site_temp' , axis = 1 ) . values X , y = split_X_y_data ( df_temp_features , 'site_temp' ) model . fit ( X , y ) df_pv . loc [ missing_panel_temp_dts , 'panel_temp_C' ] = model . predict ( missing_dt_X ) assert df_pv [ 'panel_temp_C' ] . isnull () . sum () == 0 , 'There are still null values for the PV panel temperature' return df_pv df_pv = interpolate_missing_panel_temps ( df_pv , df_weather ) df_pv . isnull () . mean () irradiance_Wm-2 0.002016 panel_temp_C 0.000000 pv_power_mw 0.000645 dtype: float64 #exports def construct_df_irradiance_features ( df_weather , df_pv ): df_weather = df_weather . reindex ( pd . date_range ( df_weather . index . min (), df_weather . index . max (), freq = '30T' )) . ffill ( limit = 1 ) temp_loc_cols = df_weather . columns [ df_weather . columns . str . contains ( 'solar' )] df_irradiance_features = ( df_weather . copy () [ temp_loc_cols ] . assign ( site_solar = df_pv [ 'irradiance_Wm-2' ]) . pipe ( lambda df : df . assign ( hour = df . index . hour + df . index . minute / 60 )) ) df_irradiance_features = df_irradiance_features . sort_index ( axis = 1 ) return df_irradiance_features df_irradiance_features = construct_df_irradiance_features ( df_weather , df_pv ) df_irradiance_features . head () hour site_solar solar_location1 solar_location2 solar_location3 solar_location4 solar_location5 solar_location6 2015-01-01 00:00:00+00:00 0 nan 0 0 0 0 0 0 2015-01-01 00:30:00+00:00 0.5 nan 0 0 0 0 0 0 2015-01-01 01:00:00+00:00 1 nan 0 0 0 0 0 0 2015-01-01 01:30:00+00:00 1.5 nan 0 0 0 0 0 0 2015-01-01 02:00:00+00:00 2 nan 0 0 0 0 0 0 models = { 'std_linear' : LinearRegression (), 'random_forest' : RandomForestRegressor (), 'boosted' : GradientBoostingRegressor () } rerun_site_irradiance_model = False model_scores_filename = 'site_irradiance_interp_model_results.csv' X , y = split_X_y_data ( df_irradiance_features , 'site_solar' ) if ( rerun_site_irradiance_model == True ) or ( model_scores_filename not in os . listdir ( cache_data_dir )): df_model_scores = evaluate_models ( X , y , models ) df_model_scores . to_csv ( f ' { cache_data_dir } / { model_scores_filename } ' ) else : df_model_scores = pd . read_csv ( f ' { cache_data_dir } / { model_scores_filename } ' , index_col = 'metric' ) df_model_scores ('Unnamed: 0_level_0', 'metric') ('std_linear', 'Unnamed: 1_level_1') ('random_forest', 'Unnamed: 2_level_1') ('boosted', 'Unnamed: 3_level_1') mae 57.4977 37.5087 49.6956 rmse 110.546 78.8525 99.1964 top_model = df_model_scores . T [ 'rmse' ] . idxmin () df_pred = generate_kfold_preds ( X , y , models [ top_model ]) df_pred . head () pred true 0 0.000657 0 1 0.00044 0 2 0.000239 0 3 0.001181 0 4 0.001369 0 plt . scatter ( df_pred [ 'true' ], df_pred [ 'pred' ], s = 1 ) plt . xlabel ( 'Obervation' ) plt . ylabel ( 'Prediction' ) Text(0, 0.5, 'Prediction') #exports def interpolate_missing_site_irradiance ( df_pv , df_weather , model = RandomForestRegressor ()): missing_site_irradiance_dts = df_pv . index [ df_pv [ 'irradiance_Wm-2' ] . isnull ()] if len ( missing_site_irradiance_dts ) == 0 : # i.e. no missing values return df_pv df_irradiance_features = construct_df_irradiance_features ( df_weather , df_pv ) missing_dt_X = df_irradiance_features . loc [ missing_site_irradiance_dts ] . drop ( 'site_solar' , axis = 1 ) . values X , y = split_X_y_data ( df_irradiance_features , 'site_solar' ) model . fit ( X , y ) df_pv . loc [ missing_site_irradiance_dts , 'irradiance_Wm-2' ] = model . predict ( missing_dt_X ) assert df_pv [ 'irradiance_Wm-2' ] . isnull () . sum () == 0 , 'There are still null values for the solar site irradiance' return df_pv df_pv = interpolate_missing_site_irradiance ( df_pv , df_weather ) df_pv . isnull () . mean () irradiance_Wm-2 0.000000 panel_temp_C 0.000000 pv_power_mw 0.000645 dtype: float64 Now that we have the irradiance and temperature we're ready to start filling in the missing values for power output, again using the same regression interpolation method #exports def construct_df_power_features ( df_pv ): df_power_features = ( df_pv . pipe ( lambda df : df . assign ( hour = df . index . hour + df . index . minute / 60 )) . sort_index ( axis = 1 ) ) return df_power_features df_power_features = construct_df_power_features ( df_pv ) df_power_features . head () ('Unnamed: 0_level_0', 'datetime') ('hour', 'Unnamed: 1_level_1') ('irradiance_Wm-2', 'Unnamed: 2_level_1') ('panel_temp_C', 'Unnamed: 3_level_1') ('pv_power_mw', 'Unnamed: 4_level_1') 2017-11-03 00:00:00+00:00 0 0 7.05 0 2017-11-03 00:30:00+00:00 0.5 0 7.38 0 2017-11-03 01:00:00+00:00 1 0 7.7 0 2017-11-03 01:30:00+00:00 1.5 0 7.48 0 2017-11-03 02:00:00+00:00 2 0 7.2 0 models = { 'std_linear' : LinearRegression (), 'random_forest' : RandomForestRegressor (), 'boosted' : GradientBoostingRegressor () } rerun_site_power_model = False model_scores_filename = 'site_power_interp_model_results.csv' X , y , dates = split_X_y_data_with_index ( df_power_features , 'pv_power_mw' ) if ( rerun_site_power_model == True ) or ( model_scores_filename not in os . listdir ( cache_data_dir )): df_model_scores = evaluate_models ( X , y , models ) df_model_scores . to_csv ( f ' { cache_data_dir } / { model_scores_filename } ' ) else : df_model_scores = pd . read_csv ( f ' { cache_data_dir } / { model_scores_filename } ' , index_col = 'metric' ) df_model_scores ('Unnamed: 0_level_0', 'metric') ('std_linear', 'Unnamed: 1_level_1') ('random_forest', 'Unnamed: 2_level_1') ('boosted', 'Unnamed: 3_level_1') mae 0.061519 0.041122 0.043927 rmse 0.14598 0.135822 0.133212 top_model = df_model_scores . T [ 'rmse' ] . idxmin () df_pred = generate_kfold_preds ( X , y , models [ top_model ]) df_pred . head () pred true 0 -0.000829 0 1 -0.000634 0 2 -0.000178 0 3 0.000163 0 4 -0.001097 0 plt . scatter ( df_pred [ 'true' ], df_pred [ 'pred' ], s = 1 ) plt . xlabel ( 'Obervation' ) plt . ylabel ( 'Prediction' ) Text(0, 0.5, 'Prediction')","title":"Handling Missing Values"},{"location":"02-cleaning/#anomalous-data-points-in-pv-data","text":"The PV data shows a number of points where the observed value is 0 but the prediction is much higher. First let's try and identify them (setting the tolerance to be lower will capture more values as anomalous). def identify_anomalies_pv ( df_pred , tolerance = 0.1 ): foo = df_pred . copy () foo [ 'difference' ] = foo . pred - foo . true foo = foo [( foo . difference > tolerance ) & ( foo . true == 0 )] return foo . index anomalous_dates = dates [ identify_anomalies_pv ( df_pred )] anomalous_df = df_power_features [ df_power_features . index . isin ( anomalous_dates )] plt . hist ( anomalous_df . hour ) # Check this histogram to eyeball if any unreasonable anomalous values are caught by the tolerance (e.g. late at night) (array([ 3., 9., 24., 22., 34., 22., 30., 12., 9., 3.]), array([ 6. , 7.25, 8.5 , 9.75, 11. , 12.25, 13.5 , 14.75, 16. , 17.25, 18.5 ]), <BarContainer object of 10 artists>) Replace these values in df_power_features . df_power_features_clean = df_power_features . copy () df_power_features_clean . loc [ df_power_features_clean . index . isin ( anomalous_dates ), 'pv_power_mw' ] = np . nan Rerun the previous model fitting and check the pred vs. actual graph. models = { 'std_linear' : LinearRegression (), 'random_forest' : RandomForestRegressor (), 'boosted' : GradientBoostingRegressor () } rerun_site_power_model = False model_scores_filename = 'site_power_interp_clean_model_results.csv' X , y , dates = split_X_y_data_with_index ( df_power_features_clean , 'pv_power_mw' ) if ( rerun_site_power_model == True ) or ( model_scores_filename not in os . listdir ( cache_data_dir )): df_model_scores = evaluate_models ( X , y , models ) df_model_scores . to_csv ( f ' { cache_data_dir } / { model_scores_filename } ' ) else : df_model_scores = pd . read_csv ( f ' { cache_data_dir } / { model_scores_filename } ' , index_col = 'metric' ) top_model = df_model_scores . T [ 'rmse' ] . idxmin () df_pred = generate_kfold_preds ( X , y , models [ top_model ]) plt . scatter ( df_pred [ 'true' ], df_pred [ 'pred' ], s = 1 ) plt . xlabel ( 'Obervation' ) plt . ylabel ( 'Prediction' ) Text(0, 0.5, 'Prediction') The above graph looks to be a cleaner with tolerance at 0.1. It looks like there might still be some which aren't though. Consider lowering the tolerance. #exports def pv_anomalies_to_nan ( df_pv , model = GradientBoostingRegressor (), tolerance = 0.1 ): \"\"\" Run this function to identify places where predicted values for pv_power_mw are much larger than true values and where the true value is 0 (we expect these are anomalies) and make these values nan. \"\"\" df_power_features = construct_df_power_features ( df_pv ) X , y , dates = split_X_y_data_with_index ( df_power_features , 'pv_power_mw' ) df_pred = generate_kfold_preds ( X , y , model ) df_pred [ 'difference' ] = df_pred . pred - df_pred . true df_pred [ 'datetime' ] = dates df_pred = df_pred . set_index ( 'datetime' ) anomalous_idx = df_pred [( df_pred . difference > tolerance ) & ( df_pred . true == 0 )] . index df_pv . loc [ df_pv . index . isin ( anomalous_idx ), 'pv_power_mw' ] = np . nan return df_pv df_pv = pv_anomalies_to_nan ( df_pv ) #exports def interpolate_missing_site_power ( df_pv , model = RandomForestRegressor ()): missing_site_power_dts = df_pv . index [ df_pv [ 'pv_power_mw' ] . isnull ()] if len ( missing_site_power_dts ) == 0 : # i.e. no missing values return df_pv df_power_features = construct_df_power_features ( df_pv ) missing_dt_X = df_power_features . loc [ missing_site_power_dts ] . drop ( 'pv_power_mw' , axis = 1 ) . values X , y = split_X_y_data ( df_power_features , 'pv_power_mw' ) model . fit ( X , y ) df_pv . loc [ missing_site_power_dts , 'pv_power_mw' ] = model . predict ( missing_dt_X ) assert df_pv [ 'pv_power_mw' ] . isnull () . sum () == 0 , 'There are still null values for the solar site power' return df_pv df_pv = interpolate_missing_site_power ( df_pv ) df_pv . isnull () . mean () irradiance_Wm-2 0.0 panel_temp_C 0.0 pv_power_mw 0.0 dtype: float64 #exports def interpolate_missing_weather_solar ( df_pv , df_weather , weather_col = 'solar_location2' , model = RandomForestRegressor ()): missing_weather_solar_dts = df_weather . index [ df_weather [ weather_col ] . isnull ()] if len ( missing_weather_solar_dts ) == 0 : # i.e. no missing values return df_pv df_irradiance_features = construct_df_irradiance_features ( df_weather , df_pv ) . drop ( 'site_solar' , axis = 1 ) missing_dt_X = df_irradiance_features . loc [ missing_weather_solar_dts ] . drop ( weather_col , axis = 1 ) . values X , y = split_X_y_data ( df_irradiance_features , weather_col ) model . fit ( X , y ) df_weather . loc [ missing_weather_solar_dts , weather_col ] = model . predict ( missing_dt_X ) assert df_weather [ weather_col ] . isnull () . sum () == 0 , 'There are still null values for the weather dataset solar observations' return df_weather df_weather = interpolate_missing_weather_solar ( df_pv , df_weather , model = LinearRegression ()) df_weather . isnull () . mean () solar_location1 0.0000 solar_location2 0.0000 solar_location3 0.0000 solar_location4 0.0000 solar_location5 0.0000 solar_location6 0.0000 temp_location1 0.0000 temp_location2 0.0000 temp_location3 0.0000 temp_location4 0.0011 temp_location5 0.0000 temp_location6 0.0000 dtype: float64","title":"Anomalous data points in PV data"},{"location":"02-cleaning/#interpolate-missing-temp-data","text":"foo = interpolate_missing_temps ( df_weather , 'temp_location4' , model = LinearRegression ()) foo . isnull () . mean () solar_location1 0.0 solar_location2 0.0 solar_location3 0.0 solar_location4 0.0 solar_location5 0.0 solar_location6 0.0 temp_location1 0.0 temp_location2 0.0 temp_location3 0.0 temp_location4 0.0 temp_location5 0.0 temp_location6 0.0 dtype: float64 Finally we'll export the relevant code to our batopt module","title":"Interpolate Missing Temp Data"},{"location":"03-charging/","text":"Battery Charging \u00b6 Imports \u00b6 #exports import numpy as np import pandas as pd import os import matplotlib.pyplot as plt import seaborn as sns import joblib from moepy.lowess import quantile_model from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression from sklearn.metrics import make_scorer , r2_score , mean_absolute_error , mean_squared_error from sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor from skopt.plots import plot_objective from skopt.space import Real , Categorical , Integer from batopt import clean , discharge , utils import FEAutils as hlp # Should do some investigation of how the panel temp influences performance User Stories \u00b6 raw_data_dir = '../data/raw' intermediate_data_dir = '../data/intermediate' cache_data_dir = '../data/nb-cache' charge_opt_model_fp = '../models/charge_opt.sav' Loading Data \u00b6 df = clean . combine_training_datasets ( intermediate_data_dir ) . interpolate ( limit = 1 ) df . head () demand pv weather demand_MW irradiance_Wm-2 panel_temp_C pv_power_mw solar_location1 solar_location2 solar_location3 solar_location4 solar_location5 solar_location6 temp_location1 temp_location2 temp_location3 temp_location4 temp_location5 temp_location6 holidays 2015-01-01 00:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.75 9.65 8.83 7.58 11.62 11.22 nan 2015-01-01 00:30:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.83 9.705 8.865 7.6 11.635 11.27 nan 2015-01-01 01:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.91 9.76 8.9 7.62 11.65 11.32 nan 2015-01-01 01:30:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.95 9.78 9 7.615 11.65 11.31 nan 2015-01-01 02:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.99 9.8 9.1 7.61 11.65 11.3 nan df . pv_power_mw . plot () <AxesSubplot:> Correlations between the solar variables: solar_cols = [ c for c in df . columns if 'solar_location' in c ] solar_cols . append ( 'irradiance_Wm-2' ) solar_cols . append ( 'panel_temp_C' ) solar_cols . append ( 'pv_power_mw' ) fig , ax = plt . subplots ( dpi = 250 ) df_solar = df . filter ( solar_cols ) . copy () ax = sns . heatmap ( df_solar . corr (), cmap = 'viridis' ) fig . savefig ( '../img/solar_corrplot.png' ) As in the demand data, estimating the quantiles for the solar PV output: #exports def estimate_daily_solar_quantiles ( x , y , x_pred = np . linspace ( 0 , 23.5 , 100 ), ** model_kwargs ): # Fitting the model df_quantiles = quantile_model ( x , y , x_pred = x_pred , ** model_kwargs ) # Cleaning names and sorting for plotting df_quantiles . columns = [ f 'p { int ( col * 100 ) } ' for col in df_quantiles . columns ] df_quantiles = df_quantiles [ df_quantiles . columns [:: - 1 ]] return df_quantiles dts = df . index . tz_convert ( 'Europe/London' ) x = np . array ( dts . hour + dts . minute / 60 ) y = df [ 'pv_power_mw' ] . values rerun_daily_solar_model = False daily_solar_filename = 'daily_solar_quantile_model_results.csv' if ( rerun_daily_solar_model == True ) or ( daily_solar_filename not in os . listdir ( cache_data_dir )): df_quantiles = estimate_daily_solar_quantiles ( x , y , frac = 0.2 , num_fits = 48 , robust_iters = 3 ) df_quantiles . to_csv ( f ' { cache_data_dir } / { daily_solar_filename } ' ) else : df_quantiles = pd . read_csv ( f ' { cache_data_dir } / { daily_solar_filename } ' , index_col = 'x' ) And plotting x_jittered = x + ( np . random . uniform ( size = len ( x )) - 0.5 ) / 2.5 # Plotting fig , ax = plt . subplots ( dpi = 250 ) ax . scatter ( x_jittered , y , s = 0.2 , color = 'k' , alpha = 0.5 ) df_quantiles . plot ( cmap = 'viridis' , legend = False , ax = ax ) hlp . hide_spines ( ax ) ax . legend ( frameon = False , bbox_to_anchor = ( 1 , 0.9 ), title = 'Percentiles' ) ax . set_xlabel ( 'Time of Day' ) ax . set_ylabel ( 'Demand (MW)' ) ax . set_xlim ( 0 , 24 ) ax . set_ylim ( 0 , 4 ) fig . savefig ( '../img/daily_solar_profile.png' ) Proportion of days during which we can fully charge the battery \u00b6 It may be useful to know the proportion of days during which the battery can be fully charged. df_solar_hrs = df . between_time ( '00:00:00' , '15:00:00' ) pv_generation = df_solar_hrs . groupby ( df_solar_hrs . index . date ) . sum ()[ 'pv_power_mw' ] * 0.5 # available daily energy from PV fig , ax = plt . subplots () ax . hist ( pv_generation , bins = 20 ) plt . show () prop = np . sum ( pv_generation >= 6 ) / pv_generation . size print ( \"Proportion of days where solar generation exceeds 6 MWh: {:.2f} %\" . format ( prop * 100 )) Proportion of days where solar generation exceeds 6 MWh: 29.85% Optimal charging with perfect foresight \u00b6 We will now develop an algorithm to determine the optimal charging schedule given a perfect solar forecast. The scoring function for the generation component rewards us taking as much energy as possible from solar PV. The proportion of energy from PV for a day \\(d\\) is given by \\( \\(p_{d,1} = \\frac{\\sum{P_{d,k}}}{\\sum{B_{d,k}}}\\) \\) where we are summing over all periods \\(k\\) . An equivalent equation is applies for \\(p_{d,2}\\) which is the energy that is drawn from the grid. The scoring function rewards \\(p_{d,1}\\) over \\(p_{d,2}\\) in a ratio of 3 to 1. Any schedule which fully exploits the solar PV potential until the battery is charged is equally good in terms of the scoring function. However, it may be worth considering methods which give a smoother charge profile for the purposes of producing a robust model for unseen days. In addition, we need to have a method of intelligently allocating charge when the solar PV potential is less than the capacity of the battery. Some possible methods for this: Naively reallocate over the middle of they day (say 09:00--15:00) Add charge to periods where charge has already been committed. Use a forecast for PV output and allocate charge proportionally to the forecast. s_pv = df [ 'pv_power_mw' ] . dropna () solar_profile = discharge . sample_random_days ( s_pv ) solar_profile . plot () <AxesSubplot:> For perfect foresight, any schedule that draws all of the available solar power or 6 MWh (if the total solar production exceeds 6 MWh) is equally good. This first approach will aim to draw greedily from until 6 MWh is satisfied, or all of the solar production has been expended. In cases where there is not enough solar PV to fill the battery, we will then uniformly add the remaining capacity across all periods. Note: this seems to work on this dataset but won't if there is a very large spike in solar PV, such topping up uniformly causes a constraint to be violated. It also may not work if the number of periods over which we top up is decreased. #exports def extract_solar_profile ( s_solar_sample_dt , start_time = '00:00' , end_time = '15:00' ): dt = str ( s_solar_sample_dt . index [ 0 ] . date ()) solar_profile = s_solar_sample_dt [ f ' { dt } { start_time } ' : f ' { dt } { end_time } ' ] . values return solar_profile def charge_profile_greedy ( solar_profile , capacity = 6 , initial_charge = 0 , max_charge_rate = 2.5 , time_unit = 0.5 ): order = np . flip ( np . argsort ( solar_profile )) charge = initial_charge solution = np . zeros ( len ( solar_profile )) for i in order : solar_available = np . minimum ( solar_profile [ i ], max_charge_rate ) solar_available = min ( solar_available , ( capacity - charge ) / time_unit ) solution [ i ] = solar_available charge = np . sum ( solution ) * time_unit if charge > capacity : break return solution def topup_charge_naive ( charge_profile , capacity = 6 , time_unit = 0.5 , period_start = 16 , period_end = 30 ): charge = np . sum ( charge_profile ) * time_unit spare_cap = capacity - charge topup_value = spare_cap / (( period_end - period_start ) * time_unit ) new_profile = np . copy ( charge_profile ) new_profile [ period_start : period_end ] += topup_value # Add topup_value uniformly between start and end periods return new_profile def optimal_charge_profile ( solar_profile , capacity = 6 , time_unit = 0.5 , max_charge_rate = 2.5 ): solution = charge_profile_greedy ( solar_profile ) solution = topup_charge_naive ( solution ) assert np . isclose ( np . sum ( solution ), capacity / time_unit ), \"Does not meet capacity constraint\" . format ( np . sum ( solution )) assert np . all ( solution <= max_charge_rate ), \"Does not meet max charge rate constraint. Max is {} \" . format ( np . max ( solution )) return solution random_solar_profile = discharge . sample_random_day ( s_pv ) . pipe ( extract_solar_profile ) x = optimal_charge_profile ( random_solar_profile ) # Note there is sometimes a rounding error here plt . plot ( x ) [<matplotlib.lines.Line2D at 0x25382e34520>] The danger with this method is that it can be quite spiky. I wonder if this (a) makes the function difficult to learn (b) is too risky as compared with hedging bets with a more smoother approach. Smooth Approach \u00b6 We can use the same peak flattening algorithm developed for the dischrge optimisation adj_random_solar_profile = discharge . flatten_peak ( random_solar_profile ) plt . plot ( random_solar_profile ) plt . plot ( adj_random_solar_profile ) [<matplotlib.lines.Line2D at 0x25382e8f160>] Which we can deduct from the original evening profile to construct the charge profile #exports construct_charge_profile = lambda solar_profile , adj_solar_profile : solar_profile - adj_solar_profile charge_profile = construct_charge_profile ( random_solar_profile , adj_random_solar_profile ) plt . plot ( charge_profile ) [<matplotlib.lines.Line2D at 0x25382ed7e80>] Rather than the sample day we've just used we'll now repeat this step for all days we have pv data on, returning a series of the new charge values that can be easily added to the discharge values #exports def construct_charge_s ( s_pv , start_time = '00:00' , end_time = '15:00' ): s_charge = pd . Series ( index = s_pv . index , dtype = float ) . fillna ( 0 ) for dt in s_pv . index . strftime ( '%Y-%m- %d ' ) . unique (): solar_profile = s_pv [ dt ] . pipe ( extract_solar_profile , start_time = start_time , end_time = end_time ) adj_solar_profile = discharge . flatten_peak ( solar_profile ) charge_profile = construct_charge_profile ( solar_profile , adj_solar_profile ) s_charge [ f ' { dt } { start_time } ' : f ' { dt } { end_time } ' ] = charge_profile return s_charge def charge_is_valid ( charge_profile , capacity = 6 , max_charge_rate = 2.5 , time_unit = 0.5 ): \"\"\" Function determining if a charge profile is valid (and fully charges the battery) \"\"\" if np . all ( np . isclose ( capacity / time_unit , charge_profile . groupby ( charge_profile . index . date ) . sum ())) is False : return False elif np . all ( charge_profile . groupby ( charge_profile . index . date ) . max () <= max_charge_rate ) is False : return False else : return True s_charge = construct_charge_s ( s_pv , start_time = '00:00' , end_time = '15:00' ) s_charge . iloc [: 48 * 7 ] . plot () charge_is_valid ( s_charge ) True With the greedy algorithm we can analyse the periods during which charging occurs: s_charge . groupby ( s_charge . index . time ) . mean () . plot () <AxesSubplot:xlabel='time'> Unsurprisingly we never charge before 5am. We can therefore truncate our training to just look at 05:00--15:30. Confirm that the optimal charge adds up to 6 MWh each day: s_charge . groupby ( s_charge . index . date ) . sum () . round ( 10 ) . value_counts () 12.000000 763 12.130526 1 12.160000 1 12.118333 1 12.211000 1 12.240000 1 12.095714 1 12.469091 1 12.081538 1 12.555455 1 12.132308 1 12.235714 1 12.003333 1 12.007100 1 dtype: int64 Model development: charging \u00b6 Following the same structure as battery discharge, we will aim to predict the optimal charge schedule. #exports def construct_df_charge_features ( df , dt_rng = None ): if dt_rng is None : dt_rng = pd . date_range ( df . index . min (), df . index . max (), freq = '30T' ) df_features = pd . DataFrame ( index = dt_rng ) # Filtering for the temperature weather data temp_loc_cols = df . columns [ df . columns . str . contains ( 'temp_location' )] df_features . loc [ df . index , temp_loc_cols ] = df [ temp_loc_cols ] . copy () df_features = df_features . ffill ( limit = 1 ) # Adding lagged solar df_features [ 'pv_7d_lag' ] = df [ 'pv_power_mw' ] . shift ( 48 * 7 ) # Adding solar irradiance data solar_loc_cols = df . columns [ df . columns . str . contains ( 'solar_location' )] df_features . loc [ df . index , solar_loc_cols ] = df [ solar_loc_cols ] . copy () df_features = df_features . ffill ( limit = 1 ) # Adding datetime features dts = df_features . index . tz_convert ( 'Europe/London' ) # We want to use the 'behavioural' timezone df_features [ 'weekend' ] = dts . dayofweek . isin ([ 5 , 6 ]) . astype ( int ) df_features [ 'dow' ] = dts . dayofweek hour = dts . hour + dts . minute / 60 df_features [ 'sin_hour' ] = np . sin ( 2 * np . pi * hour / 24 ) df_features [ 'cos_hour' ] = np . cos ( 2 * np . pi * hour / 24 ) df_features [ 'sin_doy' ] = np . sin ( 2 * np . pi * dts . dayofyear / 365 ) df_features [ 'cos_doy' ] = np . cos ( 2 * np . pi * dts . dayofyear / 365 ) # Removing some extraneous features cols = [ c for c in df_features . columns if 'solar_location4' not in c and 'solar_location1' not in c ] df_features = df_features . filter ( cols ) # Add rolling solar solar_cols = [ c for c in df_features . columns if 'solar_location' in c ] df_features [[ col + '_rolling' for col in solar_cols ]] = df_features . rolling ( 3 ) . mean ()[ solar_cols ] # Add rolling temp temp_cols = [ c for c in df_features . columns if 'temp_location' in c ] df_features [[ col + '_rolling' for col in temp_cols ]] = df_features . rolling ( 3 ) . mean ()[ temp_cols ] # Removing NaN values df_features = df_features . dropna () return df_features #exports def extract_charging_datetimes ( df , start_hour = 4 , end_hour = 15 ): hour = df . index . hour + df . index . minute / 60 charging_datetimes = df . index [( hour >= start_hour ) & ( hour <= end_hour )] return charging_datetimes #exports def prepare_training_input_data ( intermediate_data_dir , start_hour = 4 ): # Loading input data df = clean . combine_training_datasets ( intermediate_data_dir ) . interpolate ( limit = 1 ) df_features = construct_df_charge_features ( df ) # Filtering for overlapping feature and target data dt_idx = pd . date_range ( df_features . index . min (), df [ 'pv_power_mw' ] . dropna () . index . max () - pd . Timedelta ( minutes = 30 ), freq = '30T' ) s_pv = df . loc [ dt_idx , 'pv_power_mw' ] print ( s_pv ) df_features = df_features . loc [ dt_idx ] # Constructing the charge series s_charge = construct_charge_s ( s_pv , start_time = f '0 { start_hour } :00' , end_time = '15:00' ) # Filtering for evening datetimes charging_datetimes = extract_charging_datetimes ( df_features , start_hour = start_hour ) X = df_features . loc [ charging_datetimes ] y = s_charge . loc [ charging_datetimes ] return X , y X , y = prepare_training_input_data ( intermediate_data_dir ) X . shape , y . shape 2017-11-10 00:00:00+00:00 0.0 2017-11-10 00:30:00+00:00 0.0 2017-11-10 01:00:00+00:00 0.0 2017-11-10 01:30:00+00:00 0.0 2017-11-10 02:00:00+00:00 0.0 ... 2019-12-17 21:30:00+00:00 0.0 2019-12-17 22:00:00+00:00 0.0 2019-12-17 22:30:00+00:00 0.0 2019-12-17 23:00:00+00:00 0.0 2019-12-17 23:30:00+00:00 0.0 Freq: 30T, Name: pv_power_mw, Length: 36864, dtype: float64 ((17664, 27), (17664,)) random_day = pd . to_datetime ( np . random . choice ( y . index . date )) plt . plot ( y [ y . index . date == random_day ]) [<matplotlib.lines.Line2D at 0x25382f4b220>] df_pred = clean . generate_kfold_preds ( X . values , y . values , RandomForestRegressor (), index = X . index ) df_pred . head () pred true 2017-11-10 04:00:00+00:00 0.066136 0 2017-11-10 04:30:00+00:00 0.064022 0 2017-11-10 05:00:00+00:00 0.099278 0 2017-11-10 05:30:00+00:00 0.031904 0 2017-11-10 06:00:00+00:00 0.133574 0 plt . scatter ( df_pred . true , df_pred . pred , s = 0.1 ) plt . ylabel ( \"Predicted\" ) plt . xlabel ( \"Actual\" ) Text(0.5, 0, 'Actual') We need to fix the predictions such that they satisfy the battery constraints. We will do this in the same way as applied in the battery discharge component, first clipping the charge rate to be between 0--2.5MW, then normalising such that the total charge sums to 6 MWh. #exports def normalise_total_charge ( s_pred , charge = 6. , time_unit = 0.5 ): s_daily_charge = s_pred . groupby ( s_pred . index . date ) . sum () for date , total_charge in s_daily_charge . items (): with np . errstate ( divide = 'ignore' , invalid = 'ignore' ): s_pred . loc [ str ( date )] *= charge / ( time_unit * total_charge ) return s_pred clip_charge_rate = lambda s_pred , max_rate = 2.5 , min_rate = 0 : s_pred . clip ( lower = min_rate , upper = max_rate ) post_pred_charge_proc_func = lambda s_pred : ( s_pred . pipe ( clip_charge_rate ) . pipe ( normalise_total_charge ) ) post_pred_charge_proc_func ( df_pred [ 'true' ]) . groupby ( df_pred . index . date ) . sum () . value_counts () 12.0 393 12.0 157 12.0 138 12.0 40 12.0 31 12.0 4 12.0 4 12.0 1 Name: true, dtype: int64 Model Comparison Metrics \u00b6 Schedules are scored according to the proportion of the total battery charge that comes from solar: \\(p_{d,1} = \\frac{\\sum{P_{d,k}}}{\\sum{B_{d,k}}}\\) . We will first write a function which evaluates this scoring function for a charging schedule and solar profile. #exports def score_charging ( schedule , solar_profile ): # The actual pv charge is the minimum of the scheduled charge and the actual solar availability actual_pv_charge = np . minimum ( schedule , solar_profile ) score = np . sum ( actual_pv_charge ) / np . sum ( schedule ) return score # example: df_pred [ 'pred' ] = post_pred_charge_proc_func ( df_pred [ 'pred' ]) schedule = discharge . sample_random_day ( df_pred [ 'pred' ]) solar_profile = df . loc [ schedule . index ][ 'pv_power_mw' ] print ( \"Score for random day: {} \" . format ( score_charging ( schedule , solar_profile ))) # example: schedule = df_pred [ 'pred' ] solar_profile = df . loc [ schedule . index ][ 'pv_power_mw' ] print ( \"Score for entire dataset: {} \" . format ( score_charging ( schedule , solar_profile ))) Score for random day: 0.6541809834766956 Score for entire dataset: 0.8135381339249057 However remember that some days there is not enough solar PV to fill the battery. It would be good to know what % of the max score we achieved. That is, the sum of our PV charge over the total available PV capacity (capped at 6 MWh per day). #exports def max_available_solar ( solar_profile , max_charge_rate = 2.5 , capacity_mwh = 6 , time_unit = 0.5 ): \"\"\" Return the solar PV potential available to the battery. That is, the total PV potential with a daily cap of 6 MWh. \"\"\" available = solar_profile . clip ( 0 , 2.5 ) . groupby ( solar_profile . index . date ) . sum () * time_unit clipped = np . clip ( available . values , 0 , capacity_mwh ) total = np . sum ( clipped ) return total Now we need a function to evaluate a schedule as a proportion of the max available score. That is, the total PV charge used by the battery divided by the total available solar PV. #exports def prop_max_solar ( schedule , solar_profile , time_unit = 0.5 ): \"\"\" Get the proportion of maximum solar exploitation for charging schedule, given a solar PV profile \"\"\" actual_pv_charge = np . sum ( np . minimum ( schedule , solar_profile ) * time_unit ) max_pv_charge = max_available_solar ( solar_profile ) return actual_pv_charge / max_pv_charge def construct_solar_exploit_calculator ( solar_profile , charging_datetimes = None , scorer = False ): if charging_datetimes is None : charging_datetimes = extract_charging_datetimes ( solar_profile ) def calc_solar_exploitation ( y , y_pred ): # Checking evening datetimes if hasattr ( y_pred , 'index' ) == True : charging_datetimes = extract_charging_datetimes ( y_pred ) assert y_pred . shape [ 0 ] == solar_profile . loc [ charging_datetimes ] . shape [ 0 ], f 'The prediction series must be the same length as the number of evening datetimes in the main dataframe, { y_pred . shape [ 0 ] } { s_demand . loc [ evening_datetimes ] . shape [ 0 ] } ' exploitation_pct = 100 * prop_max_solar ( y_pred , solar_profile . loc [ charging_datetimes ]) return exploitation_pct if scorer == True : return make_scorer ( calc_solar_exploitation ) else : return calc_solar_exploitation # example: df_pred [ 'pred' ] = post_pred_charge_proc_func ( df_pred [ 'pred' ]) schedule = discharge . sample_random_day ( df_pred [ 'pred' ]) solar_profile = df . loc [ schedule . index ][ 'pv_power_mw' ] print ( \"Solar exploitation for random day: {} \" . format ( prop_max_solar ( schedule , solar_profile ))) # example: schedule = df_pred [ 'pred' ] solar_profile = df . loc [ schedule . index ][ 'pv_power_mw' ] print ( \"Solar exploitation for entire dataset: {} \" . format ( prop_max_solar ( schedule , solar_profile ))) Solar exploitation for random day: 0.9995421307529543 Solar exploitation for entire dataset: 0.9570752322597926 Model comparison \u00b6 Now let's try some different models and view their scores and the proportion of maximum PV potential: models = { 'std_linear' : LinearRegression (), 'boosted' : GradientBoostingRegressor (), 'random_forest' : RandomForestRegressor (), } charging_datetimes = extract_charging_datetimes ( X ) solar_exploit_calc = construct_solar_exploit_calculator ( df [ 'pv_power_mw' ], charging_datetimes ) for key in models : df_pred = clean . generate_kfold_preds ( X . values , y . values , models [ key ], index = X . index ) df_pred [ 'pred' ] = post_pred_charge_proc_func ( df_pred [ 'pred' ]) schedule = df_pred [ 'pred' ] solar_profile = df . loc [ schedule . index ][ 'pv_power_mw' ] score = score_charging ( schedule , solar_profile ) exploitation_pct = solar_exploit_calc ( df_pred [ 'true' ], df_pred [ 'pred' ]) print ( f \"Model: ` { key } ` Score: { score : .3f } Proportion of max: { exploitation_pct : .3f } %\" ) Model: `std_linear` Score: 0.788 Proportion of max: 92.685% Model: `boosted` Score: 0.797 Proportion of max: 93.818% Model: `random_forest` Score: 0.814 Proportion of max: 95.769% Final check that the predictions meet the constraints: print ( df_pred [ 'pred' ] . groupby ( df_pred . index . date ) . sum () . value_counts ()) # Should sum to 12 MWh for all days print ( np . max ( df_pred [ 'pred' ])) # Max should not exceed 2.5 MW 12.0 282 12.0 179 12.0 171 12.0 74 12.0 43 12.0 12 12.0 7 Name: pred, dtype: int64 1.8012238069778999 Checking out the average day: average_day = df_pred . pred . astype ( 'float' ) . groupby ( df_pred . index . time ) . mean () . values plt . plot ( average_day ) [<matplotlib.lines.Line2D at 0x25383639640>] We'll also create a wrapper for fitting and saving the model #exports def fit_and_save_charging_model ( X , y , charge_opt_model_fp , model_class = RandomForestRegressor , ** model_params ): model = model_class ( ** model_params ) model . fit ( X , y ) with open ( charge_opt_model_fp , 'wb' ) as fp : joblib . dump ( model , fp ) return Held out set \u00b6 We'll test this on a hold-out set def get_train_test ( arr , start_of_test_period ): train_arr = arr [: pd . to_datetime ( start_of_test_period , utc = True )] test_arr = arr [ pd . to_datetime ( start_of_test_period , utc = True ):] return train_arr , test_arr start_of_test_period = '2019-02-04' X_train , X_test = get_train_test ( X , start_of_test_period ) y_train , y_test = get_train_test ( y , start_of_test_period ) best_model = LinearRegression () best_model . fit ( X_train , y_train ) preds = pd . Series ( best_model . predict ( X_test ), index = X_test . index ) print ( prop_max_solar ( preds , y_test )) 0.7155203185335218 Hyperparameter Tuning \u00b6 We're now ready to tune the hyper-parameters X , y = prepare_training_input_data ( intermediate_data_dir ) charging_datetimes = extract_charging_datetimes ( X ) solar_exploit_scorer = construct_solar_exploit_calculator ( solar_profile = df [ 'pv_power_mw' ], charging_datetimes = charging_datetimes , scorer = True ) groups = charging_datetimes . date pipeline = Pipeline ([ ( 'pandas_RF' , utils . PandasRandomForestRegressor ()) ]) search_spaces = { 'pandas_RF__min_samples_leaf' : Integer ( 1 , 20 , 'uniform' ), 'pandas_RF__criterion' : Categorical ([ 'mse' , 'mae' ]), 'pandas_RF__n_estimators' : Integer ( 10 , 150 , 'uniform' ), 'pandas_RF__max_features' : Categorical ([ 'auto' , 'sqrt' ]), 'pandas_RF__max_depth' : Integer ( 5 , 200 , 'uniform' ), 'pandas_RF__min_samples_split' : Integer ( 2 , 10 , 'uniform' ), 'pandas_RF__min_samples_leaf' : Integer ( 1 , 4 , 'uniform' ), 'pandas_RF__bootstrap' : Categorical ([ True , False ]) } opt = utils . BayesSearchCV ( pipeline , search_spaces , n_iter = 20 , verbose = 1 , cv = 4 , # 8 works well for me as that's how many concurrent workers I can use scoring = solar_exploit_scorer , n_jobs =- 1 ) fit_BayesSearchCV = False if fit_BayesSearchCV == True : opt . fit ( X , y , groups = groups ) print ( f 'validation score: { opt . best_score_ } ' ) print ( f 'test score: { opt . score ( X , y ) } ' ) print ( f 'best params: { opt . best_params_ } ' ) _ = plot_objective ( opt . optimizer_results_ [ 0 ]) plt . show () 2017-11-10 00:00:00+00:00 0.0 2017-11-10 00:30:00+00:00 0.0 2017-11-10 01:00:00+00:00 0.0 2017-11-10 01:30:00+00:00 0.0 2017-11-10 02:00:00+00:00 0.0 ... 2019-12-17 21:30:00+00:00 0.0 2019-12-17 22:00:00+00:00 0.0 2019-12-17 22:30:00+00:00 0.0 2019-12-17 23:00:00+00:00 0.0 2019-12-17 23:30:00+00:00 0.0 Freq: 30T, Name: pv_power_mw, Length: 36864, dtype: float64 We'll now use the tuned values to fit our model model_params = { 'criterion' : 'mse' , 'min_samples_leaf' : 4 , 'min_samples_split' : 2 , 'n_estimators' : 100 , } model = RandomForestRegressor ( ** model_params ) df_pred = clean . generate_kfold_preds ( X . values , y . values , model , index = X . index ) df_pred [ 'pred' ] = post_pred_charge_proc_func ( df_pred [ 'pred' ]) schedule = df_pred [ 'pred' ] solar_profile = df . loc [ schedule . index ][ 'pv_power_mw' ] exploitation_pct = solar_exploit_calc ( df_pred [ 'true' ], df_pred [ 'pred' ]) print ( exploitation_pct ) 95.50297361291675 We'll quickly check the residuals fig , ax = plt . subplots () ax . scatter ( df_pred . true , df_pred . pred , s = 0.1 ) plt . show () Test evaluation \u00b6 #exports def prepare_test_feature_data ( raw_data_dir , intermediate_data_dir , test_start_date = None , test_end_date = None , start_time = '08:00' , end_time = '23:59' ): # Loading input data df_features = ( clean . combine_training_datasets ( intermediate_data_dir ) . interpolate ( limit = 1 ) . pipe ( construct_df_charge_features ) ) # Loading default index (latest submission) if test_end_date is None or test_start_date is None : index = discharge . load_latest_submission_template ( raw_data_dir ) . index else : index = df_features [ test_start_date : test_end_date ] . index # Filtering feature data on submission datetimes df_features = df_features . loc [ index ] . between_time ( start_time , end_time ) return df_features df_submission_template = discharge . load_latest_submission_template ( raw_data_dir ) df_features = prepare_test_feature_data ( raw_data_dir , intermediate_data_dir ) df_features . head () --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-45-d1a30d3ccf60> in <module> 1 df_submission_template = discharge.load_latest_submission_template(raw_data_dir) ----> 2 df_features = prepare_test_feature_data(raw_data_dir, intermediate_data_dir) 3 4 df_features.head() <ipython-input-44-e0ab5bc5a9db> in prepare_test_feature_data(raw_data_dir, intermediate_data_dir, test_start_date, test_end_date, start_time, end_time) 15 16 # Filtering feature data on submission datetimes ---> 17 df_features = df_features.loc[index].between_time(start_time, end_time) 18 19 return df_features ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in __getitem__(self, key) 892 893 maybe_callable = com.apply_if_callable(key, self.obj) --> 894 return self._getitem_axis(maybe_callable, axis=axis) 895 896 def _is_scalar_access(self, key: Tuple): ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in _getitem_axis(self, key, axis) 1110 raise ValueError(\"Cannot index with multidimensional key\") 1111 -> 1112 return self._getitem_iterable(key, axis=axis) 1113 1114 # nested tuple slicing ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in _getitem_iterable(self, key, axis) 1050 1051 # A collection of keys -> 1052 keyarr, indexer = self._get_listlike_indexer(key, axis, raise_missing=False) 1053 return self.obj._reindex_with_indexers( 1054 {axis: [keyarr, indexer]}, copy=True, allow_dups=True ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in _get_listlike_indexer(self, key, axis, raise_missing) 1263 keyarr, indexer, new_indexer = ax._reindex_non_unique(keyarr) 1264 -> 1265 self._validate_read_indexer(keyarr, indexer, axis, raise_missing=raise_missing) 1266 return keyarr, indexer 1267 ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in _validate_read_indexer(self, key, indexer, axis, raise_missing) 1305 if missing == len(indexer): 1306 axis_name = self.obj._get_axis_name(axis) -> 1307 raise KeyError(f\"None of [{key}] are in the [{axis_name}]\") 1308 1309 ax = self.obj._get_axis(axis) KeyError: \"None of [DatetimeIndex(['2020-07-03 00:00:00+00:00', '2020-07-03 00:30:00+00:00',\\n '2020-07-03 01:00:00+00:00', '2020-07-03 01:30:00+00:00',\\n '2020-07-03 02:00:00+00:00', '2020-07-03 02:30:00+00:00',\\n '2020-07-03 03:00:00+00:00', '2020-07-03 03:30:00+00:00',\\n '2020-07-03 04:00:00+00:00', '2020-07-03 04:30:00+00:00',\\n ...\\n '2020-07-09 19:00:00+00:00', '2020-07-09 19:30:00+00:00',\\n '2020-07-09 20:00:00+00:00', '2020-07-09 20:30:00+00:00',\\n '2020-07-09 21:00:00+00:00', '2020-07-09 21:30:00+00:00',\\n '2020-07-09 22:00:00+00:00', '2020-07-09 22:30:00+00:00',\\n '2020-07-09 23:00:00+00:00', '2020-07-09 23:30:00+00:00'],\\n dtype='datetime64[ns, UTC]', name='datetime', length=336, freq=None)] are in the [index]\" #exports def optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , charge_opt_model_fp , test_start_date = None , test_end_date = None , start_time = '08:00' , end_time = '23:59' ): df_features = prepare_test_feature_data ( raw_data_dir , intermediate_data_dir , test_start_date = test_start_date , test_end_date = test_end_date , start_time = start_time , end_time = end_time ) charging_datetimes = extract_charging_datetimes ( df_features ) X_test = df_features . loc [ charging_datetimes ] . values model = discharge . load_trained_model ( charge_opt_model_fp ) charge_profile = model . predict ( X_test ) s_charge_profile = pd . Series ( charge_profile , index = charging_datetimes ) s_charge_profile = s_charge_profile . reindex ( df_features . index ) . fillna ( 0 ) s_charge_profile = post_pred_charge_proc_func ( s_charge_profile ) assert charge_is_valid ( s_charge_profile ), \"Charging profile is invalid\" return s_charge_profile s_charge_profile = optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , charge_opt_model_fp ) print ( f \"Charge is valid: { charge_is_valid ( s_charge_profile ) } \" ) s_charge_profile . plot () Finally we'll export the relevant code to our batopt module","title":"Charging"},{"location":"03-charging/#battery-charging","text":"","title":"Battery Charging"},{"location":"03-charging/#imports","text":"#exports import numpy as np import pandas as pd import os import matplotlib.pyplot as plt import seaborn as sns import joblib from moepy.lowess import quantile_model from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression from sklearn.metrics import make_scorer , r2_score , mean_absolute_error , mean_squared_error from sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor from skopt.plots import plot_objective from skopt.space import Real , Categorical , Integer from batopt import clean , discharge , utils import FEAutils as hlp # Should do some investigation of how the panel temp influences performance","title":"Imports"},{"location":"03-charging/#user-stories","text":"raw_data_dir = '../data/raw' intermediate_data_dir = '../data/intermediate' cache_data_dir = '../data/nb-cache' charge_opt_model_fp = '../models/charge_opt.sav'","title":"User Stories"},{"location":"03-charging/#loading-data","text":"df = clean . combine_training_datasets ( intermediate_data_dir ) . interpolate ( limit = 1 ) df . head () demand pv weather demand_MW irradiance_Wm-2 panel_temp_C pv_power_mw solar_location1 solar_location2 solar_location3 solar_location4 solar_location5 solar_location6 temp_location1 temp_location2 temp_location3 temp_location4 temp_location5 temp_location6 holidays 2015-01-01 00:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.75 9.65 8.83 7.58 11.62 11.22 nan 2015-01-01 00:30:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.83 9.705 8.865 7.6 11.635 11.27 nan 2015-01-01 01:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.91 9.76 8.9 7.62 11.65 11.32 nan 2015-01-01 01:30:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.95 9.78 9 7.615 11.65 11.31 nan 2015-01-01 02:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.99 9.8 9.1 7.61 11.65 11.3 nan df . pv_power_mw . plot () <AxesSubplot:> Correlations between the solar variables: solar_cols = [ c for c in df . columns if 'solar_location' in c ] solar_cols . append ( 'irradiance_Wm-2' ) solar_cols . append ( 'panel_temp_C' ) solar_cols . append ( 'pv_power_mw' ) fig , ax = plt . subplots ( dpi = 250 ) df_solar = df . filter ( solar_cols ) . copy () ax = sns . heatmap ( df_solar . corr (), cmap = 'viridis' ) fig . savefig ( '../img/solar_corrplot.png' ) As in the demand data, estimating the quantiles for the solar PV output: #exports def estimate_daily_solar_quantiles ( x , y , x_pred = np . linspace ( 0 , 23.5 , 100 ), ** model_kwargs ): # Fitting the model df_quantiles = quantile_model ( x , y , x_pred = x_pred , ** model_kwargs ) # Cleaning names and sorting for plotting df_quantiles . columns = [ f 'p { int ( col * 100 ) } ' for col in df_quantiles . columns ] df_quantiles = df_quantiles [ df_quantiles . columns [:: - 1 ]] return df_quantiles dts = df . index . tz_convert ( 'Europe/London' ) x = np . array ( dts . hour + dts . minute / 60 ) y = df [ 'pv_power_mw' ] . values rerun_daily_solar_model = False daily_solar_filename = 'daily_solar_quantile_model_results.csv' if ( rerun_daily_solar_model == True ) or ( daily_solar_filename not in os . listdir ( cache_data_dir )): df_quantiles = estimate_daily_solar_quantiles ( x , y , frac = 0.2 , num_fits = 48 , robust_iters = 3 ) df_quantiles . to_csv ( f ' { cache_data_dir } / { daily_solar_filename } ' ) else : df_quantiles = pd . read_csv ( f ' { cache_data_dir } / { daily_solar_filename } ' , index_col = 'x' ) And plotting x_jittered = x + ( np . random . uniform ( size = len ( x )) - 0.5 ) / 2.5 # Plotting fig , ax = plt . subplots ( dpi = 250 ) ax . scatter ( x_jittered , y , s = 0.2 , color = 'k' , alpha = 0.5 ) df_quantiles . plot ( cmap = 'viridis' , legend = False , ax = ax ) hlp . hide_spines ( ax ) ax . legend ( frameon = False , bbox_to_anchor = ( 1 , 0.9 ), title = 'Percentiles' ) ax . set_xlabel ( 'Time of Day' ) ax . set_ylabel ( 'Demand (MW)' ) ax . set_xlim ( 0 , 24 ) ax . set_ylim ( 0 , 4 ) fig . savefig ( '../img/daily_solar_profile.png' )","title":"Loading Data"},{"location":"03-charging/#proportion-of-days-during-which-we-can-fully-charge-the-battery","text":"It may be useful to know the proportion of days during which the battery can be fully charged. df_solar_hrs = df . between_time ( '00:00:00' , '15:00:00' ) pv_generation = df_solar_hrs . groupby ( df_solar_hrs . index . date ) . sum ()[ 'pv_power_mw' ] * 0.5 # available daily energy from PV fig , ax = plt . subplots () ax . hist ( pv_generation , bins = 20 ) plt . show () prop = np . sum ( pv_generation >= 6 ) / pv_generation . size print ( \"Proportion of days where solar generation exceeds 6 MWh: {:.2f} %\" . format ( prop * 100 )) Proportion of days where solar generation exceeds 6 MWh: 29.85%","title":"Proportion of days during which we can fully charge the battery"},{"location":"03-charging/#optimal-charging-with-perfect-foresight","text":"We will now develop an algorithm to determine the optimal charging schedule given a perfect solar forecast. The scoring function for the generation component rewards us taking as much energy as possible from solar PV. The proportion of energy from PV for a day \\(d\\) is given by \\( \\(p_{d,1} = \\frac{\\sum{P_{d,k}}}{\\sum{B_{d,k}}}\\) \\) where we are summing over all periods \\(k\\) . An equivalent equation is applies for \\(p_{d,2}\\) which is the energy that is drawn from the grid. The scoring function rewards \\(p_{d,1}\\) over \\(p_{d,2}\\) in a ratio of 3 to 1. Any schedule which fully exploits the solar PV potential until the battery is charged is equally good in terms of the scoring function. However, it may be worth considering methods which give a smoother charge profile for the purposes of producing a robust model for unseen days. In addition, we need to have a method of intelligently allocating charge when the solar PV potential is less than the capacity of the battery. Some possible methods for this: Naively reallocate over the middle of they day (say 09:00--15:00) Add charge to periods where charge has already been committed. Use a forecast for PV output and allocate charge proportionally to the forecast. s_pv = df [ 'pv_power_mw' ] . dropna () solar_profile = discharge . sample_random_days ( s_pv ) solar_profile . plot () <AxesSubplot:> For perfect foresight, any schedule that draws all of the available solar power or 6 MWh (if the total solar production exceeds 6 MWh) is equally good. This first approach will aim to draw greedily from until 6 MWh is satisfied, or all of the solar production has been expended. In cases where there is not enough solar PV to fill the battery, we will then uniformly add the remaining capacity across all periods. Note: this seems to work on this dataset but won't if there is a very large spike in solar PV, such topping up uniformly causes a constraint to be violated. It also may not work if the number of periods over which we top up is decreased. #exports def extract_solar_profile ( s_solar_sample_dt , start_time = '00:00' , end_time = '15:00' ): dt = str ( s_solar_sample_dt . index [ 0 ] . date ()) solar_profile = s_solar_sample_dt [ f ' { dt } { start_time } ' : f ' { dt } { end_time } ' ] . values return solar_profile def charge_profile_greedy ( solar_profile , capacity = 6 , initial_charge = 0 , max_charge_rate = 2.5 , time_unit = 0.5 ): order = np . flip ( np . argsort ( solar_profile )) charge = initial_charge solution = np . zeros ( len ( solar_profile )) for i in order : solar_available = np . minimum ( solar_profile [ i ], max_charge_rate ) solar_available = min ( solar_available , ( capacity - charge ) / time_unit ) solution [ i ] = solar_available charge = np . sum ( solution ) * time_unit if charge > capacity : break return solution def topup_charge_naive ( charge_profile , capacity = 6 , time_unit = 0.5 , period_start = 16 , period_end = 30 ): charge = np . sum ( charge_profile ) * time_unit spare_cap = capacity - charge topup_value = spare_cap / (( period_end - period_start ) * time_unit ) new_profile = np . copy ( charge_profile ) new_profile [ period_start : period_end ] += topup_value # Add topup_value uniformly between start and end periods return new_profile def optimal_charge_profile ( solar_profile , capacity = 6 , time_unit = 0.5 , max_charge_rate = 2.5 ): solution = charge_profile_greedy ( solar_profile ) solution = topup_charge_naive ( solution ) assert np . isclose ( np . sum ( solution ), capacity / time_unit ), \"Does not meet capacity constraint\" . format ( np . sum ( solution )) assert np . all ( solution <= max_charge_rate ), \"Does not meet max charge rate constraint. Max is {} \" . format ( np . max ( solution )) return solution random_solar_profile = discharge . sample_random_day ( s_pv ) . pipe ( extract_solar_profile ) x = optimal_charge_profile ( random_solar_profile ) # Note there is sometimes a rounding error here plt . plot ( x ) [<matplotlib.lines.Line2D at 0x25382e34520>] The danger with this method is that it can be quite spiky. I wonder if this (a) makes the function difficult to learn (b) is too risky as compared with hedging bets with a more smoother approach.","title":"Optimal charging with perfect foresight"},{"location":"03-charging/#smooth-approach","text":"We can use the same peak flattening algorithm developed for the dischrge optimisation adj_random_solar_profile = discharge . flatten_peak ( random_solar_profile ) plt . plot ( random_solar_profile ) plt . plot ( adj_random_solar_profile ) [<matplotlib.lines.Line2D at 0x25382e8f160>] Which we can deduct from the original evening profile to construct the charge profile #exports construct_charge_profile = lambda solar_profile , adj_solar_profile : solar_profile - adj_solar_profile charge_profile = construct_charge_profile ( random_solar_profile , adj_random_solar_profile ) plt . plot ( charge_profile ) [<matplotlib.lines.Line2D at 0x25382ed7e80>] Rather than the sample day we've just used we'll now repeat this step for all days we have pv data on, returning a series of the new charge values that can be easily added to the discharge values #exports def construct_charge_s ( s_pv , start_time = '00:00' , end_time = '15:00' ): s_charge = pd . Series ( index = s_pv . index , dtype = float ) . fillna ( 0 ) for dt in s_pv . index . strftime ( '%Y-%m- %d ' ) . unique (): solar_profile = s_pv [ dt ] . pipe ( extract_solar_profile , start_time = start_time , end_time = end_time ) adj_solar_profile = discharge . flatten_peak ( solar_profile ) charge_profile = construct_charge_profile ( solar_profile , adj_solar_profile ) s_charge [ f ' { dt } { start_time } ' : f ' { dt } { end_time } ' ] = charge_profile return s_charge def charge_is_valid ( charge_profile , capacity = 6 , max_charge_rate = 2.5 , time_unit = 0.5 ): \"\"\" Function determining if a charge profile is valid (and fully charges the battery) \"\"\" if np . all ( np . isclose ( capacity / time_unit , charge_profile . groupby ( charge_profile . index . date ) . sum ())) is False : return False elif np . all ( charge_profile . groupby ( charge_profile . index . date ) . max () <= max_charge_rate ) is False : return False else : return True s_charge = construct_charge_s ( s_pv , start_time = '00:00' , end_time = '15:00' ) s_charge . iloc [: 48 * 7 ] . plot () charge_is_valid ( s_charge ) True With the greedy algorithm we can analyse the periods during which charging occurs: s_charge . groupby ( s_charge . index . time ) . mean () . plot () <AxesSubplot:xlabel='time'> Unsurprisingly we never charge before 5am. We can therefore truncate our training to just look at 05:00--15:30. Confirm that the optimal charge adds up to 6 MWh each day: s_charge . groupby ( s_charge . index . date ) . sum () . round ( 10 ) . value_counts () 12.000000 763 12.130526 1 12.160000 1 12.118333 1 12.211000 1 12.240000 1 12.095714 1 12.469091 1 12.081538 1 12.555455 1 12.132308 1 12.235714 1 12.003333 1 12.007100 1 dtype: int64","title":"Smooth Approach"},{"location":"03-charging/#model-development-charging","text":"Following the same structure as battery discharge, we will aim to predict the optimal charge schedule. #exports def construct_df_charge_features ( df , dt_rng = None ): if dt_rng is None : dt_rng = pd . date_range ( df . index . min (), df . index . max (), freq = '30T' ) df_features = pd . DataFrame ( index = dt_rng ) # Filtering for the temperature weather data temp_loc_cols = df . columns [ df . columns . str . contains ( 'temp_location' )] df_features . loc [ df . index , temp_loc_cols ] = df [ temp_loc_cols ] . copy () df_features = df_features . ffill ( limit = 1 ) # Adding lagged solar df_features [ 'pv_7d_lag' ] = df [ 'pv_power_mw' ] . shift ( 48 * 7 ) # Adding solar irradiance data solar_loc_cols = df . columns [ df . columns . str . contains ( 'solar_location' )] df_features . loc [ df . index , solar_loc_cols ] = df [ solar_loc_cols ] . copy () df_features = df_features . ffill ( limit = 1 ) # Adding datetime features dts = df_features . index . tz_convert ( 'Europe/London' ) # We want to use the 'behavioural' timezone df_features [ 'weekend' ] = dts . dayofweek . isin ([ 5 , 6 ]) . astype ( int ) df_features [ 'dow' ] = dts . dayofweek hour = dts . hour + dts . minute / 60 df_features [ 'sin_hour' ] = np . sin ( 2 * np . pi * hour / 24 ) df_features [ 'cos_hour' ] = np . cos ( 2 * np . pi * hour / 24 ) df_features [ 'sin_doy' ] = np . sin ( 2 * np . pi * dts . dayofyear / 365 ) df_features [ 'cos_doy' ] = np . cos ( 2 * np . pi * dts . dayofyear / 365 ) # Removing some extraneous features cols = [ c for c in df_features . columns if 'solar_location4' not in c and 'solar_location1' not in c ] df_features = df_features . filter ( cols ) # Add rolling solar solar_cols = [ c for c in df_features . columns if 'solar_location' in c ] df_features [[ col + '_rolling' for col in solar_cols ]] = df_features . rolling ( 3 ) . mean ()[ solar_cols ] # Add rolling temp temp_cols = [ c for c in df_features . columns if 'temp_location' in c ] df_features [[ col + '_rolling' for col in temp_cols ]] = df_features . rolling ( 3 ) . mean ()[ temp_cols ] # Removing NaN values df_features = df_features . dropna () return df_features #exports def extract_charging_datetimes ( df , start_hour = 4 , end_hour = 15 ): hour = df . index . hour + df . index . minute / 60 charging_datetimes = df . index [( hour >= start_hour ) & ( hour <= end_hour )] return charging_datetimes #exports def prepare_training_input_data ( intermediate_data_dir , start_hour = 4 ): # Loading input data df = clean . combine_training_datasets ( intermediate_data_dir ) . interpolate ( limit = 1 ) df_features = construct_df_charge_features ( df ) # Filtering for overlapping feature and target data dt_idx = pd . date_range ( df_features . index . min (), df [ 'pv_power_mw' ] . dropna () . index . max () - pd . Timedelta ( minutes = 30 ), freq = '30T' ) s_pv = df . loc [ dt_idx , 'pv_power_mw' ] print ( s_pv ) df_features = df_features . loc [ dt_idx ] # Constructing the charge series s_charge = construct_charge_s ( s_pv , start_time = f '0 { start_hour } :00' , end_time = '15:00' ) # Filtering for evening datetimes charging_datetimes = extract_charging_datetimes ( df_features , start_hour = start_hour ) X = df_features . loc [ charging_datetimes ] y = s_charge . loc [ charging_datetimes ] return X , y X , y = prepare_training_input_data ( intermediate_data_dir ) X . shape , y . shape 2017-11-10 00:00:00+00:00 0.0 2017-11-10 00:30:00+00:00 0.0 2017-11-10 01:00:00+00:00 0.0 2017-11-10 01:30:00+00:00 0.0 2017-11-10 02:00:00+00:00 0.0 ... 2019-12-17 21:30:00+00:00 0.0 2019-12-17 22:00:00+00:00 0.0 2019-12-17 22:30:00+00:00 0.0 2019-12-17 23:00:00+00:00 0.0 2019-12-17 23:30:00+00:00 0.0 Freq: 30T, Name: pv_power_mw, Length: 36864, dtype: float64 ((17664, 27), (17664,)) random_day = pd . to_datetime ( np . random . choice ( y . index . date )) plt . plot ( y [ y . index . date == random_day ]) [<matplotlib.lines.Line2D at 0x25382f4b220>] df_pred = clean . generate_kfold_preds ( X . values , y . values , RandomForestRegressor (), index = X . index ) df_pred . head () pred true 2017-11-10 04:00:00+00:00 0.066136 0 2017-11-10 04:30:00+00:00 0.064022 0 2017-11-10 05:00:00+00:00 0.099278 0 2017-11-10 05:30:00+00:00 0.031904 0 2017-11-10 06:00:00+00:00 0.133574 0 plt . scatter ( df_pred . true , df_pred . pred , s = 0.1 ) plt . ylabel ( \"Predicted\" ) plt . xlabel ( \"Actual\" ) Text(0.5, 0, 'Actual') We need to fix the predictions such that they satisfy the battery constraints. We will do this in the same way as applied in the battery discharge component, first clipping the charge rate to be between 0--2.5MW, then normalising such that the total charge sums to 6 MWh. #exports def normalise_total_charge ( s_pred , charge = 6. , time_unit = 0.5 ): s_daily_charge = s_pred . groupby ( s_pred . index . date ) . sum () for date , total_charge in s_daily_charge . items (): with np . errstate ( divide = 'ignore' , invalid = 'ignore' ): s_pred . loc [ str ( date )] *= charge / ( time_unit * total_charge ) return s_pred clip_charge_rate = lambda s_pred , max_rate = 2.5 , min_rate = 0 : s_pred . clip ( lower = min_rate , upper = max_rate ) post_pred_charge_proc_func = lambda s_pred : ( s_pred . pipe ( clip_charge_rate ) . pipe ( normalise_total_charge ) ) post_pred_charge_proc_func ( df_pred [ 'true' ]) . groupby ( df_pred . index . date ) . sum () . value_counts () 12.0 393 12.0 157 12.0 138 12.0 40 12.0 31 12.0 4 12.0 4 12.0 1 Name: true, dtype: int64","title":"Model development: charging"},{"location":"03-charging/#model-comparison-metrics","text":"Schedules are scored according to the proportion of the total battery charge that comes from solar: \\(p_{d,1} = \\frac{\\sum{P_{d,k}}}{\\sum{B_{d,k}}}\\) . We will first write a function which evaluates this scoring function for a charging schedule and solar profile. #exports def score_charging ( schedule , solar_profile ): # The actual pv charge is the minimum of the scheduled charge and the actual solar availability actual_pv_charge = np . minimum ( schedule , solar_profile ) score = np . sum ( actual_pv_charge ) / np . sum ( schedule ) return score # example: df_pred [ 'pred' ] = post_pred_charge_proc_func ( df_pred [ 'pred' ]) schedule = discharge . sample_random_day ( df_pred [ 'pred' ]) solar_profile = df . loc [ schedule . index ][ 'pv_power_mw' ] print ( \"Score for random day: {} \" . format ( score_charging ( schedule , solar_profile ))) # example: schedule = df_pred [ 'pred' ] solar_profile = df . loc [ schedule . index ][ 'pv_power_mw' ] print ( \"Score for entire dataset: {} \" . format ( score_charging ( schedule , solar_profile ))) Score for random day: 0.6541809834766956 Score for entire dataset: 0.8135381339249057 However remember that some days there is not enough solar PV to fill the battery. It would be good to know what % of the max score we achieved. That is, the sum of our PV charge over the total available PV capacity (capped at 6 MWh per day). #exports def max_available_solar ( solar_profile , max_charge_rate = 2.5 , capacity_mwh = 6 , time_unit = 0.5 ): \"\"\" Return the solar PV potential available to the battery. That is, the total PV potential with a daily cap of 6 MWh. \"\"\" available = solar_profile . clip ( 0 , 2.5 ) . groupby ( solar_profile . index . date ) . sum () * time_unit clipped = np . clip ( available . values , 0 , capacity_mwh ) total = np . sum ( clipped ) return total Now we need a function to evaluate a schedule as a proportion of the max available score. That is, the total PV charge used by the battery divided by the total available solar PV. #exports def prop_max_solar ( schedule , solar_profile , time_unit = 0.5 ): \"\"\" Get the proportion of maximum solar exploitation for charging schedule, given a solar PV profile \"\"\" actual_pv_charge = np . sum ( np . minimum ( schedule , solar_profile ) * time_unit ) max_pv_charge = max_available_solar ( solar_profile ) return actual_pv_charge / max_pv_charge def construct_solar_exploit_calculator ( solar_profile , charging_datetimes = None , scorer = False ): if charging_datetimes is None : charging_datetimes = extract_charging_datetimes ( solar_profile ) def calc_solar_exploitation ( y , y_pred ): # Checking evening datetimes if hasattr ( y_pred , 'index' ) == True : charging_datetimes = extract_charging_datetimes ( y_pred ) assert y_pred . shape [ 0 ] == solar_profile . loc [ charging_datetimes ] . shape [ 0 ], f 'The prediction series must be the same length as the number of evening datetimes in the main dataframe, { y_pred . shape [ 0 ] } { s_demand . loc [ evening_datetimes ] . shape [ 0 ] } ' exploitation_pct = 100 * prop_max_solar ( y_pred , solar_profile . loc [ charging_datetimes ]) return exploitation_pct if scorer == True : return make_scorer ( calc_solar_exploitation ) else : return calc_solar_exploitation # example: df_pred [ 'pred' ] = post_pred_charge_proc_func ( df_pred [ 'pred' ]) schedule = discharge . sample_random_day ( df_pred [ 'pred' ]) solar_profile = df . loc [ schedule . index ][ 'pv_power_mw' ] print ( \"Solar exploitation for random day: {} \" . format ( prop_max_solar ( schedule , solar_profile ))) # example: schedule = df_pred [ 'pred' ] solar_profile = df . loc [ schedule . index ][ 'pv_power_mw' ] print ( \"Solar exploitation for entire dataset: {} \" . format ( prop_max_solar ( schedule , solar_profile ))) Solar exploitation for random day: 0.9995421307529543 Solar exploitation for entire dataset: 0.9570752322597926","title":"Model Comparison Metrics"},{"location":"03-charging/#model-comparison","text":"Now let's try some different models and view their scores and the proportion of maximum PV potential: models = { 'std_linear' : LinearRegression (), 'boosted' : GradientBoostingRegressor (), 'random_forest' : RandomForestRegressor (), } charging_datetimes = extract_charging_datetimes ( X ) solar_exploit_calc = construct_solar_exploit_calculator ( df [ 'pv_power_mw' ], charging_datetimes ) for key in models : df_pred = clean . generate_kfold_preds ( X . values , y . values , models [ key ], index = X . index ) df_pred [ 'pred' ] = post_pred_charge_proc_func ( df_pred [ 'pred' ]) schedule = df_pred [ 'pred' ] solar_profile = df . loc [ schedule . index ][ 'pv_power_mw' ] score = score_charging ( schedule , solar_profile ) exploitation_pct = solar_exploit_calc ( df_pred [ 'true' ], df_pred [ 'pred' ]) print ( f \"Model: ` { key } ` Score: { score : .3f } Proportion of max: { exploitation_pct : .3f } %\" ) Model: `std_linear` Score: 0.788 Proportion of max: 92.685% Model: `boosted` Score: 0.797 Proportion of max: 93.818% Model: `random_forest` Score: 0.814 Proportion of max: 95.769% Final check that the predictions meet the constraints: print ( df_pred [ 'pred' ] . groupby ( df_pred . index . date ) . sum () . value_counts ()) # Should sum to 12 MWh for all days print ( np . max ( df_pred [ 'pred' ])) # Max should not exceed 2.5 MW 12.0 282 12.0 179 12.0 171 12.0 74 12.0 43 12.0 12 12.0 7 Name: pred, dtype: int64 1.8012238069778999 Checking out the average day: average_day = df_pred . pred . astype ( 'float' ) . groupby ( df_pred . index . time ) . mean () . values plt . plot ( average_day ) [<matplotlib.lines.Line2D at 0x25383639640>] We'll also create a wrapper for fitting and saving the model #exports def fit_and_save_charging_model ( X , y , charge_opt_model_fp , model_class = RandomForestRegressor , ** model_params ): model = model_class ( ** model_params ) model . fit ( X , y ) with open ( charge_opt_model_fp , 'wb' ) as fp : joblib . dump ( model , fp ) return","title":"Model comparison"},{"location":"03-charging/#held-out-set","text":"We'll test this on a hold-out set def get_train_test ( arr , start_of_test_period ): train_arr = arr [: pd . to_datetime ( start_of_test_period , utc = True )] test_arr = arr [ pd . to_datetime ( start_of_test_period , utc = True ):] return train_arr , test_arr start_of_test_period = '2019-02-04' X_train , X_test = get_train_test ( X , start_of_test_period ) y_train , y_test = get_train_test ( y , start_of_test_period ) best_model = LinearRegression () best_model . fit ( X_train , y_train ) preds = pd . Series ( best_model . predict ( X_test ), index = X_test . index ) print ( prop_max_solar ( preds , y_test )) 0.7155203185335218","title":"Held out set"},{"location":"03-charging/#hyperparameter-tuning","text":"We're now ready to tune the hyper-parameters X , y = prepare_training_input_data ( intermediate_data_dir ) charging_datetimes = extract_charging_datetimes ( X ) solar_exploit_scorer = construct_solar_exploit_calculator ( solar_profile = df [ 'pv_power_mw' ], charging_datetimes = charging_datetimes , scorer = True ) groups = charging_datetimes . date pipeline = Pipeline ([ ( 'pandas_RF' , utils . PandasRandomForestRegressor ()) ]) search_spaces = { 'pandas_RF__min_samples_leaf' : Integer ( 1 , 20 , 'uniform' ), 'pandas_RF__criterion' : Categorical ([ 'mse' , 'mae' ]), 'pandas_RF__n_estimators' : Integer ( 10 , 150 , 'uniform' ), 'pandas_RF__max_features' : Categorical ([ 'auto' , 'sqrt' ]), 'pandas_RF__max_depth' : Integer ( 5 , 200 , 'uniform' ), 'pandas_RF__min_samples_split' : Integer ( 2 , 10 , 'uniform' ), 'pandas_RF__min_samples_leaf' : Integer ( 1 , 4 , 'uniform' ), 'pandas_RF__bootstrap' : Categorical ([ True , False ]) } opt = utils . BayesSearchCV ( pipeline , search_spaces , n_iter = 20 , verbose = 1 , cv = 4 , # 8 works well for me as that's how many concurrent workers I can use scoring = solar_exploit_scorer , n_jobs =- 1 ) fit_BayesSearchCV = False if fit_BayesSearchCV == True : opt . fit ( X , y , groups = groups ) print ( f 'validation score: { opt . best_score_ } ' ) print ( f 'test score: { opt . score ( X , y ) } ' ) print ( f 'best params: { opt . best_params_ } ' ) _ = plot_objective ( opt . optimizer_results_ [ 0 ]) plt . show () 2017-11-10 00:00:00+00:00 0.0 2017-11-10 00:30:00+00:00 0.0 2017-11-10 01:00:00+00:00 0.0 2017-11-10 01:30:00+00:00 0.0 2017-11-10 02:00:00+00:00 0.0 ... 2019-12-17 21:30:00+00:00 0.0 2019-12-17 22:00:00+00:00 0.0 2019-12-17 22:30:00+00:00 0.0 2019-12-17 23:00:00+00:00 0.0 2019-12-17 23:30:00+00:00 0.0 Freq: 30T, Name: pv_power_mw, Length: 36864, dtype: float64 We'll now use the tuned values to fit our model model_params = { 'criterion' : 'mse' , 'min_samples_leaf' : 4 , 'min_samples_split' : 2 , 'n_estimators' : 100 , } model = RandomForestRegressor ( ** model_params ) df_pred = clean . generate_kfold_preds ( X . values , y . values , model , index = X . index ) df_pred [ 'pred' ] = post_pred_charge_proc_func ( df_pred [ 'pred' ]) schedule = df_pred [ 'pred' ] solar_profile = df . loc [ schedule . index ][ 'pv_power_mw' ] exploitation_pct = solar_exploit_calc ( df_pred [ 'true' ], df_pred [ 'pred' ]) print ( exploitation_pct ) 95.50297361291675 We'll quickly check the residuals fig , ax = plt . subplots () ax . scatter ( df_pred . true , df_pred . pred , s = 0.1 ) plt . show ()","title":"Hyperparameter Tuning"},{"location":"03-charging/#test-evaluation","text":"#exports def prepare_test_feature_data ( raw_data_dir , intermediate_data_dir , test_start_date = None , test_end_date = None , start_time = '08:00' , end_time = '23:59' ): # Loading input data df_features = ( clean . combine_training_datasets ( intermediate_data_dir ) . interpolate ( limit = 1 ) . pipe ( construct_df_charge_features ) ) # Loading default index (latest submission) if test_end_date is None or test_start_date is None : index = discharge . load_latest_submission_template ( raw_data_dir ) . index else : index = df_features [ test_start_date : test_end_date ] . index # Filtering feature data on submission datetimes df_features = df_features . loc [ index ] . between_time ( start_time , end_time ) return df_features df_submission_template = discharge . load_latest_submission_template ( raw_data_dir ) df_features = prepare_test_feature_data ( raw_data_dir , intermediate_data_dir ) df_features . head () --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-45-d1a30d3ccf60> in <module> 1 df_submission_template = discharge.load_latest_submission_template(raw_data_dir) ----> 2 df_features = prepare_test_feature_data(raw_data_dir, intermediate_data_dir) 3 4 df_features.head() <ipython-input-44-e0ab5bc5a9db> in prepare_test_feature_data(raw_data_dir, intermediate_data_dir, test_start_date, test_end_date, start_time, end_time) 15 16 # Filtering feature data on submission datetimes ---> 17 df_features = df_features.loc[index].between_time(start_time, end_time) 18 19 return df_features ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in __getitem__(self, key) 892 893 maybe_callable = com.apply_if_callable(key, self.obj) --> 894 return self._getitem_axis(maybe_callable, axis=axis) 895 896 def _is_scalar_access(self, key: Tuple): ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in _getitem_axis(self, key, axis) 1110 raise ValueError(\"Cannot index with multidimensional key\") 1111 -> 1112 return self._getitem_iterable(key, axis=axis) 1113 1114 # nested tuple slicing ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in _getitem_iterable(self, key, axis) 1050 1051 # A collection of keys -> 1052 keyarr, indexer = self._get_listlike_indexer(key, axis, raise_missing=False) 1053 return self.obj._reindex_with_indexers( 1054 {axis: [keyarr, indexer]}, copy=True, allow_dups=True ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in _get_listlike_indexer(self, key, axis, raise_missing) 1263 keyarr, indexer, new_indexer = ax._reindex_non_unique(keyarr) 1264 -> 1265 self._validate_read_indexer(keyarr, indexer, axis, raise_missing=raise_missing) 1266 return keyarr, indexer 1267 ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in _validate_read_indexer(self, key, indexer, axis, raise_missing) 1305 if missing == len(indexer): 1306 axis_name = self.obj._get_axis_name(axis) -> 1307 raise KeyError(f\"None of [{key}] are in the [{axis_name}]\") 1308 1309 ax = self.obj._get_axis(axis) KeyError: \"None of [DatetimeIndex(['2020-07-03 00:00:00+00:00', '2020-07-03 00:30:00+00:00',\\n '2020-07-03 01:00:00+00:00', '2020-07-03 01:30:00+00:00',\\n '2020-07-03 02:00:00+00:00', '2020-07-03 02:30:00+00:00',\\n '2020-07-03 03:00:00+00:00', '2020-07-03 03:30:00+00:00',\\n '2020-07-03 04:00:00+00:00', '2020-07-03 04:30:00+00:00',\\n ...\\n '2020-07-09 19:00:00+00:00', '2020-07-09 19:30:00+00:00',\\n '2020-07-09 20:00:00+00:00', '2020-07-09 20:30:00+00:00',\\n '2020-07-09 21:00:00+00:00', '2020-07-09 21:30:00+00:00',\\n '2020-07-09 22:00:00+00:00', '2020-07-09 22:30:00+00:00',\\n '2020-07-09 23:00:00+00:00', '2020-07-09 23:30:00+00:00'],\\n dtype='datetime64[ns, UTC]', name='datetime', length=336, freq=None)] are in the [index]\" #exports def optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , charge_opt_model_fp , test_start_date = None , test_end_date = None , start_time = '08:00' , end_time = '23:59' ): df_features = prepare_test_feature_data ( raw_data_dir , intermediate_data_dir , test_start_date = test_start_date , test_end_date = test_end_date , start_time = start_time , end_time = end_time ) charging_datetimes = extract_charging_datetimes ( df_features ) X_test = df_features . loc [ charging_datetimes ] . values model = discharge . load_trained_model ( charge_opt_model_fp ) charge_profile = model . predict ( X_test ) s_charge_profile = pd . Series ( charge_profile , index = charging_datetimes ) s_charge_profile = s_charge_profile . reindex ( df_features . index ) . fillna ( 0 ) s_charge_profile = post_pred_charge_proc_func ( s_charge_profile ) assert charge_is_valid ( s_charge_profile ), \"Charging profile is invalid\" return s_charge_profile s_charge_profile = optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , charge_opt_model_fp ) print ( f \"Charge is valid: { charge_is_valid ( s_charge_profile ) } \" ) s_charge_profile . plot () Finally we'll export the relevant code to our batopt module","title":"Test evaluation"},{"location":"04-discharging/","text":"Battery Discharging \u00b6 #exports import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.pipeline import Pipeline from sklearn.model_selection import KFold from sklearn.metrics import make_scorer , r2_score , mean_absolute_error , mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor from statsmodels.tsa.stattools import acf from moepy.lowess import Lowess , quantile_model from batopt import clean , utils import os import random import joblib from ipypb import track import FEAutils as hlp User Inputs \u00b6 raw_data_dir = '../data/raw' intermediate_data_dir = '../data/intermediate' cache_data_dir = '../data/nb-cache' discharge_opt_model_fp = '../models/discharge_opt.sav' Preparing Data \u00b6 We'll start by loading the datasets, we'll interpolate the weather data which is at an hourly granularity df = clean . combine_training_datasets ( intermediate_data_dir ) . interpolate ( limit = 1 ) df . tail () demand pv weather demand_MW irradiance_Wm-2 panel_temp_C pv_power_mw solar_location1 solar_location2 solar_location3 solar_location4 solar_location5 solar_location6 temp_location1 temp_location2 temp_location3 temp_location4 temp_location5 temp_location6 holidays 2019-03-16 21:30:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 6.87 7.035 6.185 6.76 8.595 8.755 0 2019-03-16 22:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 6.73 6.85 6.03 6.19 8.55 8.71 0 2019-03-16 22:30:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 6.575 6.695 5.94 5.97 8.44 8.595 0 2019-03-16 23:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 6.42 6.54 5.85 5.75 8.33 8.48 0 2019-03-16 23:30:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 6.42 6.54 5.85 5.75 8.33 8.48 0 Next we'll construct our features dataframe #exports def construct_df_discharge_features ( df , dt_rng = None ): if dt_rng is None : dt_rng = pd . date_range ( df . index . min (), df . index . max (), freq = '30T' ) df_features = pd . DataFrame ( index = dt_rng ) # Filtering for the temperature weather data temp_loc_cols = df . columns [ df . columns . str . contains ( 'temp_location' )] df_features . loc [ df . index , temp_loc_cols ] = df [ temp_loc_cols ] . copy () df_features = df_features . ffill ( limit = 1 ) df_features [ 'spatial_avg_temp' ] = df_features . mean ( axis = 1 ) # Should look into excluding temp_location5 and temp_location6 df_features [ 'daily_avg_temp' ] = pd . Series ( df_features . index . date , index = df_features . index ) . map ( df_features [ 'spatial_avg_temp' ] . groupby ( df_features . index . date ) . mean () . to_dict ()) # Adding lagged demand df_features [ 'SP_demand_7d_lag' ] = df [ 'demand_MW' ] . shift ( 48 * 7 ) s_evening_demand = df [ 'demand_MW' ] . between_time ( '15:30' , '21:00' ) dt_to_lagged_evening_avg = s_evening_demand . groupby ( s_evening_demand . index . date ) . mean () . shift ( 7 ) . to_dict () dt_to_lagged_evening_max = s_evening_demand . groupby ( s_evening_demand . index . date ) . max () . shift ( 7 ) . to_dict () df_features [ 'evening_demand_avg_7d_lag' ] = pd . Series ( df_features . index . date , index = df_features . index ) . map ( dt_to_lagged_evening_avg ) df_features [ 'evening_demand_max_7d_lag' ] = pd . Series ( df_features . index . date , index = df_features . index ) . map ( dt_to_lagged_evening_max ) # Adding datetime features dts = df_features . index . tz_convert ( 'Europe/London' ) # We want to use the 'behavioural' timezone df_features [ 'weekend' ] = dts . dayofweek . isin ([ 5 , 6 ]) . astype ( int ) df_features [ 'hour' ] = dts . hour + dts . minute / 60 df_features [ 'doy' ] = dts . dayofyear df_features [ 'dow' ] = dts . dayofweek # Removing NaN values df_features = df_features . dropna () return df_features df_features = construct_df_discharge_features ( df ) df_features . tail () temp_location1 temp_location2 temp_location3 temp_location4 temp_location5 temp_location6 spatial_avg_temp daily_avg_temp SP_demand_7d_lag evening_demand_avg_7d_lag evening_demand_max_7d_lag weekend hour doy dow 2019-03-16 21:30:00+00:00 6.87 7.035 6.185 6.76 8.595 8.755 7.36667 9.56998 3.2 3.94833 4.54 1 21.5 75 5 2019-03-16 22:00:00+00:00 6.73 6.85 6.03 6.19 8.55 8.71 7.17667 9.56998 2.97 3.94833 4.54 1 22 75 5 2019-03-16 22:30:00+00:00 6.575 6.695 5.94 5.97 8.44 8.595 7.03583 9.56998 2.69 3.94833 4.54 1 22.5 75 5 2019-03-16 23:00:00+00:00 6.42 6.54 5.85 5.75 8.33 8.48 6.895 9.56998 2.51 3.94833 4.54 1 23 75 5 2019-03-16 23:30:00+00:00 6.42 6.54 5.85 5.75 8.33 8.48 6.895 9.56998 2.33 3.94833 4.54 1 23.5 75 5 We'll now create demand and features time-series with the same date indexes dt_idx = pd . date_range ( df_features . index . min (), df [ 'demand_MW' ] . dropna () . index . max () - pd . Timedelta ( minutes = 30 ), freq = '30T' ) s_demand = df . loc [ dt_idx , 'demand_MW' ] df_features = df_features . loc [ dt_idx ] Exploratory Demand Analysis \u00b6 We'll start by exploring the relationship between time of day and demand, in this instance fitting a quantile LOWESS model to get a probabilistic view of likely loads at specific times of day #exports def estimate_daily_demand_quantiles ( x , y , x_pred = np . linspace ( 0 , 23.5 , 100 ), ** model_kwargs ): # Fitting the model df_quantiles = quantile_model ( x , y , x_pred = x_pred , ** model_kwargs ) # Cleaning names and sorting for plotting df_quantiles . columns = [ f 'p { int ( col * 100 ) } ' for col in df_quantiles . columns ] df_quantiles = df_quantiles [ df_quantiles . columns [:: - 1 ]] return df_quantiles dts = df . index . tz_convert ( 'Europe/London' ) x = dts . hour + dts . minute / 60 y = df [ 'demand_MW' ] . values rerun_daily_demand_model = False daily_demand_filename = 'daily_demand_quantile_model_results.csv' if ( rerun_daily_demand_model == True ) or ( daily_demand_filename not in os . listdir ( cache_data_dir )): df_quantiles = estimate_daily_demand_quantiles ( x , y , frac = 0.2 , num_fits = 48 , robust_iters = 3 ) df_quantiles . to_csv ( f ' { cache_data_dir } / { daily_demand_filename } ' ) else : df_quantiles = pd . read_csv ( f ' { cache_data_dir } / { daily_demand_filename } ' , index_col = 'x' ) df_quantiles . head () ('Unnamed: 0_level_0', 'x') ('p90', 'Unnamed: 1_level_1') ('p80', 'Unnamed: 2_level_1') ('p70', 'Unnamed: 3_level_1') ('p60', 'Unnamed: 4_level_1') ('p50', 'Unnamed: 5_level_1') ('p40', 'Unnamed: 6_level_1') ('p30', 'Unnamed: 7_level_1') ('p20', 'Unnamed: 8_level_1') ('p10', 'Unnamed: 9_level_1') 0 2.79849 2.71229 2.64641 2.56191 2.39701 2.10668 1.88967 1.81977 1.77097 0.237374 2.76187 2.67597 2.60871 2.5275 2.3697 2.09069 1.86875 1.79585 1.74577 0.474747 2.72532 2.63973 2.57116 2.49312 2.34246 2.0744 1.84797 1.77185 1.72058 0.712121 2.68871 2.60348 2.53355 2.45863 2.31533 2.05794 1.82746 1.74759 1.69522 0.949495 2.65212 2.56737 2.49614 2.42416 2.28829 2.04086 1.80704 1.72319 1.6696 We'll now visualise these quantile fits alongside the raw data N.b. the x values have been slightly jittered in order to make their distribution easier to visualise x_jittered = x + ( np . random . uniform ( size = len ( x )) - 0.5 ) / 2.5 # Plotting fig , ax = plt . subplots ( dpi = 250 ) ax . scatter ( x_jittered , y , s = 0.2 , color = 'k' , alpha = 0.5 ) df_quantiles . plot ( cmap = 'viridis' , legend = False , ax = ax ) hlp . hide_spines ( ax ) ax . legend ( frameon = False , bbox_to_anchor = ( 1 , 0.9 ), title = 'Percentiles' ) ax . set_xlabel ( 'Time of Day' ) ax . set_ylabel ( 'Demand (MW)' ) ax . set_xlim ( 0 , 24 ) ax . set_ylim ( 1 , 6 ) fig . savefig ( '../img/daily_demand_profile.png' ) One of the issues with the quantile fit is that it hides the a lot of the spikiness in individual daily profiles, here we'll create a function for randomly sampling days so we can visualise them alongside each other. #exports reset_idx_dt = lambda s , dt = '2020-01-01' : s . index - ( s . index [ 0 ] - pd . to_datetime ( dt , utc = True )) def sample_random_day ( s ): random_dt = random . choice ( s . index . date ) s_sample_dt = s . loc [ str ( random_dt )] return s_sample_dt def sample_random_days ( s , num_days = 5 ): df_sample_dts = pd . DataFrame () for _ in range ( num_days ): s_sample_dt = sample_random_day ( s ) dt = str ( s_sample_dt . index [ 0 ] . date ()) s_sample_dt . index = reset_idx_dt ( s_sample_dt ) df_sample_dts [ dt ] = s_sample_dt df_sample_dts = df_sample_dts . sort_index ( axis = 1 ) return df_sample_dts df_sample_dts = sample_random_days ( s_demand ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) df_sample_dts . plot ( ax = ax ) ax . legend ( frameon = False ) hlp . hide_spines ( ax ) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Demand (MW)' ) _ = plt . setp ( ax . get_xmajorticklabels (), visible = False ) We'll also check the auto-correlation of the demand time-series, particularly in regard to the the correlation of the most recent value with the value from one week prior (what will be available for the test data) acf_array = acf ( s_demand , fft = True , nlags = 48 * 7 * 2 ) day_blocks = [ 0 ] + list ( np . array ([[ i + 1 ] * 48 for i in range ( 14 )]) . reshape ( - 1 )) s_acf_days_max = pd . Series ( acf_array ) . groupby ( day_blocks ) . max () corr_with_last_weeks_SP = s_acf_days_max . loc [ 7 ] # Plotting fig , ax = plt . subplots () s_acf_days_max . plot . bar ( ax = ax ) ax . plot ([ - 0.5 , 14.5 ], [ corr_with_last_weeks_SP , corr_with_last_weeks_SP ], 'k--' ) ax . set_ylim ( 0.7 , 1 ) ax . set_xlabel ( 'Day' ) ax . set_ylabel ( 'Correlation' ) hlp . hide_spines ( ax ) We'll also fit a quantile model for the relationship between temperature and demand x = df [ df . columns [ df . columns . str . contains ( 'temp_location' )]] . mean ( axis = 1 ) . values y = df [ 'demand_MW' ] . values rerun_temp_demand_model = False temp_demand_filename = 'temp_demand_quantile_model_results.csv' if ( rerun_temp_demand_model == True ) or ( temp_demand_filename not in os . listdir ( cache_data_dir )): df_quantiles = estimate_daily_demand_quantiles ( x , y , frac = 0.35 , num_fits = 48 , robust_iters = 5 ) df_quantiles . to_csv ( f ' { cache_data_dir } / { temp_demand_filename } ' ) else : df_quantiles = pd . read_csv ( f ' { cache_data_dir } / { temp_demand_filename } ' , index_col = 'x' ) df_quantiles . head () ('Unnamed: 0_level_0', 'x') ('p90', 'Unnamed: 1_level_1') ('p80', 'Unnamed: 2_level_1') ('p70', 'Unnamed: 3_level_1') ('p60', 'Unnamed: 4_level_1') ('p50', 'Unnamed: 5_level_1') ('p40', 'Unnamed: 6_level_1') ('p30', 'Unnamed: 7_level_1') ('p20', 'Unnamed: 8_level_1') ('p10', 'Unnamed: 9_level_1') 0 4.97915 4.54269 4.2198 3.87288 3.37484 2.90171 2.70959 2.59651 2.46055 0.237374 4.96727 4.52208 4.20217 3.86098 3.37012 2.89377 2.69721 2.58177 2.44486 0.474747 4.95515 4.50144 4.18453 3.84911 3.36556 2.88588 2.685 2.56718 2.42925 0.712121 4.94284 4.48071 4.16686 3.83725 3.3611 2.87787 2.67285 2.55265 2.41369 0.949495 4.93029 4.45984 4.14914 3.82542 3.35674 2.86969 2.66073 2.53816 2.39816 Which we'll visualise similarly to the daily demand profile fig , ax = plt . subplots ( dpi = 250 ) ax . scatter ( x , y , s = 0.2 , color = 'k' , alpha = 0.5 ) df_quantiles . plot ( cmap = 'viridis' , legend = False , ax = ax ) hlp . hide_spines ( ax ) ax . legend ( frameon = False , bbox_to_anchor = ( 1 , 0.25 ), title = 'Percentiles' ) ax . set_xlabel ( 'Temperature (degC)' ) ax . set_ylabel ( 'Demand (MW)' ) ax . set_xlim ( 0 , 24 ) ax . set_ylim ( 1 , 6 ) fig . savefig ( '../img/temp_demand_profile.png' ) # ^ Should use daily average (or evening block) instead of each SP Strategy Development with Perfect Foresight \u00b6 Here we'll develop a charging strategy for when we have perfect foresight, starting by sampling a random day from the demand series and then extracting the evening profile as an array from that #exports def extract_evening_demand_profile ( s_demand_sample_dt , start_time = '15:30' , end_time = '20:30' ): dt = str ( s_demand_sample_dt . index [ 0 ] . date ()) evening_demand_profile = s_demand_sample_dt [ f ' { dt } { start_time } ' : f ' { dt } { end_time } ' ] . values return evening_demand_profile evening_demand_profile = sample_random_day ( s_demand ) . pipe ( extract_evening_demand_profile ) plt . plot ( evening_demand_profile ) [<matplotlib.lines.Line2D at 0x2051e5fe790>] We'll then write an algorithm for peak flattening #exports def flatten_peak ( evening_demand_profile , charge = 6 , time_unit = 0.5 ): peak = max ( evening_demand_profile ) adj_evening_demand_profile = evening_demand_profile . copy () while charge > 0 : num_periods_plateaued = ( evening_demand_profile >= peak ) . sum () # If the evening demand profile has been fully flattened # then split up the remaining charge equally across all SPs fully_flattened = len ( set ( adj_evening_demand_profile )) == 1 if fully_flattened == True : remaining_discharge_rate_for_each_SP = ( 1 / time_unit ) * charge / len ( adj_evening_demand_profile ) adj_evening_demand_profile -= remaining_discharge_rate_for_each_SP charge = 0 break # If there is still a peak then determine the next highest value else : peak = max ( adj_evening_demand_profile ) highest_non_peak = max ( adj_evening_demand_profile [ peak > adj_evening_demand_profile ]) proposed_additional_discharge = time_unit * ( adj_evening_demand_profile . sum () - np . minimum ( adj_evening_demand_profile , highest_non_peak ) . sum ()) # if its possible to reduce the peak to the next highest value do so if charge >= proposed_additional_discharge : adj_evening_demand_profile = np . minimum ( adj_evening_demand_profile , highest_non_peak ) charge -= proposed_additional_discharge # If the capacity constraints are broken when reducing to the next # highest value then just lower the current peak as far as possible else : new_peak = peak - (( 1 / time_unit ) * charge / ( num_periods_plateaued + 1 )) adj_evening_demand_profile = np . minimum ( adj_evening_demand_profile , new_peak ) charge = 0 return adj_evening_demand_profile adj_evening_demand_profile = flatten_peak ( evening_demand_profile ) plt . plot ( evening_demand_profile ) plt . plot ( adj_evening_demand_profile ) [<matplotlib.lines.Line2D at 0x2051e599f70>] Which we can deduct from the original evening profile to construct the discharge profile #exports construct_discharge_profile = lambda evening_demand_profile , adj_evening_demand_profile : - ( evening_demand_profile - adj_evening_demand_profile ) discharge_profile = construct_discharge_profile ( evening_demand_profile , adj_evening_demand_profile ) plt . plot ( discharge_profile ) [<matplotlib.lines.Line2D at 0x2051e53a490>] Rather than the sample day we've just used we'll now repeat this step for all days we have demand data on, returning a series of the new discharge values that can be easily added to the charging values #exports def construct_discharge_s ( s_demand , start_time = '15:30' , end_time = '20:30' ): s_discharge = pd . Series ( index = s_demand . index , dtype = float ) . fillna ( 0 ) for dt in s_demand . index . strftime ( '%Y-%m- %d ' ) . unique (): evening_demand_profile = s_demand [ dt ] . pipe ( extract_evening_demand_profile ) adj_evening_demand_profile = flatten_peak ( evening_demand_profile ) discharge_profile = construct_discharge_profile ( evening_demand_profile , adj_evening_demand_profile ) s_discharge [ f ' { dt } { start_time } ' : f ' { dt } { end_time } ' ] = discharge_profile return s_discharge s_discharge = construct_discharge_s ( s_demand , start_time = '15:30' , end_time = '20:30' ) s_discharge . iloc [: 48 * 7 ] . plot () <AxesSubplot:> We can also use this discharging profile to see what the new peaks look like s_demand . iloc [: 48 * 7 ] . plot () ( s_demand + s_discharge ) . iloc [: 48 * 7 ] . plot () <AxesSubplot:> Strategy Development under Uncertainty \u00b6 Our overall approach can be thought of as follows: Generate an optimal discharge profile under perfect foresight Train a regression model to emulate the optimal discharge profile Clean profile to ensure that constraints aren't broken and the full 6 MWh is fully utilised We've generated our optimal discharge profile, now we're ready to train the model. We'll first split up our X and y values, filtering only for those that fall into the evening period #exports def extract_evening_datetimes ( df ): hour = df . index . hour + df . index . minute / 60 evening_datetimes = df . index [( 20.5 >= hour ) & ( 15.5 <= hour )] return evening_datetimes evening_datetimes = extract_evening_datetimes ( df_features ) X = df_features . loc [ evening_datetimes ] . values y = s_discharge . loc [ evening_datetimes ] . values We'll create a basic prediction using a standard linear model df_pred = clean . generate_kfold_preds ( X , y , LinearRegression (), index = evening_datetimes ) df_pred . head () pred true 2017-11-10 15:30:00+00:00 -0.300207 -0.125455 2017-11-10 16:00:00+00:00 -0.554003 -0.565455 2017-11-10 16:30:00+00:00 -1.03486 -1.12546 2017-11-10 17:00:00+00:00 -1.55369 -1.58546 2017-11-10 17:30:00+00:00 -1.65075 -1.66545 However, in this approach there's nothing to enforce the battery constraints, namely maximum total discharge and instantaneous discharge rate. This becomes apparant when we visualise the distribution of total discharge volumes each evening. s_daily_discharge = df_pred [ 'pred' ] . groupby ( df_pred . index . date ) . sum () sns . distplot ( s_daily_discharge ) C:\\Users\\Ayrto\\anaconda3\\envs\\batopt\\lib\\site-packages\\seaborn\\distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) <AxesSubplot:xlabel='pred', ylabel='Density'> To account for this we can normalise each daily discharge profile by the ratio between the current total discharge and the maximum current discharge #exports def normalise_total_discharge ( s_pred , charge = 6 , time_unit = 0.5 ): s_daily_discharge = s_pred . groupby ( s_pred . index . date ) . sum () for date , total_discharge in s_daily_discharge . items (): s_pred . loc [ str ( date )] *= - charge / ( time_unit * total_discharge ) return s_pred s_daily_discharge = ( df_pred [ 'pred' ] . pipe ( normalise_total_discharge ) . groupby ( df_pred . index . date ) . sum () . round ( 10 ) ) s_daily_discharge . value_counts () -12.0 485 Name: pred, dtype: int64 We also need to ensure that the discharge rate remains within the bounds of the problem definition, i.e. no greater than -2.5 MW #exports clip_discharge_rate = lambda s_pred , max_rate =- 2.5 , min_rate = 0 : s_pred . clip ( lower = max_rate , upper = min_rate ) s_pred = df_pred [ 'pred' ] . pipe ( clip_discharge_rate ) s_pred . head () 2017-11-10 15:30:00+00:00 -0.297164 2017-11-10 16:00:00+00:00 -0.548387 2017-11-10 16:30:00+00:00 -1.024373 2017-11-10 17:00:00+00:00 -1.537942 2017-11-10 17:30:00+00:00 -1.634021 Name: pred, dtype: object We'll now combine these post prediction processing steps into a single function, ready to use in our model evaluation Note that the normalisation must come after the clipping. Otherwise the total charge constraint can be violated if the model predicts a discharge > 0 #exports post_pred_discharge_proc_func = lambda s_pred : ( s_pred . pipe ( clip_discharge_rate ) . pipe ( normalise_total_discharge ) ) post_pred_discharge_proc_func ( s_pred ) . groupby ( s_pred . index . date ) . sum () . round ( 10 ) . value_counts () -12.0 485 Name: pred, dtype: int64 We'll create a new function that evaluates our discharge profile in terms of the peak reduction achieved relative the reduction using an optimal discharge profile. We'll then use this and our standard mae and rmse metrics to evaluate some different models. #exports def construct_peak_reduction_calculator ( s_demand , evening_datetimes = None , scorer = False ): if evening_datetimes is None : evening_datetimes = extract_evening_datetimes ( s_demand ) def calc_peak_reduction ( y , y_pred ): # Checking evening datetimes if hasattr ( y_pred , 'index' ) == True : evening_datetimes = extract_evening_datetimes ( y_pred ) assert y_pred . shape [ 0 ] == s_demand . loc [ evening_datetimes ] . shape [ 0 ], f 'The prediction series must be the same length as the number of evening datetimes in the main dataframe, { y_pred . shape [ 0 ] } { s_demand . loc [ evening_datetimes ] . shape [ 0 ] } ' # Post-processing the discharge profile to handle constraints y_pred = post_pred_discharge_proc_func ( y_pred ) # Identifying daily peaks s_old_peaks = s_demand . loc [ evening_datetimes ] . groupby ( evening_datetimes . date ) . max () s_new_peaks = ( s_demand . loc [ evening_datetimes ] + y_pred ) . groupby ( evening_datetimes . date ) . max () s_optimal_peaks = ( s_demand . loc [ evening_datetimes ] + y ) . groupby ( evening_datetimes . date ) . max () # Calculating the peak reduction s_new_pct_peak_reduction = 100 * ( s_old_peaks - s_new_peaks ) / s_old_peaks s_optimal_pct_peak_reduction = 100 * ( s_old_peaks - s_optimal_peaks ) / s_old_peaks # after cleaning anomalous demand data should add an assert to check for non finite values pct_of_max_possible_reduction = 100 * ( s_new_pct_peak_reduction . replace ( np . inf , np . nan ) . dropna () . mean () / s_optimal_pct_peak_reduction . replace ( np . inf , np . nan ) . dropna () . mean ()) return pct_of_max_possible_reduction if scorer == True : return make_scorer ( calc_peak_reduction ) else : return calc_peak_reduction def evaluate_discharge_models ( df , models , features_kwargs = {}): df_features = construct_df_discharge_features ( df , ** features_kwargs ) s_discharge = construct_discharge_s ( df [ 'demand_MW' ], start_time = '15:30' , end_time = '20:30' ) evening_datetimes = extract_evening_datetimes ( df_features ) X = df_features . loc [ evening_datetimes ] . values y = s_discharge . loc [ evening_datetimes ] . values model_scores = dict () peak_reduction_calc = construct_peak_reduction_calculator ( s_demand = df [ 'demand_MW' ], evening_datetimes = evening_datetimes ) for model_name , model in track ( models . items ()): df_pred = clean . generate_kfold_preds ( X , y , model , index = evening_datetimes ) df_pred [ 'pred' ] = post_pred_discharge_proc_func ( df_pred [ 'pred' ]) model_scores [ model_name ] = { 'pct_optimal_reduction' : peak_reduction_calc ( df_pred [ 'true' ], df_pred [ 'pred' ]), 'optimal_discharge_mae' : mean_absolute_error ( df_pred [ 'true' ], df_pred [ 'pred' ]), 'optimal_discharge_rmse' : np . sqrt ( mean_squared_error ( df_pred [ 'true' ], df_pred [ 'pred' ])) } df_model_scores = pd . DataFrame ( model_scores ) df_model_scores . index . name = 'metric' df_model_scores . columns . name = 'model' return df_model_scores models = { 'std_linear' : LinearRegression (), 'random_forest' : RandomForestRegressor (), 'boosted' : GradientBoostingRegressor () } rerun_discharge_opt_model = False discharge_opt_filename = 'discharge_optimisation_model_results.csv' if ( rerun_discharge_opt_model == True ) or ( discharge_opt_filename not in os . listdir ( cache_data_dir )): df_model_scores = evaluate_discharge_models ( df . loc [ df_features . index ], models ) df_model_scores . to_csv ( f ' { cache_data_dir } / { discharge_opt_filename } ' ) else : df_model_scores = pd . read_csv ( f ' { cache_data_dir } / { discharge_opt_filename } ' , index_col = 'metric' ) df_model_scores ('Unnamed: 0_level_0', 'metric') ('std_linear', 'Unnamed: 1_level_1') ('random_forest', 'Unnamed: 2_level_1') ('boosted', 'Unnamed: 3_level_1') pct_optimal_reduction 85.6542 87.98 86.787 optimal_discharge_mae 0.121212 0.092669 0.100329 optimal_discharge_rmse 0.16245 0.121869 0.131338 We'll then generate a prediction time-series using the best performing model rerun_discharge_pred_model = False discharge_pred_filename = 'discharge_optimisation_model_pred.csv' if ( rerun_discharge_pred_model == True ) or ( discharge_pred_filename not in os . listdir ( cache_data_dir )): top_model = df_model_scores . T [ 'pct_optimal_reduction' ] . idxmax () df_pred = clean . generate_kfold_preds ( X , y , models [ top_model ], index = evening_datetimes ) df_pred [ 'pred' ] = post_pred_discharge_proc_func ( df_pred [ 'pred' ]) df_pred . to_csv ( f ' { cache_data_dir } / { discharge_pred_filename } ' ) else : df_pred = pd . read_csv ( f ' { cache_data_dir } / { discharge_pred_filename } ' ) df_pred [ 'datetime' ] = pd . to_datetime ( df_pred [ 'datetime' ], utc = True ) df_pred = df_pred . set_index ( 'datetime' ) df_pred . head () ('Unnamed: 0_level_0', 'datetime') ('pred', 'Unnamed: 1_level_1') ('true', 'Unnamed: 2_level_1') 2017-11-10 15:30:00+00:00 -0.213378 -0.125455 2017-11-10 16:00:00+00:00 -0.735541 -0.565455 2017-11-10 16:30:00+00:00 -1.19438 -1.12546 2017-11-10 17:00:00+00:00 -1.57704 -1.58546 2017-11-10 17:30:00+00:00 -1.61426 -1.66545 We'll quickly check the residuals time-series for model-drift s_residuals = df_pred . diff ( 1 , axis = 1 ) . dropna ( axis = 1 ) . iloc [:, 0 ] s_residuals . plot ( linewidth = 0.3 ) <AxesSubplot:xlabel='datetime'> As well as the scatter-plot between the true and estimated optimal charging rates plt . scatter ( df_pred [ 'true' ], df_pred [ 'pred' ], s = 1 ) plt . xlabel ( 'Obervation' ) plt . ylabel ( 'Prediction' ) Text(0, 0.5, 'Prediction') Pipeline Integration Helpers \u00b6 #exports def prepare_training_input_data ( intermediate_data_dir ): # Loading input data df = clean . combine_training_datasets ( intermediate_data_dir ) . interpolate ( limit = 1 ) df_features = construct_df_discharge_features ( df ) # Filtering for overlapping feature and target data dt_idx = pd . date_range ( df_features . index . min (), df [ 'demand_MW' ] . dropna () . index . max () - pd . Timedelta ( minutes = 30 ), freq = '30T' ) s_demand = df . loc [ dt_idx , 'demand_MW' ] df_features = df_features . loc [ dt_idx ] # Constructing the discharge series s_discharge = construct_discharge_s ( s_demand , start_time = '15:30' , end_time = '20:30' ) # Filtering for evening datetimes evening_datetimes = extract_evening_datetimes ( df_features ) X = df_features . loc [ evening_datetimes ] y = s_discharge . loc [ evening_datetimes ] return X , y X , y = prepare_training_input_data ( intermediate_data_dir ) X . shape , y . shape ((5335, 15), (5335,)) #exports def fit_and_save_model ( X , y , discharge_opt_model_fp , model_class = RandomForestRegressor , ** model_params ): model = model_class ( ** model_params ) model . fit ( X , y ) with open ( discharge_opt_model_fp , 'wb' ) as fp : joblib . dump ( model , fp ) return %% time fit_and_save_model ( X , y , discharge_opt_model_fp ) Wall time: 6.65 s #exports def load_trained_model ( discharge_opt_model_fp ): with open ( discharge_opt_model_fp , 'rb' ) as fp : model = joblib . load ( fp ) return model %% time model = load_trained_model ( discharge_opt_model_fp ) model Wall time: 176 ms RandomForestRegressor() #exports def load_latest_submission_template ( raw_data_dir , latest_submission_template_name = None ): if latest_submission_template_name is None : latest_submission_template_name = max ([ filename for filename in os . listdir ( raw_data_dir ) if 'teamname_set' in filename ]) df_submission_template = pd . read_csv ( f ' { raw_data_dir } / { latest_submission_template_name } ' ) df_submission_template [ 'datetime' ] = pd . to_datetime ( df_submission_template [ 'datetime' ], utc = True ) df_submission_template = df_submission_template . set_index ( 'datetime' ) return df_submission_template def prepare_test_feature_data ( raw_data_dir , intermediate_data_dir , test_start_date = None , test_end_date = None ): # Loading input data df_features = ( clean . combine_training_datasets ( intermediate_data_dir ) . interpolate ( limit = 1 ) . pipe ( construct_df_discharge_features ) ) # Loading default index (latest submission) if test_end_date is None or test_start_date is None : index = load_latest_submission_template ( raw_data_dir ) . index else : index = df_features [ test_start_date : test_end_date ] . index # Filtering feature data on submission datetimes df_features = df_features . loc [ index ] return df_features df_features = prepare_test_feature_data ( raw_data_dir , intermediate_data_dir ) df_features . head () ('Unnamed: 0_level_0', 'datetime') ('temp_location1', 'Unnamed: 1_level_1') ('temp_location2', 'Unnamed: 2_level_1') ('temp_location3', 'Unnamed: 3_level_1') ('temp_location4', 'Unnamed: 4_level_1') ('temp_location5', 'Unnamed: 5_level_1') ('temp_location6', 'Unnamed: 6_level_1') ('spatial_avg_temp', 'Unnamed: 7_level_1') ('daily_avg_temp', 'Unnamed: 8_level_1') ('SP_demand_7d_lag', 'Unnamed: 9_level_1') ('evening_demand_avg_7d_lag', 'Unnamed: 10_level_1') ('evening_demand_max_7d_lag', 'Unnamed: 11_level_1') ('weekend', 'Unnamed: 12_level_1') ('hour', 'Unnamed: 13_level_1') ('doy', 'Unnamed: 14_level_1') ('dow', 'Unnamed: 15_level_1') 2019-03-10 00:00:00+00:00 9.69 8.98 7.01 5.83 11.59 11.22 9.05333 8.16849 2.43 4.22667 4.85 1 0 69 6 2019-03-10 00:30:00+00:00 10.45 9.91 7.74 5.745 11.8 11.425 9.51167 8.16849 2.4 4.22667 4.85 1 0.5 69 6 2019-03-10 01:00:00+00:00 11.21 10.84 8.47 5.66 12.01 11.63 9.97 8.16849 2.28 4.22667 4.85 1 1 69 6 2019-03-10 01:30:00+00:00 11.225 11.08 9.57 5.785 12.075 11.69 10.2375 8.16849 2.11 4.22667 4.85 1 1.5 69 6 2019-03-10 02:00:00+00:00 11.24 11.32 10.67 5.91 12.14 11.75 10.505 8.16849 2.03 4.22667 4.85 1 2 69 6 #exports def optimise_test_discharge_profile ( raw_data_dir , intermediate_data_dir , discharge_opt_model_fp , test_start_date = None , test_end_date = None ): df_features = prepare_test_feature_data ( raw_data_dir , intermediate_data_dir , test_start_date = test_start_date , test_end_date = test_end_date ) evening_datetimes = extract_evening_datetimes ( df_features ) X_test = df_features . loc [ evening_datetimes ] . values model = load_trained_model ( discharge_opt_model_fp ) discharge_profile = model . predict ( X_test ) s_discharge_profile = pd . Series ( discharge_profile , index = evening_datetimes ) s_discharge_profile = s_discharge_profile . reindex ( df_features . index ) . fillna ( 0 ) s_discharge_profile = post_pred_discharge_proc_func ( s_discharge_profile ) return s_discharge_profile s_discharge_profile = optimise_test_discharge_profile ( raw_data_dir , intermediate_data_dir , discharge_opt_model_fp ) s_discharge_profile . plot () <AxesSubplot:xlabel='datetime'> Finally we'll export the relevant code to our batopt module","title":"Discharging"},{"location":"04-discharging/#battery-discharging","text":"#exports import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.pipeline import Pipeline from sklearn.model_selection import KFold from sklearn.metrics import make_scorer , r2_score , mean_absolute_error , mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor from statsmodels.tsa.stattools import acf from moepy.lowess import Lowess , quantile_model from batopt import clean , utils import os import random import joblib from ipypb import track import FEAutils as hlp","title":"Battery Discharging"},{"location":"04-discharging/#user-inputs","text":"raw_data_dir = '../data/raw' intermediate_data_dir = '../data/intermediate' cache_data_dir = '../data/nb-cache' discharge_opt_model_fp = '../models/discharge_opt.sav'","title":"User Inputs"},{"location":"04-discharging/#preparing-data","text":"We'll start by loading the datasets, we'll interpolate the weather data which is at an hourly granularity df = clean . combine_training_datasets ( intermediate_data_dir ) . interpolate ( limit = 1 ) df . tail () demand pv weather demand_MW irradiance_Wm-2 panel_temp_C pv_power_mw solar_location1 solar_location2 solar_location3 solar_location4 solar_location5 solar_location6 temp_location1 temp_location2 temp_location3 temp_location4 temp_location5 temp_location6 holidays 2019-03-16 21:30:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 6.87 7.035 6.185 6.76 8.595 8.755 0 2019-03-16 22:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 6.73 6.85 6.03 6.19 8.55 8.71 0 2019-03-16 22:30:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 6.575 6.695 5.94 5.97 8.44 8.595 0 2019-03-16 23:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 6.42 6.54 5.85 5.75 8.33 8.48 0 2019-03-16 23:30:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 6.42 6.54 5.85 5.75 8.33 8.48 0 Next we'll construct our features dataframe #exports def construct_df_discharge_features ( df , dt_rng = None ): if dt_rng is None : dt_rng = pd . date_range ( df . index . min (), df . index . max (), freq = '30T' ) df_features = pd . DataFrame ( index = dt_rng ) # Filtering for the temperature weather data temp_loc_cols = df . columns [ df . columns . str . contains ( 'temp_location' )] df_features . loc [ df . index , temp_loc_cols ] = df [ temp_loc_cols ] . copy () df_features = df_features . ffill ( limit = 1 ) df_features [ 'spatial_avg_temp' ] = df_features . mean ( axis = 1 ) # Should look into excluding temp_location5 and temp_location6 df_features [ 'daily_avg_temp' ] = pd . Series ( df_features . index . date , index = df_features . index ) . map ( df_features [ 'spatial_avg_temp' ] . groupby ( df_features . index . date ) . mean () . to_dict ()) # Adding lagged demand df_features [ 'SP_demand_7d_lag' ] = df [ 'demand_MW' ] . shift ( 48 * 7 ) s_evening_demand = df [ 'demand_MW' ] . between_time ( '15:30' , '21:00' ) dt_to_lagged_evening_avg = s_evening_demand . groupby ( s_evening_demand . index . date ) . mean () . shift ( 7 ) . to_dict () dt_to_lagged_evening_max = s_evening_demand . groupby ( s_evening_demand . index . date ) . max () . shift ( 7 ) . to_dict () df_features [ 'evening_demand_avg_7d_lag' ] = pd . Series ( df_features . index . date , index = df_features . index ) . map ( dt_to_lagged_evening_avg ) df_features [ 'evening_demand_max_7d_lag' ] = pd . Series ( df_features . index . date , index = df_features . index ) . map ( dt_to_lagged_evening_max ) # Adding datetime features dts = df_features . index . tz_convert ( 'Europe/London' ) # We want to use the 'behavioural' timezone df_features [ 'weekend' ] = dts . dayofweek . isin ([ 5 , 6 ]) . astype ( int ) df_features [ 'hour' ] = dts . hour + dts . minute / 60 df_features [ 'doy' ] = dts . dayofyear df_features [ 'dow' ] = dts . dayofweek # Removing NaN values df_features = df_features . dropna () return df_features df_features = construct_df_discharge_features ( df ) df_features . tail () temp_location1 temp_location2 temp_location3 temp_location4 temp_location5 temp_location6 spatial_avg_temp daily_avg_temp SP_demand_7d_lag evening_demand_avg_7d_lag evening_demand_max_7d_lag weekend hour doy dow 2019-03-16 21:30:00+00:00 6.87 7.035 6.185 6.76 8.595 8.755 7.36667 9.56998 3.2 3.94833 4.54 1 21.5 75 5 2019-03-16 22:00:00+00:00 6.73 6.85 6.03 6.19 8.55 8.71 7.17667 9.56998 2.97 3.94833 4.54 1 22 75 5 2019-03-16 22:30:00+00:00 6.575 6.695 5.94 5.97 8.44 8.595 7.03583 9.56998 2.69 3.94833 4.54 1 22.5 75 5 2019-03-16 23:00:00+00:00 6.42 6.54 5.85 5.75 8.33 8.48 6.895 9.56998 2.51 3.94833 4.54 1 23 75 5 2019-03-16 23:30:00+00:00 6.42 6.54 5.85 5.75 8.33 8.48 6.895 9.56998 2.33 3.94833 4.54 1 23.5 75 5 We'll now create demand and features time-series with the same date indexes dt_idx = pd . date_range ( df_features . index . min (), df [ 'demand_MW' ] . dropna () . index . max () - pd . Timedelta ( minutes = 30 ), freq = '30T' ) s_demand = df . loc [ dt_idx , 'demand_MW' ] df_features = df_features . loc [ dt_idx ]","title":"Preparing Data"},{"location":"04-discharging/#exploratory-demand-analysis","text":"We'll start by exploring the relationship between time of day and demand, in this instance fitting a quantile LOWESS model to get a probabilistic view of likely loads at specific times of day #exports def estimate_daily_demand_quantiles ( x , y , x_pred = np . linspace ( 0 , 23.5 , 100 ), ** model_kwargs ): # Fitting the model df_quantiles = quantile_model ( x , y , x_pred = x_pred , ** model_kwargs ) # Cleaning names and sorting for plotting df_quantiles . columns = [ f 'p { int ( col * 100 ) } ' for col in df_quantiles . columns ] df_quantiles = df_quantiles [ df_quantiles . columns [:: - 1 ]] return df_quantiles dts = df . index . tz_convert ( 'Europe/London' ) x = dts . hour + dts . minute / 60 y = df [ 'demand_MW' ] . values rerun_daily_demand_model = False daily_demand_filename = 'daily_demand_quantile_model_results.csv' if ( rerun_daily_demand_model == True ) or ( daily_demand_filename not in os . listdir ( cache_data_dir )): df_quantiles = estimate_daily_demand_quantiles ( x , y , frac = 0.2 , num_fits = 48 , robust_iters = 3 ) df_quantiles . to_csv ( f ' { cache_data_dir } / { daily_demand_filename } ' ) else : df_quantiles = pd . read_csv ( f ' { cache_data_dir } / { daily_demand_filename } ' , index_col = 'x' ) df_quantiles . head () ('Unnamed: 0_level_0', 'x') ('p90', 'Unnamed: 1_level_1') ('p80', 'Unnamed: 2_level_1') ('p70', 'Unnamed: 3_level_1') ('p60', 'Unnamed: 4_level_1') ('p50', 'Unnamed: 5_level_1') ('p40', 'Unnamed: 6_level_1') ('p30', 'Unnamed: 7_level_1') ('p20', 'Unnamed: 8_level_1') ('p10', 'Unnamed: 9_level_1') 0 2.79849 2.71229 2.64641 2.56191 2.39701 2.10668 1.88967 1.81977 1.77097 0.237374 2.76187 2.67597 2.60871 2.5275 2.3697 2.09069 1.86875 1.79585 1.74577 0.474747 2.72532 2.63973 2.57116 2.49312 2.34246 2.0744 1.84797 1.77185 1.72058 0.712121 2.68871 2.60348 2.53355 2.45863 2.31533 2.05794 1.82746 1.74759 1.69522 0.949495 2.65212 2.56737 2.49614 2.42416 2.28829 2.04086 1.80704 1.72319 1.6696 We'll now visualise these quantile fits alongside the raw data N.b. the x values have been slightly jittered in order to make their distribution easier to visualise x_jittered = x + ( np . random . uniform ( size = len ( x )) - 0.5 ) / 2.5 # Plotting fig , ax = plt . subplots ( dpi = 250 ) ax . scatter ( x_jittered , y , s = 0.2 , color = 'k' , alpha = 0.5 ) df_quantiles . plot ( cmap = 'viridis' , legend = False , ax = ax ) hlp . hide_spines ( ax ) ax . legend ( frameon = False , bbox_to_anchor = ( 1 , 0.9 ), title = 'Percentiles' ) ax . set_xlabel ( 'Time of Day' ) ax . set_ylabel ( 'Demand (MW)' ) ax . set_xlim ( 0 , 24 ) ax . set_ylim ( 1 , 6 ) fig . savefig ( '../img/daily_demand_profile.png' ) One of the issues with the quantile fit is that it hides the a lot of the spikiness in individual daily profiles, here we'll create a function for randomly sampling days so we can visualise them alongside each other. #exports reset_idx_dt = lambda s , dt = '2020-01-01' : s . index - ( s . index [ 0 ] - pd . to_datetime ( dt , utc = True )) def sample_random_day ( s ): random_dt = random . choice ( s . index . date ) s_sample_dt = s . loc [ str ( random_dt )] return s_sample_dt def sample_random_days ( s , num_days = 5 ): df_sample_dts = pd . DataFrame () for _ in range ( num_days ): s_sample_dt = sample_random_day ( s ) dt = str ( s_sample_dt . index [ 0 ] . date ()) s_sample_dt . index = reset_idx_dt ( s_sample_dt ) df_sample_dts [ dt ] = s_sample_dt df_sample_dts = df_sample_dts . sort_index ( axis = 1 ) return df_sample_dts df_sample_dts = sample_random_days ( s_demand ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) df_sample_dts . plot ( ax = ax ) ax . legend ( frameon = False ) hlp . hide_spines ( ax ) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Demand (MW)' ) _ = plt . setp ( ax . get_xmajorticklabels (), visible = False ) We'll also check the auto-correlation of the demand time-series, particularly in regard to the the correlation of the most recent value with the value from one week prior (what will be available for the test data) acf_array = acf ( s_demand , fft = True , nlags = 48 * 7 * 2 ) day_blocks = [ 0 ] + list ( np . array ([[ i + 1 ] * 48 for i in range ( 14 )]) . reshape ( - 1 )) s_acf_days_max = pd . Series ( acf_array ) . groupby ( day_blocks ) . max () corr_with_last_weeks_SP = s_acf_days_max . loc [ 7 ] # Plotting fig , ax = plt . subplots () s_acf_days_max . plot . bar ( ax = ax ) ax . plot ([ - 0.5 , 14.5 ], [ corr_with_last_weeks_SP , corr_with_last_weeks_SP ], 'k--' ) ax . set_ylim ( 0.7 , 1 ) ax . set_xlabel ( 'Day' ) ax . set_ylabel ( 'Correlation' ) hlp . hide_spines ( ax ) We'll also fit a quantile model for the relationship between temperature and demand x = df [ df . columns [ df . columns . str . contains ( 'temp_location' )]] . mean ( axis = 1 ) . values y = df [ 'demand_MW' ] . values rerun_temp_demand_model = False temp_demand_filename = 'temp_demand_quantile_model_results.csv' if ( rerun_temp_demand_model == True ) or ( temp_demand_filename not in os . listdir ( cache_data_dir )): df_quantiles = estimate_daily_demand_quantiles ( x , y , frac = 0.35 , num_fits = 48 , robust_iters = 5 ) df_quantiles . to_csv ( f ' { cache_data_dir } / { temp_demand_filename } ' ) else : df_quantiles = pd . read_csv ( f ' { cache_data_dir } / { temp_demand_filename } ' , index_col = 'x' ) df_quantiles . head () ('Unnamed: 0_level_0', 'x') ('p90', 'Unnamed: 1_level_1') ('p80', 'Unnamed: 2_level_1') ('p70', 'Unnamed: 3_level_1') ('p60', 'Unnamed: 4_level_1') ('p50', 'Unnamed: 5_level_1') ('p40', 'Unnamed: 6_level_1') ('p30', 'Unnamed: 7_level_1') ('p20', 'Unnamed: 8_level_1') ('p10', 'Unnamed: 9_level_1') 0 4.97915 4.54269 4.2198 3.87288 3.37484 2.90171 2.70959 2.59651 2.46055 0.237374 4.96727 4.52208 4.20217 3.86098 3.37012 2.89377 2.69721 2.58177 2.44486 0.474747 4.95515 4.50144 4.18453 3.84911 3.36556 2.88588 2.685 2.56718 2.42925 0.712121 4.94284 4.48071 4.16686 3.83725 3.3611 2.87787 2.67285 2.55265 2.41369 0.949495 4.93029 4.45984 4.14914 3.82542 3.35674 2.86969 2.66073 2.53816 2.39816 Which we'll visualise similarly to the daily demand profile fig , ax = plt . subplots ( dpi = 250 ) ax . scatter ( x , y , s = 0.2 , color = 'k' , alpha = 0.5 ) df_quantiles . plot ( cmap = 'viridis' , legend = False , ax = ax ) hlp . hide_spines ( ax ) ax . legend ( frameon = False , bbox_to_anchor = ( 1 , 0.25 ), title = 'Percentiles' ) ax . set_xlabel ( 'Temperature (degC)' ) ax . set_ylabel ( 'Demand (MW)' ) ax . set_xlim ( 0 , 24 ) ax . set_ylim ( 1 , 6 ) fig . savefig ( '../img/temp_demand_profile.png' ) # ^ Should use daily average (or evening block) instead of each SP","title":"Exploratory Demand Analysis"},{"location":"04-discharging/#strategy-development-with-perfect-foresight","text":"Here we'll develop a charging strategy for when we have perfect foresight, starting by sampling a random day from the demand series and then extracting the evening profile as an array from that #exports def extract_evening_demand_profile ( s_demand_sample_dt , start_time = '15:30' , end_time = '20:30' ): dt = str ( s_demand_sample_dt . index [ 0 ] . date ()) evening_demand_profile = s_demand_sample_dt [ f ' { dt } { start_time } ' : f ' { dt } { end_time } ' ] . values return evening_demand_profile evening_demand_profile = sample_random_day ( s_demand ) . pipe ( extract_evening_demand_profile ) plt . plot ( evening_demand_profile ) [<matplotlib.lines.Line2D at 0x2051e5fe790>] We'll then write an algorithm for peak flattening #exports def flatten_peak ( evening_demand_profile , charge = 6 , time_unit = 0.5 ): peak = max ( evening_demand_profile ) adj_evening_demand_profile = evening_demand_profile . copy () while charge > 0 : num_periods_plateaued = ( evening_demand_profile >= peak ) . sum () # If the evening demand profile has been fully flattened # then split up the remaining charge equally across all SPs fully_flattened = len ( set ( adj_evening_demand_profile )) == 1 if fully_flattened == True : remaining_discharge_rate_for_each_SP = ( 1 / time_unit ) * charge / len ( adj_evening_demand_profile ) adj_evening_demand_profile -= remaining_discharge_rate_for_each_SP charge = 0 break # If there is still a peak then determine the next highest value else : peak = max ( adj_evening_demand_profile ) highest_non_peak = max ( adj_evening_demand_profile [ peak > adj_evening_demand_profile ]) proposed_additional_discharge = time_unit * ( adj_evening_demand_profile . sum () - np . minimum ( adj_evening_demand_profile , highest_non_peak ) . sum ()) # if its possible to reduce the peak to the next highest value do so if charge >= proposed_additional_discharge : adj_evening_demand_profile = np . minimum ( adj_evening_demand_profile , highest_non_peak ) charge -= proposed_additional_discharge # If the capacity constraints are broken when reducing to the next # highest value then just lower the current peak as far as possible else : new_peak = peak - (( 1 / time_unit ) * charge / ( num_periods_plateaued + 1 )) adj_evening_demand_profile = np . minimum ( adj_evening_demand_profile , new_peak ) charge = 0 return adj_evening_demand_profile adj_evening_demand_profile = flatten_peak ( evening_demand_profile ) plt . plot ( evening_demand_profile ) plt . plot ( adj_evening_demand_profile ) [<matplotlib.lines.Line2D at 0x2051e599f70>] Which we can deduct from the original evening profile to construct the discharge profile #exports construct_discharge_profile = lambda evening_demand_profile , adj_evening_demand_profile : - ( evening_demand_profile - adj_evening_demand_profile ) discharge_profile = construct_discharge_profile ( evening_demand_profile , adj_evening_demand_profile ) plt . plot ( discharge_profile ) [<matplotlib.lines.Line2D at 0x2051e53a490>] Rather than the sample day we've just used we'll now repeat this step for all days we have demand data on, returning a series of the new discharge values that can be easily added to the charging values #exports def construct_discharge_s ( s_demand , start_time = '15:30' , end_time = '20:30' ): s_discharge = pd . Series ( index = s_demand . index , dtype = float ) . fillna ( 0 ) for dt in s_demand . index . strftime ( '%Y-%m- %d ' ) . unique (): evening_demand_profile = s_demand [ dt ] . pipe ( extract_evening_demand_profile ) adj_evening_demand_profile = flatten_peak ( evening_demand_profile ) discharge_profile = construct_discharge_profile ( evening_demand_profile , adj_evening_demand_profile ) s_discharge [ f ' { dt } { start_time } ' : f ' { dt } { end_time } ' ] = discharge_profile return s_discharge s_discharge = construct_discharge_s ( s_demand , start_time = '15:30' , end_time = '20:30' ) s_discharge . iloc [: 48 * 7 ] . plot () <AxesSubplot:> We can also use this discharging profile to see what the new peaks look like s_demand . iloc [: 48 * 7 ] . plot () ( s_demand + s_discharge ) . iloc [: 48 * 7 ] . plot () <AxesSubplot:>","title":"Strategy Development with Perfect Foresight"},{"location":"04-discharging/#strategy-development-under-uncertainty","text":"Our overall approach can be thought of as follows: Generate an optimal discharge profile under perfect foresight Train a regression model to emulate the optimal discharge profile Clean profile to ensure that constraints aren't broken and the full 6 MWh is fully utilised We've generated our optimal discharge profile, now we're ready to train the model. We'll first split up our X and y values, filtering only for those that fall into the evening period #exports def extract_evening_datetimes ( df ): hour = df . index . hour + df . index . minute / 60 evening_datetimes = df . index [( 20.5 >= hour ) & ( 15.5 <= hour )] return evening_datetimes evening_datetimes = extract_evening_datetimes ( df_features ) X = df_features . loc [ evening_datetimes ] . values y = s_discharge . loc [ evening_datetimes ] . values We'll create a basic prediction using a standard linear model df_pred = clean . generate_kfold_preds ( X , y , LinearRegression (), index = evening_datetimes ) df_pred . head () pred true 2017-11-10 15:30:00+00:00 -0.300207 -0.125455 2017-11-10 16:00:00+00:00 -0.554003 -0.565455 2017-11-10 16:30:00+00:00 -1.03486 -1.12546 2017-11-10 17:00:00+00:00 -1.55369 -1.58546 2017-11-10 17:30:00+00:00 -1.65075 -1.66545 However, in this approach there's nothing to enforce the battery constraints, namely maximum total discharge and instantaneous discharge rate. This becomes apparant when we visualise the distribution of total discharge volumes each evening. s_daily_discharge = df_pred [ 'pred' ] . groupby ( df_pred . index . date ) . sum () sns . distplot ( s_daily_discharge ) C:\\Users\\Ayrto\\anaconda3\\envs\\batopt\\lib\\site-packages\\seaborn\\distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) <AxesSubplot:xlabel='pred', ylabel='Density'> To account for this we can normalise each daily discharge profile by the ratio between the current total discharge and the maximum current discharge #exports def normalise_total_discharge ( s_pred , charge = 6 , time_unit = 0.5 ): s_daily_discharge = s_pred . groupby ( s_pred . index . date ) . sum () for date , total_discharge in s_daily_discharge . items (): s_pred . loc [ str ( date )] *= - charge / ( time_unit * total_discharge ) return s_pred s_daily_discharge = ( df_pred [ 'pred' ] . pipe ( normalise_total_discharge ) . groupby ( df_pred . index . date ) . sum () . round ( 10 ) ) s_daily_discharge . value_counts () -12.0 485 Name: pred, dtype: int64 We also need to ensure that the discharge rate remains within the bounds of the problem definition, i.e. no greater than -2.5 MW #exports clip_discharge_rate = lambda s_pred , max_rate =- 2.5 , min_rate = 0 : s_pred . clip ( lower = max_rate , upper = min_rate ) s_pred = df_pred [ 'pred' ] . pipe ( clip_discharge_rate ) s_pred . head () 2017-11-10 15:30:00+00:00 -0.297164 2017-11-10 16:00:00+00:00 -0.548387 2017-11-10 16:30:00+00:00 -1.024373 2017-11-10 17:00:00+00:00 -1.537942 2017-11-10 17:30:00+00:00 -1.634021 Name: pred, dtype: object We'll now combine these post prediction processing steps into a single function, ready to use in our model evaluation Note that the normalisation must come after the clipping. Otherwise the total charge constraint can be violated if the model predicts a discharge > 0 #exports post_pred_discharge_proc_func = lambda s_pred : ( s_pred . pipe ( clip_discharge_rate ) . pipe ( normalise_total_discharge ) ) post_pred_discharge_proc_func ( s_pred ) . groupby ( s_pred . index . date ) . sum () . round ( 10 ) . value_counts () -12.0 485 Name: pred, dtype: int64 We'll create a new function that evaluates our discharge profile in terms of the peak reduction achieved relative the reduction using an optimal discharge profile. We'll then use this and our standard mae and rmse metrics to evaluate some different models. #exports def construct_peak_reduction_calculator ( s_demand , evening_datetimes = None , scorer = False ): if evening_datetimes is None : evening_datetimes = extract_evening_datetimes ( s_demand ) def calc_peak_reduction ( y , y_pred ): # Checking evening datetimes if hasattr ( y_pred , 'index' ) == True : evening_datetimes = extract_evening_datetimes ( y_pred ) assert y_pred . shape [ 0 ] == s_demand . loc [ evening_datetimes ] . shape [ 0 ], f 'The prediction series must be the same length as the number of evening datetimes in the main dataframe, { y_pred . shape [ 0 ] } { s_demand . loc [ evening_datetimes ] . shape [ 0 ] } ' # Post-processing the discharge profile to handle constraints y_pred = post_pred_discharge_proc_func ( y_pred ) # Identifying daily peaks s_old_peaks = s_demand . loc [ evening_datetimes ] . groupby ( evening_datetimes . date ) . max () s_new_peaks = ( s_demand . loc [ evening_datetimes ] + y_pred ) . groupby ( evening_datetimes . date ) . max () s_optimal_peaks = ( s_demand . loc [ evening_datetimes ] + y ) . groupby ( evening_datetimes . date ) . max () # Calculating the peak reduction s_new_pct_peak_reduction = 100 * ( s_old_peaks - s_new_peaks ) / s_old_peaks s_optimal_pct_peak_reduction = 100 * ( s_old_peaks - s_optimal_peaks ) / s_old_peaks # after cleaning anomalous demand data should add an assert to check for non finite values pct_of_max_possible_reduction = 100 * ( s_new_pct_peak_reduction . replace ( np . inf , np . nan ) . dropna () . mean () / s_optimal_pct_peak_reduction . replace ( np . inf , np . nan ) . dropna () . mean ()) return pct_of_max_possible_reduction if scorer == True : return make_scorer ( calc_peak_reduction ) else : return calc_peak_reduction def evaluate_discharge_models ( df , models , features_kwargs = {}): df_features = construct_df_discharge_features ( df , ** features_kwargs ) s_discharge = construct_discharge_s ( df [ 'demand_MW' ], start_time = '15:30' , end_time = '20:30' ) evening_datetimes = extract_evening_datetimes ( df_features ) X = df_features . loc [ evening_datetimes ] . values y = s_discharge . loc [ evening_datetimes ] . values model_scores = dict () peak_reduction_calc = construct_peak_reduction_calculator ( s_demand = df [ 'demand_MW' ], evening_datetimes = evening_datetimes ) for model_name , model in track ( models . items ()): df_pred = clean . generate_kfold_preds ( X , y , model , index = evening_datetimes ) df_pred [ 'pred' ] = post_pred_discharge_proc_func ( df_pred [ 'pred' ]) model_scores [ model_name ] = { 'pct_optimal_reduction' : peak_reduction_calc ( df_pred [ 'true' ], df_pred [ 'pred' ]), 'optimal_discharge_mae' : mean_absolute_error ( df_pred [ 'true' ], df_pred [ 'pred' ]), 'optimal_discharge_rmse' : np . sqrt ( mean_squared_error ( df_pred [ 'true' ], df_pred [ 'pred' ])) } df_model_scores = pd . DataFrame ( model_scores ) df_model_scores . index . name = 'metric' df_model_scores . columns . name = 'model' return df_model_scores models = { 'std_linear' : LinearRegression (), 'random_forest' : RandomForestRegressor (), 'boosted' : GradientBoostingRegressor () } rerun_discharge_opt_model = False discharge_opt_filename = 'discharge_optimisation_model_results.csv' if ( rerun_discharge_opt_model == True ) or ( discharge_opt_filename not in os . listdir ( cache_data_dir )): df_model_scores = evaluate_discharge_models ( df . loc [ df_features . index ], models ) df_model_scores . to_csv ( f ' { cache_data_dir } / { discharge_opt_filename } ' ) else : df_model_scores = pd . read_csv ( f ' { cache_data_dir } / { discharge_opt_filename } ' , index_col = 'metric' ) df_model_scores ('Unnamed: 0_level_0', 'metric') ('std_linear', 'Unnamed: 1_level_1') ('random_forest', 'Unnamed: 2_level_1') ('boosted', 'Unnamed: 3_level_1') pct_optimal_reduction 85.6542 87.98 86.787 optimal_discharge_mae 0.121212 0.092669 0.100329 optimal_discharge_rmse 0.16245 0.121869 0.131338 We'll then generate a prediction time-series using the best performing model rerun_discharge_pred_model = False discharge_pred_filename = 'discharge_optimisation_model_pred.csv' if ( rerun_discharge_pred_model == True ) or ( discharge_pred_filename not in os . listdir ( cache_data_dir )): top_model = df_model_scores . T [ 'pct_optimal_reduction' ] . idxmax () df_pred = clean . generate_kfold_preds ( X , y , models [ top_model ], index = evening_datetimes ) df_pred [ 'pred' ] = post_pred_discharge_proc_func ( df_pred [ 'pred' ]) df_pred . to_csv ( f ' { cache_data_dir } / { discharge_pred_filename } ' ) else : df_pred = pd . read_csv ( f ' { cache_data_dir } / { discharge_pred_filename } ' ) df_pred [ 'datetime' ] = pd . to_datetime ( df_pred [ 'datetime' ], utc = True ) df_pred = df_pred . set_index ( 'datetime' ) df_pred . head () ('Unnamed: 0_level_0', 'datetime') ('pred', 'Unnamed: 1_level_1') ('true', 'Unnamed: 2_level_1') 2017-11-10 15:30:00+00:00 -0.213378 -0.125455 2017-11-10 16:00:00+00:00 -0.735541 -0.565455 2017-11-10 16:30:00+00:00 -1.19438 -1.12546 2017-11-10 17:00:00+00:00 -1.57704 -1.58546 2017-11-10 17:30:00+00:00 -1.61426 -1.66545 We'll quickly check the residuals time-series for model-drift s_residuals = df_pred . diff ( 1 , axis = 1 ) . dropna ( axis = 1 ) . iloc [:, 0 ] s_residuals . plot ( linewidth = 0.3 ) <AxesSubplot:xlabel='datetime'> As well as the scatter-plot between the true and estimated optimal charging rates plt . scatter ( df_pred [ 'true' ], df_pred [ 'pred' ], s = 1 ) plt . xlabel ( 'Obervation' ) plt . ylabel ( 'Prediction' ) Text(0, 0.5, 'Prediction')","title":"Strategy Development under Uncertainty"},{"location":"04-discharging/#pipeline-integration-helpers","text":"#exports def prepare_training_input_data ( intermediate_data_dir ): # Loading input data df = clean . combine_training_datasets ( intermediate_data_dir ) . interpolate ( limit = 1 ) df_features = construct_df_discharge_features ( df ) # Filtering for overlapping feature and target data dt_idx = pd . date_range ( df_features . index . min (), df [ 'demand_MW' ] . dropna () . index . max () - pd . Timedelta ( minutes = 30 ), freq = '30T' ) s_demand = df . loc [ dt_idx , 'demand_MW' ] df_features = df_features . loc [ dt_idx ] # Constructing the discharge series s_discharge = construct_discharge_s ( s_demand , start_time = '15:30' , end_time = '20:30' ) # Filtering for evening datetimes evening_datetimes = extract_evening_datetimes ( df_features ) X = df_features . loc [ evening_datetimes ] y = s_discharge . loc [ evening_datetimes ] return X , y X , y = prepare_training_input_data ( intermediate_data_dir ) X . shape , y . shape ((5335, 15), (5335,)) #exports def fit_and_save_model ( X , y , discharge_opt_model_fp , model_class = RandomForestRegressor , ** model_params ): model = model_class ( ** model_params ) model . fit ( X , y ) with open ( discharge_opt_model_fp , 'wb' ) as fp : joblib . dump ( model , fp ) return %% time fit_and_save_model ( X , y , discharge_opt_model_fp ) Wall time: 6.65 s #exports def load_trained_model ( discharge_opt_model_fp ): with open ( discharge_opt_model_fp , 'rb' ) as fp : model = joblib . load ( fp ) return model %% time model = load_trained_model ( discharge_opt_model_fp ) model Wall time: 176 ms RandomForestRegressor() #exports def load_latest_submission_template ( raw_data_dir , latest_submission_template_name = None ): if latest_submission_template_name is None : latest_submission_template_name = max ([ filename for filename in os . listdir ( raw_data_dir ) if 'teamname_set' in filename ]) df_submission_template = pd . read_csv ( f ' { raw_data_dir } / { latest_submission_template_name } ' ) df_submission_template [ 'datetime' ] = pd . to_datetime ( df_submission_template [ 'datetime' ], utc = True ) df_submission_template = df_submission_template . set_index ( 'datetime' ) return df_submission_template def prepare_test_feature_data ( raw_data_dir , intermediate_data_dir , test_start_date = None , test_end_date = None ): # Loading input data df_features = ( clean . combine_training_datasets ( intermediate_data_dir ) . interpolate ( limit = 1 ) . pipe ( construct_df_discharge_features ) ) # Loading default index (latest submission) if test_end_date is None or test_start_date is None : index = load_latest_submission_template ( raw_data_dir ) . index else : index = df_features [ test_start_date : test_end_date ] . index # Filtering feature data on submission datetimes df_features = df_features . loc [ index ] return df_features df_features = prepare_test_feature_data ( raw_data_dir , intermediate_data_dir ) df_features . head () ('Unnamed: 0_level_0', 'datetime') ('temp_location1', 'Unnamed: 1_level_1') ('temp_location2', 'Unnamed: 2_level_1') ('temp_location3', 'Unnamed: 3_level_1') ('temp_location4', 'Unnamed: 4_level_1') ('temp_location5', 'Unnamed: 5_level_1') ('temp_location6', 'Unnamed: 6_level_1') ('spatial_avg_temp', 'Unnamed: 7_level_1') ('daily_avg_temp', 'Unnamed: 8_level_1') ('SP_demand_7d_lag', 'Unnamed: 9_level_1') ('evening_demand_avg_7d_lag', 'Unnamed: 10_level_1') ('evening_demand_max_7d_lag', 'Unnamed: 11_level_1') ('weekend', 'Unnamed: 12_level_1') ('hour', 'Unnamed: 13_level_1') ('doy', 'Unnamed: 14_level_1') ('dow', 'Unnamed: 15_level_1') 2019-03-10 00:00:00+00:00 9.69 8.98 7.01 5.83 11.59 11.22 9.05333 8.16849 2.43 4.22667 4.85 1 0 69 6 2019-03-10 00:30:00+00:00 10.45 9.91 7.74 5.745 11.8 11.425 9.51167 8.16849 2.4 4.22667 4.85 1 0.5 69 6 2019-03-10 01:00:00+00:00 11.21 10.84 8.47 5.66 12.01 11.63 9.97 8.16849 2.28 4.22667 4.85 1 1 69 6 2019-03-10 01:30:00+00:00 11.225 11.08 9.57 5.785 12.075 11.69 10.2375 8.16849 2.11 4.22667 4.85 1 1.5 69 6 2019-03-10 02:00:00+00:00 11.24 11.32 10.67 5.91 12.14 11.75 10.505 8.16849 2.03 4.22667 4.85 1 2 69 6 #exports def optimise_test_discharge_profile ( raw_data_dir , intermediate_data_dir , discharge_opt_model_fp , test_start_date = None , test_end_date = None ): df_features = prepare_test_feature_data ( raw_data_dir , intermediate_data_dir , test_start_date = test_start_date , test_end_date = test_end_date ) evening_datetimes = extract_evening_datetimes ( df_features ) X_test = df_features . loc [ evening_datetimes ] . values model = load_trained_model ( discharge_opt_model_fp ) discharge_profile = model . predict ( X_test ) s_discharge_profile = pd . Series ( discharge_profile , index = evening_datetimes ) s_discharge_profile = s_discharge_profile . reindex ( df_features . index ) . fillna ( 0 ) s_discharge_profile = post_pred_discharge_proc_func ( s_discharge_profile ) return s_discharge_profile s_discharge_profile = optimise_test_discharge_profile ( raw_data_dir , intermediate_data_dir , discharge_opt_model_fp ) s_discharge_profile . plot () <AxesSubplot:xlabel='datetime'> Finally we'll export the relevant code to our batopt module","title":"Pipeline Integration Helpers"},{"location":"05-constraints/","text":"Battery Constraints \u00b6 #exports import numpy as np import pandas as pd Loading an Example Charging Rate Profile \u00b6 df_charge_rate = pd.read_csv('../data/output/latest_submission.csv') s_charge_rate = df_charge_rate.set_index('datetime')['charge_MW'] s_charge_rate.index = pd.to_datetime(s_charge_rate.index) s_charge_rate.head() datetime 2018-07-23 00:00:00+00:00 0.0 2018-07-23 00:30:00+00:00 0.0 2018-07-23 01:00:00+00:00 0.0 2018-07-23 01:30:00+00:00 0.0 2018-07-23 02:00:00+00:00 0.0 Name: charge_MW, dtype: float64 Checking for Nulls \u00b6 Before we start doing anything clever we'll do a simple check for null values #exports def check_for_nulls(s_charge_rate): assert s_charge_rate.isnull().sum()==0, 'There are null values in the charge rate time-series' check_for_nulls(s_charge_rate) Converting a charging schedule to capacity \u00b6 The solution is given in terms of the battery charge/discharge schedule, but it is also necessary to satisfy constraints on the capacity of the battery (see below). The charge is determined by \\(C_{t+1} = C_{t} + 0.5B_{t}\\) We'll start by generating the charge state time-series #exports def construct_charge_state_s(s_charge_rate: pd.Series, time_unit: float=0.5) -> pd.Series: s_charge_state = (s_charge_rate .cumsum() .divide(1/time_unit) ) return s_charge_state s_charge_state = construct_charge_state_s(s_charge_rate) s_charge_state.plot() <AxesSubplot:xlabel='datetime'> Checking Capacity Constraints \u00b6 \\(0 \\leq C \\leq C_{max}\\) We'll confirm that the bounds of the values in the charging time-series do not fall outside of the 0-6 MWh capacity of the battery #exports doesnt_exceed_charge_state_min = lambda s_charge_state, min_charge=0: (s_charge_state.round(10)<min_charge).sum()==0 doesnt_exceed_charge_state_max = lambda s_charge_state, max_charge=6: (s_charge_state.round(10)>max_charge).sum()==0 def check_capacity_constraints(s_charge_state, min_charge=0, max_charge=6): assert doesnt_exceed_charge_state_min(s_charge_state, min_charge), 'The state of charge falls below 0 MWh which is beyond the bounds of possibility' assert doesnt_exceed_charge_state_max(s_charge_state, max_charge), 'The state of charge exceeds the 6 MWh capacity' return check_capacity_constraints(s_charge_state) Checking Full Utilisation \u00b6 We'll also check that the battery falls to 0 MWh and rises to 6 MWh each day #exports check_all_values_equal = lambda s, value=0: (s==0).mean()==1 charge_state_always_drops_to_0MWh = lambda s_charge_state, min_charge=0: s_charge_state.groupby(s_charge_state.index.date).min().round(10).pipe(check_all_values_equal, min_charge) charge_state_always_gets_to_6MWh = lambda s_charge_state, max_charge=6: s_charge_state.groupby(s_charge_state.index.date).min().round(10).pipe(check_all_values_equal, max_charge) def check_full_utilisation(s_charge_state, min_charge=0, max_charge=6): assert charge_state_always_drops_to_0MWh(s_charge_state, min_charge), 'The state of charge does not always drop to 0 MWh each day' assert charge_state_always_gets_to_6MWh(s_charge_state, max_charge), 'The state of charge does not always rise to 6 MWh each day' return check_full_utilisation(s_charge_state) Checking Charge Rates \u00b6 \\(B_{min} \\leq B \\leq B_{max}\\) We'll then check that the minimum and maximum rates fall inside the -2.5 - 2.5 MW allowed by the battery #exports doesnt_exceed_charge_rate_min = lambda s_charge_rate, min_rate=-2.5: (s_charge_rate.round(10)<min_rate).sum()==0 doesnt_exceed_charge_rate_max = lambda s_charge_rate, max_rate=2.5: (s_charge_rate.round(10)>max_rate).sum()==0 def check_rate_constraints(s_charge_rate, min_rate=-2.5, max_rate=2.5): assert doesnt_exceed_charge_rate_min(s_charge_rate, min_rate), 'The rate of charge falls below -2.5 MW limit' assert doesnt_exceed_charge_rate_max(s_charge_rate, max_rate), 'The rate of charge exceeds the 2.5 MW limit' return check_rate_constraints(s_charge_rate) Checking Charge/Discharge/Inactive Periods \u00b6 We can only charge the battery between periods 1 (00:00) and 31 (15:00) inclusive, and discharge between periods 32 (15:30) and 42 (20:30) inclusive. For periods 43 to 48, there should be no activity, and the day must start with \\(C=0\\) . #exports charge_is_0_at_midnight = lambda s_charge_state: (s_charge_state.between_time('23:30', '23:59').round(10)==0).mean()==1 all_charge_periods_charge = lambda s_charge_rate, charge_times=('00:00', '15:00'): (s_charge_rate.between_time(charge_times[0], charge_times[1]).round(10) >= 0).mean() == 1 all_discharge_periods_discharge = lambda s_charge_rate, discharge_times=('15:30', '20:30'): (s_charge_rate.between_time(discharge_times[0], discharge_times[1]).round(10) <= 0).mean() == 1 all_inactive_periods_do_nothing = lambda s_charge_rate, inactive_times=('21:00', '23:30'): (s_charge_rate.between_time(inactive_times[0], inactive_times[1]).round(10) == 0).mean() == 1 def check_charging_patterns(s_charge_rate, s_charge_state, charge_times=('00:00', '15:00'), discharge_times=('15:30', '20:30'), inactive_times=('21:00', '23:30')): assert charge_is_0_at_midnight(s_charge_state), 'The battery is not always at 0 MWh at midnight' assert all_charge_periods_charge(s_charge_rate, charge_times), 'Some of the periods which should only be charging are instead discharging' assert all_discharge_periods_discharge(s_charge_rate, discharge_times), 'Some of the periods which should only be discharging are instead charging' assert all_inactive_periods_do_nothing(s_charge_rate, inactive_times), 'Some of the periods which should be doing nothing are instead charging/discharging' return check_charging_patterns(s_charge_rate, s_charge_state) #exports def schedule_is_legal(s_charge_rate, time_unit=0.5, min_rate=-2.5, max_rate=2.5, min_charge=0, max_charge=6, charge_times=('00:00', '15:00'), discharge_times=('15:30', '20:30'), inactive_times=('21:00', '23:30')): \"\"\" Determine if a battery schedule meets the specified constraints \"\"\" check_for_nulls(s_charge_rate) s_charge_state = construct_charge_state_s(s_charge_rate, time_unit) check_capacity_constraints(s_charge_state, min_charge, max_charge) check_full_utilisation(s_charge_state, min_charge, max_charge) check_rate_constraints(s_charge_rate, min_rate, max_rate) check_charging_patterns(s_charge_rate, s_charge_state, charge_times, discharge_times, inactive_times) return True schedule_is_legal(s_charge_rate) True Finally we'll export the relevant code to our batopt module","title":"Constraints"},{"location":"05-constraints/#battery-constraints","text":"#exports import numpy as np import pandas as pd","title":"Battery Constraints"},{"location":"05-constraints/#loading-an-example-charging-rate-profile","text":"df_charge_rate = pd.read_csv('../data/output/latest_submission.csv') s_charge_rate = df_charge_rate.set_index('datetime')['charge_MW'] s_charge_rate.index = pd.to_datetime(s_charge_rate.index) s_charge_rate.head() datetime 2018-07-23 00:00:00+00:00 0.0 2018-07-23 00:30:00+00:00 0.0 2018-07-23 01:00:00+00:00 0.0 2018-07-23 01:30:00+00:00 0.0 2018-07-23 02:00:00+00:00 0.0 Name: charge_MW, dtype: float64","title":"Loading an Example Charging Rate Profile"},{"location":"05-constraints/#checking-for-nulls","text":"Before we start doing anything clever we'll do a simple check for null values #exports def check_for_nulls(s_charge_rate): assert s_charge_rate.isnull().sum()==0, 'There are null values in the charge rate time-series' check_for_nulls(s_charge_rate)","title":"Checking for Nulls"},{"location":"05-constraints/#converting-a-charging-schedule-to-capacity","text":"The solution is given in terms of the battery charge/discharge schedule, but it is also necessary to satisfy constraints on the capacity of the battery (see below). The charge is determined by \\(C_{t+1} = C_{t} + 0.5B_{t}\\) We'll start by generating the charge state time-series #exports def construct_charge_state_s(s_charge_rate: pd.Series, time_unit: float=0.5) -> pd.Series: s_charge_state = (s_charge_rate .cumsum() .divide(1/time_unit) ) return s_charge_state s_charge_state = construct_charge_state_s(s_charge_rate) s_charge_state.plot() <AxesSubplot:xlabel='datetime'>","title":"Converting a charging schedule to capacity"},{"location":"05-constraints/#checking-capacity-constraints","text":"\\(0 \\leq C \\leq C_{max}\\) We'll confirm that the bounds of the values in the charging time-series do not fall outside of the 0-6 MWh capacity of the battery #exports doesnt_exceed_charge_state_min = lambda s_charge_state, min_charge=0: (s_charge_state.round(10)<min_charge).sum()==0 doesnt_exceed_charge_state_max = lambda s_charge_state, max_charge=6: (s_charge_state.round(10)>max_charge).sum()==0 def check_capacity_constraints(s_charge_state, min_charge=0, max_charge=6): assert doesnt_exceed_charge_state_min(s_charge_state, min_charge), 'The state of charge falls below 0 MWh which is beyond the bounds of possibility' assert doesnt_exceed_charge_state_max(s_charge_state, max_charge), 'The state of charge exceeds the 6 MWh capacity' return check_capacity_constraints(s_charge_state)","title":"Checking Capacity Constraints"},{"location":"05-constraints/#checking-full-utilisation","text":"We'll also check that the battery falls to 0 MWh and rises to 6 MWh each day #exports check_all_values_equal = lambda s, value=0: (s==0).mean()==1 charge_state_always_drops_to_0MWh = lambda s_charge_state, min_charge=0: s_charge_state.groupby(s_charge_state.index.date).min().round(10).pipe(check_all_values_equal, min_charge) charge_state_always_gets_to_6MWh = lambda s_charge_state, max_charge=6: s_charge_state.groupby(s_charge_state.index.date).min().round(10).pipe(check_all_values_equal, max_charge) def check_full_utilisation(s_charge_state, min_charge=0, max_charge=6): assert charge_state_always_drops_to_0MWh(s_charge_state, min_charge), 'The state of charge does not always drop to 0 MWh each day' assert charge_state_always_gets_to_6MWh(s_charge_state, max_charge), 'The state of charge does not always rise to 6 MWh each day' return check_full_utilisation(s_charge_state)","title":"Checking Full Utilisation"},{"location":"05-constraints/#checking-charge-rates","text":"\\(B_{min} \\leq B \\leq B_{max}\\) We'll then check that the minimum and maximum rates fall inside the -2.5 - 2.5 MW allowed by the battery #exports doesnt_exceed_charge_rate_min = lambda s_charge_rate, min_rate=-2.5: (s_charge_rate.round(10)<min_rate).sum()==0 doesnt_exceed_charge_rate_max = lambda s_charge_rate, max_rate=2.5: (s_charge_rate.round(10)>max_rate).sum()==0 def check_rate_constraints(s_charge_rate, min_rate=-2.5, max_rate=2.5): assert doesnt_exceed_charge_rate_min(s_charge_rate, min_rate), 'The rate of charge falls below -2.5 MW limit' assert doesnt_exceed_charge_rate_max(s_charge_rate, max_rate), 'The rate of charge exceeds the 2.5 MW limit' return check_rate_constraints(s_charge_rate)","title":"Checking Charge Rates"},{"location":"05-constraints/#checking-chargedischargeinactive-periods","text":"We can only charge the battery between periods 1 (00:00) and 31 (15:00) inclusive, and discharge between periods 32 (15:30) and 42 (20:30) inclusive. For periods 43 to 48, there should be no activity, and the day must start with \\(C=0\\) . #exports charge_is_0_at_midnight = lambda s_charge_state: (s_charge_state.between_time('23:30', '23:59').round(10)==0).mean()==1 all_charge_periods_charge = lambda s_charge_rate, charge_times=('00:00', '15:00'): (s_charge_rate.between_time(charge_times[0], charge_times[1]).round(10) >= 0).mean() == 1 all_discharge_periods_discharge = lambda s_charge_rate, discharge_times=('15:30', '20:30'): (s_charge_rate.between_time(discharge_times[0], discharge_times[1]).round(10) <= 0).mean() == 1 all_inactive_periods_do_nothing = lambda s_charge_rate, inactive_times=('21:00', '23:30'): (s_charge_rate.between_time(inactive_times[0], inactive_times[1]).round(10) == 0).mean() == 1 def check_charging_patterns(s_charge_rate, s_charge_state, charge_times=('00:00', '15:00'), discharge_times=('15:30', '20:30'), inactive_times=('21:00', '23:30')): assert charge_is_0_at_midnight(s_charge_state), 'The battery is not always at 0 MWh at midnight' assert all_charge_periods_charge(s_charge_rate, charge_times), 'Some of the periods which should only be charging are instead discharging' assert all_discharge_periods_discharge(s_charge_rate, discharge_times), 'Some of the periods which should only be discharging are instead charging' assert all_inactive_periods_do_nothing(s_charge_rate, inactive_times), 'Some of the periods which should be doing nothing are instead charging/discharging' return check_charging_patterns(s_charge_rate, s_charge_state) #exports def schedule_is_legal(s_charge_rate, time_unit=0.5, min_rate=-2.5, max_rate=2.5, min_charge=0, max_charge=6, charge_times=('00:00', '15:00'), discharge_times=('15:30', '20:30'), inactive_times=('21:00', '23:30')): \"\"\" Determine if a battery schedule meets the specified constraints \"\"\" check_for_nulls(s_charge_rate) s_charge_state = construct_charge_state_s(s_charge_rate, time_unit) check_capacity_constraints(s_charge_state, min_charge, max_charge) check_full_utilisation(s_charge_state, min_charge, max_charge) check_rate_constraints(s_charge_rate, min_rate, max_rate) check_charging_patterns(s_charge_rate, s_charge_state, charge_times, discharge_times, inactive_times) return True schedule_is_legal(s_charge_rate) True Finally we'll export the relevant code to our batopt module","title":"Checking Charge/Discharge/Inactive Periods"},{"location":"06-tuning/","text":"Tuning \u00b6 #exports import json import numpy as np import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.pipeline import Pipeline from mlxtend.feature_selection import SequentialFeatureSelector as SFS from skopt.plots import plot_objective from skopt.space import Real , Categorical , Integer from batopt import clean , discharge , charge , pv , utils import os from ipypb import track import FEAutils as hlp import matplotlib.pyplot as plt from IPython.display import JSON User Inputs \u00b6 intermediate_data_dir = '../data/intermediate' raw_data_dir = '../data/raw' cache_data_dir = '../data/nb-cache' Preparing Data \u00b6 First we'll load in the target and feature data for both the charging and discharging models charge_x , charge_y = pv . prepare_training_input_data ( intermediate_data_dir ) discharge_x , discharge_y = discharge . prepare_training_input_data ( intermediate_data_dir ) charge_x . head () s_demand = clean . load_training_dataset ( intermediate_data_dir , 'demand' )[ 'demand_MW' ] s_demand . head () s_pv = clean . load_training_dataset ( intermediate_data_dir , 'pv' )[ 'pv_power_mw' ] s_pv . head () #exports def get_train_test_arr ( arr , start_of_test_period ): train_arr = arr [: pd . to_datetime ( start_of_test_period , utc = True )] test_arr = arr [ pd . to_datetime ( start_of_test_period , utc = True ):] return train_arr , test_arr def get_train_test_Xy ( X , y , start_of_test_period ): x_train , x_test = get_train_test_arr ( X , start_of_test_period ) y_train , y_test = get_train_test_arr ( y , start_of_test_period ) return x_train , x_test , y_train , y_test start_of_test_period = '2020-06-15' charge_x_train , charge_x_test , charge_y_train , charge_y_test = pv . get_train_test_Xy ( charge_x , charge_y , start_of_test_period ) discharge_x_train , discharge_x_test , discharge_y_train , discharge_y_test = pv . get_train_test_Xy ( discharge_x , discharge_y , start_of_test_period ) Evaluation Metrics \u00b6 We want to evaluate each of our models based on their contribution to the final scoring value, to do this we'll first create some predictions for our discharge model. discharge_rf = RandomForestRegressor () discharge_rf . fit ( discharge_x_train , discharge_y_train ) discharge_y_pred = pd . Series ( discharge_rf . predict ( discharge_x_test ), index = discharge_x_test . index ) discharge_y_pred . plot () We'll then create a time-series of the percentage peak reduction for each day #exports def calculate_pct_peak_reduction_s ( discharge_y_pred , s_demand ): s_demand_test = s_demand . loc [ discharge_y_pred . index ] s_old_peaks = s_demand_test . groupby ( s_demand_test . index . date ) . max () s_new_peaks = ( s_demand_test + discharge_y_pred ) . groupby ( s_demand_test . index . date ) . max () s_pct_peak_reduction = 100 * ( s_old_peaks - s_new_peaks ) / s_new_peaks s_pct_peak_reduction . index = pd . to_datetime ( s_pct_peak_reduction . index ) return s_pct_peak_reduction s_pct_peak_reduction = calculate_pct_peak_reduction_s ( discharge_y_pred , s_demand ) print ( f 'The average peak reduction was { s_pct_peak_reduction . mean () : .2f } %' ) s_pct_peak_reduction . plot () We'll then repeat this with the charging model charge_rf = RandomForestRegressor () charge_rf . fit ( charge_x_train , charge_y_train ) charge_y_pred = pd . Series ( charge_rf . predict ( charge_x_test ), index = charge_x_test . index ) charge_y_pred . plot () For which we'll calculate the emissions factor series #exports def calculate_emissions_factor_s ( charge_y_pred , s_pv , solar_factor = 3 , grid_factor = 1 ): s_solar_charge_pct = ( charge_y_pred - s_pv . loc [ charge_y_pred . index ]) . clip ( 0 ) . groupby ( charge_y_pred . index . date ) . sum () / charge_y_pred . groupby ( charge_y_pred . index . date ) . sum () s_grid_charge_pct = 1 - s_solar_charge_pct s_emissions_factor = solar_factor * s_solar_charge_pct + grid_factor * s_grid_charge_pct s_emissions_factor . index = pd . to_datetime ( s_emissions_factor . index ) return s_emissions_factor s_emissions_factor = calculate_emissions_factor_s ( charge_y_pred , s_pv ) s_emissions_factor . plot () We can then combine these two steps to determine our final score for each day s_score = calculate_score_s ( discharge_y_pred , charge_y_pred , s_demand , s_pv ) print ( f 'The average score was: { s_score . mean () : .2f } ' ) s_score . plot () For the charging we can also look at how much was sourced from PV relative to the potential maximum (capped at 6 MWh per day). solar_charge = np . minimum ( charge_y_pred , s_pv . loc [ charge_y_pred . index ]) day_solar_charge = solar_charge . groupby ( solar_charge . index . date ) . sum () . clip ( 0 , 12 ) day_solar_charge . index = pd . to_datetime ( day_solar_charge . index ) solar_potential = np . clip ( s_pv . loc [ charge_y_pred . index ], 0 , 2.5 ) day_solar_potential = solar_potential . groupby ( solar_potential . index . date ) . sum () . clip ( 0 , 12 ) day_solar_potential . index = pd . to_datetime ( day_solar_potential . index ) day_solar_charge . plot () day_solar_potential . plot () pct_exploit = 100 * day_solar_charge / day_solar_potential pct_exploit . plot () plt . ylabel ( ' % e xploited' ) #exports def score_charge ( schedule , solar_profile , solar_factor = 3 , grid_factor = 1 ): # The actual pv charge is the minimum of the scheduled charge and the actual solar availability actual_pv_charge = np . minimum ( schedule . values , solar_profile . values ) actual_pv_charge = pd . Series ( actual_pv_charge , index = schedule . index ) pct_pv_charge = actual_pv_charge . groupby ( actual_pv_charge . index . date ) . sum () / schedule . groupby ( schedule . index . date ) . sum () pct_grid_charge = 1 - pct_pv_charge score = ( solar_factor * pct_pv_charge ) + ( grid_factor * pct_grid_charge ) return score def score_discharge ( schedule , demand ): new_demand = schedule + demand old_demand = demand new_peaks = new_demand . groupby ( new_demand . index . date ) . max () old_peaks = old_demand . groupby ( old_demand . index . date ) . max () pct_reduction = 100 * (( old_peaks - new_peaks ) / old_peaks ) return pct_reduction def max_charge_score ( solar_profile , solar_factor = 3 , grid_factor = 1 , capacity = 6 , time_unit = 0.5 ): pv_potential = solar_profile . groupby ( solar_profile . index . date ) . sum () . clip ( 0 , capacity / time_unit ) pct_pv_charge = pv_potential / ( capacity / time_unit ) pct_grid_charge = 1 - pct_pv_charge score = ( solar_factor * pct_pv_charge ) + ( grid_factor * pct_grid_charge ) return score def calculate_score_s ( discharge_y_pred , charge_y_pred , s_demand , s_pv , solar_factor = 3 , grid_factor = 1 ): charge_score = score_charge ( charge_y_pred , s_pv , solar_factor , grid_factor ) discharge_score = score_discharge ( discharge_y_pred , s_demand ) s_score = discharge_score * charge_score return s_score , charge_score , discharge_score def evaluate_submission ( submission , intermediate_data_dir ): if isinstance ( submission , str ): df_solution = pd . read_csv ( submission ) df_solution = df_solution . set_index ( pd . to_datetime ( df_solution . datetime , utc = True )) else : assert isinstance ( submission , pd . DataFrame ), '`submission` must either be a valid submission dataframe or a filepath to the submission' df_solution = submission df_real = clean . combine_training_datasets ( intermediate_data_dir ) df_real = df_real [ df_real . index . isin ( df_solution . index )] df_solution_charge = df_solution . between_time ( '00:00' , '15:00' ) df_solution_discharge = df_solution . between_time ( '15:30' , '20:30' ) df_real_charge = df_real . between_time ( '00:00' , '15:00' ) df_real_discharge = df_real . between_time ( '15:30' , '20:30' ) total_score , charge_score , discharge_score = calculate_score_s ( df_solution_discharge . charge_MW , df_solution_charge . charge_MW , df_real_discharge . demand_MW , df_real_charge . pv_power_mw ) df_results = pd . DataFrame ({ 'total_score' : total_score , 'charge_score' : charge_score , 'discharge_score' : discharge_score , 'max_charge_score' : max_charge_score ( df_real_charge . pv_power_mw ) }) return df_results submission_fp = '../data/output/ESAIL_set1.csv' df_results = evaluate_submission ( submission_fp , intermediate_data_dir ) df_results We can then calculate our average score over this period df_results [ 'total_score' ] . mean () Discharge Model Tuning \u00b6 We'll begin by carrying out some feature selection #exports def feature_selection ( x_train , y_train , groups = None , model = RandomForestRegressor (), min_num_features = 1 , max_num_features = None , ** sfs_kwargs ): if max_num_features is None : max_num_features = 1 + x_train . shape [ 1 ] result_features = dict () result_scores = dict () for num_features in track ( range ( min_num_features , max_num_features )): sfs = SFS ( model , k_features = num_features , ** sfs_kwargs ) sfs . fit ( x_train , y_train , groups = groups ) result_features [ num_features ] = sfs . k_feature_names_ result_scores [ num_features ] = sfs . k_score_ return result_features , result_scores peak_reduction_scorer = discharge . construct_peak_reduction_calculator ( s_demand = s_demand . loc [ discharge_x_train . index ], scorer = True ) week_groups = discharge_x_train . index . year + discharge_x_train . index . isocalendar () . week / 52 rerun_feature_selection = False feature_selection_filename = f 'feature_selection.json' if ( rerun_feature_selection == True ) or ( feature_selection_filename not in os . listdir ( cache_data_dir )): result_features , result_scores = feature_selection ( discharge_x_train , discharge_y_train , groups = week_groups , n_jobs =- 1 ) with open ( f ' { cache_data_dir } / { feature_selection_filename } ' , 'w' ) as fp : json . dump ( dict ( zip ([ 'features' , 'scores' ], [ result_features , result_scores ])), fp ) else : with open ( f ' { cache_data_dir } / { feature_selection_filename } ' , 'r' ) as fp : results = json . load ( fp ) result_features , result_scores = results [ 'features' ], results [ 'scores' ] We can visualise how the model accuracy changes with the number of features included pd . Series ( result_scores ) . plot () We'll also calculate the relative importance of each feature by counting how many times they were included in the optimal feature subset flatten_iterables = lambda iterable : [ item for sublist in list ( iterable ) for item in sublist ] s_feature_importance = pd . Series ( flatten_iterables ( result_features . values ())) . value_counts () . divide ( len ( result_features )) s_feature_importance We'll now do some hyper-parameter tuning using the skopt library features = s_feature_importance . index [: 11 ] evening_datetimes = discharge . extract_evening_datetimes ( discharge_x_train ) week_groups = discharge_x_train . index . year + discharge_x_train . index . isocalendar () . week / 52 peak_reduction_scorer = discharge . construct_peak_reduction_calculator ( s_demand = s_demand , scorer = True ) pipeline = Pipeline ([ # Add in oversampling of more recent/similar dates ( 'pandas_RF' , utils . PandasRandomForestRegressor ()) ]) search_spaces = { 'pandas_RF__min_samples_leaf' : Integer ( 1 , 20 , 'uniform' ), 'pandas_RF__criterion' : Categorical ([ 'mse' , 'mae' ]), 'pandas_RF__n_estimators' : Integer ( 50 , 250 , 'uniform' ), 'pandas_RF__max_features' : Categorical ([ 'auto' , 'sqrt' ]), 'pandas_RF__max_depth' : Integer ( 10 , 50 , 'uniform' ), 'pandas_RF__min_samples_split' : Integer ( 2 , 10 , 'uniform' ), 'pandas_RF__min_samples_leaf' : Integer ( 1 , 4 , 'uniform' ), 'pandas_RF__bootstrap' : Categorical ([ True , False ]) } opt = utils . BayesSearchCV ( pipeline , search_spaces , n_iter = 15 , verbose = 1 , cv = 8 , # 8 works well for me as that's how many concurrent workers I can use scoring = peak_reduction_scorer , n_jobs =- 1 ) fit_BayesSearchCV = False if fit_BayesSearchCV == True : opt . fit ( discharge_x_train [ features ], discharge_y_train , groups = evening_datetimes . date ) print ( f 'Cross-validation score: { opt . best_score_ : .2f } ' ) print ( f 'Hold-out score: { opt . score ( discharge_x_test [ features ], discharge_y_test ) : .2f } ' ) print ( f ' \\n Best params: \\n { opt . best_params_ } ' ) # want to be saving model runs # could include as part of a callback? Model Comparisons \u00b6 Here we'll compare our discharge v pv-forecast modelling approaches index = pd . date_range ( '2019-03-02' , '2019-03-09 23:30' , freq = '30T' , tz = 'UTC' ) %% time discharge_opt_model_fp = '../models/discharge_opt.sav' X , y = discharge . prepare_training_input_data ( intermediate_data_dir ) idxs_to_keep = sorted ( list ( set ( X . index ) - set ( index ))) X , y = X . loc [ idxs_to_keep ], y . loc [ idxs_to_keep ] discharge . fit_and_save_model ( X , y , discharge_opt_model_fp ) s_discharge_profile = discharge . optimise_test_discharge_profile ( raw_data_dir , intermediate_data_dir , discharge_opt_model_fp , index = index ) s_discharge_profile . plot () charge_opt_model_fp = '../models/charge_opt.sav' X , y = charge . prepare_training_input_data ( intermediate_data_dir , start_hour = 5 ) idxs_to_keep = sorted ( list ( set ( X . index ) - set ( index ))) X , y = X . loc [ idxs_to_keep ], y . loc [ idxs_to_keep ] charge . fit_and_save_charging_model ( X , y , charge_opt_model_fp ) s_charge_profile = charge . optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , charge_opt_model_fp , index = index ) s_charge_profile . plot () We'll now create the charging profile using the PV forecast, in this instance we'll use a linear model for the solar forecast pv_model_fp = '../models/pv_model.sav' X , y = pv . prepare_training_input_data ( intermediate_data_dir , start_hour = 5 ) idxs_to_keep = sorted ( list ( set ( X . index ) - set ( index ))) X , y = X . loc [ idxs_to_keep ], y . loc [ idxs_to_keep ] pv . fit_and_save_pv_model ( X , y , pv_model_fp ) s_charge_profile = pv . optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , pv_model_fp , index = index ) s_charge_profile . plot () In this example we repeat the same procedure using a random forest instead of linear model pv_model_fp = '../models/pv_model.sav' X , y = pv . prepare_training_input_data ( intermediate_data_dir , start_hour = 5 ) idxs_to_keep = sorted ( list ( set ( X . index ) - set ( index ))) X , y = X . loc [ idxs_to_keep ], y . loc [ idxs_to_keep ] pv . fit_and_save_pv_model ( X , y , pv_model_fp , model_class = RandomForestRegressor ) s_charge_profile = pv . optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , pv_model_fp , index = index ) s_charge_profile . plot () submission = ( s_discharge_profile + s_charge_profile ) . to_frame ( name = 'charge_MW' ) df_results = evaluate_submission ( submission , intermediate_data_dir ) df_results df_results [ 'total_score' ] . mean () submission = ( s_discharge_profile + s_charge_profile ) . to_frame ( name = 'charge_MW' ) df_results = evaluate_submission ( submission , intermediate_data_dir ) df_results [ 'total_score' ] . mean () Finally we'll export the relevant code to our batopt module","title":"Tuning"},{"location":"06-tuning/#tuning","text":"#exports import json import numpy as np import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.pipeline import Pipeline from mlxtend.feature_selection import SequentialFeatureSelector as SFS from skopt.plots import plot_objective from skopt.space import Real , Categorical , Integer from batopt import clean , discharge , charge , pv , utils import os from ipypb import track import FEAutils as hlp import matplotlib.pyplot as plt from IPython.display import JSON","title":"Tuning"},{"location":"06-tuning/#user-inputs","text":"intermediate_data_dir = '../data/intermediate' raw_data_dir = '../data/raw' cache_data_dir = '../data/nb-cache'","title":"User Inputs"},{"location":"06-tuning/#preparing-data","text":"First we'll load in the target and feature data for both the charging and discharging models charge_x , charge_y = pv . prepare_training_input_data ( intermediate_data_dir ) discharge_x , discharge_y = discharge . prepare_training_input_data ( intermediate_data_dir ) charge_x . head () s_demand = clean . load_training_dataset ( intermediate_data_dir , 'demand' )[ 'demand_MW' ] s_demand . head () s_pv = clean . load_training_dataset ( intermediate_data_dir , 'pv' )[ 'pv_power_mw' ] s_pv . head () #exports def get_train_test_arr ( arr , start_of_test_period ): train_arr = arr [: pd . to_datetime ( start_of_test_period , utc = True )] test_arr = arr [ pd . to_datetime ( start_of_test_period , utc = True ):] return train_arr , test_arr def get_train_test_Xy ( X , y , start_of_test_period ): x_train , x_test = get_train_test_arr ( X , start_of_test_period ) y_train , y_test = get_train_test_arr ( y , start_of_test_period ) return x_train , x_test , y_train , y_test start_of_test_period = '2020-06-15' charge_x_train , charge_x_test , charge_y_train , charge_y_test = pv . get_train_test_Xy ( charge_x , charge_y , start_of_test_period ) discharge_x_train , discharge_x_test , discharge_y_train , discharge_y_test = pv . get_train_test_Xy ( discharge_x , discharge_y , start_of_test_period )","title":"Preparing Data"},{"location":"06-tuning/#evaluation-metrics","text":"We want to evaluate each of our models based on their contribution to the final scoring value, to do this we'll first create some predictions for our discharge model. discharge_rf = RandomForestRegressor () discharge_rf . fit ( discharge_x_train , discharge_y_train ) discharge_y_pred = pd . Series ( discharge_rf . predict ( discharge_x_test ), index = discharge_x_test . index ) discharge_y_pred . plot () We'll then create a time-series of the percentage peak reduction for each day #exports def calculate_pct_peak_reduction_s ( discharge_y_pred , s_demand ): s_demand_test = s_demand . loc [ discharge_y_pred . index ] s_old_peaks = s_demand_test . groupby ( s_demand_test . index . date ) . max () s_new_peaks = ( s_demand_test + discharge_y_pred ) . groupby ( s_demand_test . index . date ) . max () s_pct_peak_reduction = 100 * ( s_old_peaks - s_new_peaks ) / s_new_peaks s_pct_peak_reduction . index = pd . to_datetime ( s_pct_peak_reduction . index ) return s_pct_peak_reduction s_pct_peak_reduction = calculate_pct_peak_reduction_s ( discharge_y_pred , s_demand ) print ( f 'The average peak reduction was { s_pct_peak_reduction . mean () : .2f } %' ) s_pct_peak_reduction . plot () We'll then repeat this with the charging model charge_rf = RandomForestRegressor () charge_rf . fit ( charge_x_train , charge_y_train ) charge_y_pred = pd . Series ( charge_rf . predict ( charge_x_test ), index = charge_x_test . index ) charge_y_pred . plot () For which we'll calculate the emissions factor series #exports def calculate_emissions_factor_s ( charge_y_pred , s_pv , solar_factor = 3 , grid_factor = 1 ): s_solar_charge_pct = ( charge_y_pred - s_pv . loc [ charge_y_pred . index ]) . clip ( 0 ) . groupby ( charge_y_pred . index . date ) . sum () / charge_y_pred . groupby ( charge_y_pred . index . date ) . sum () s_grid_charge_pct = 1 - s_solar_charge_pct s_emissions_factor = solar_factor * s_solar_charge_pct + grid_factor * s_grid_charge_pct s_emissions_factor . index = pd . to_datetime ( s_emissions_factor . index ) return s_emissions_factor s_emissions_factor = calculate_emissions_factor_s ( charge_y_pred , s_pv ) s_emissions_factor . plot () We can then combine these two steps to determine our final score for each day s_score = calculate_score_s ( discharge_y_pred , charge_y_pred , s_demand , s_pv ) print ( f 'The average score was: { s_score . mean () : .2f } ' ) s_score . plot () For the charging we can also look at how much was sourced from PV relative to the potential maximum (capped at 6 MWh per day). solar_charge = np . minimum ( charge_y_pred , s_pv . loc [ charge_y_pred . index ]) day_solar_charge = solar_charge . groupby ( solar_charge . index . date ) . sum () . clip ( 0 , 12 ) day_solar_charge . index = pd . to_datetime ( day_solar_charge . index ) solar_potential = np . clip ( s_pv . loc [ charge_y_pred . index ], 0 , 2.5 ) day_solar_potential = solar_potential . groupby ( solar_potential . index . date ) . sum () . clip ( 0 , 12 ) day_solar_potential . index = pd . to_datetime ( day_solar_potential . index ) day_solar_charge . plot () day_solar_potential . plot () pct_exploit = 100 * day_solar_charge / day_solar_potential pct_exploit . plot () plt . ylabel ( ' % e xploited' ) #exports def score_charge ( schedule , solar_profile , solar_factor = 3 , grid_factor = 1 ): # The actual pv charge is the minimum of the scheduled charge and the actual solar availability actual_pv_charge = np . minimum ( schedule . values , solar_profile . values ) actual_pv_charge = pd . Series ( actual_pv_charge , index = schedule . index ) pct_pv_charge = actual_pv_charge . groupby ( actual_pv_charge . index . date ) . sum () / schedule . groupby ( schedule . index . date ) . sum () pct_grid_charge = 1 - pct_pv_charge score = ( solar_factor * pct_pv_charge ) + ( grid_factor * pct_grid_charge ) return score def score_discharge ( schedule , demand ): new_demand = schedule + demand old_demand = demand new_peaks = new_demand . groupby ( new_demand . index . date ) . max () old_peaks = old_demand . groupby ( old_demand . index . date ) . max () pct_reduction = 100 * (( old_peaks - new_peaks ) / old_peaks ) return pct_reduction def max_charge_score ( solar_profile , solar_factor = 3 , grid_factor = 1 , capacity = 6 , time_unit = 0.5 ): pv_potential = solar_profile . groupby ( solar_profile . index . date ) . sum () . clip ( 0 , capacity / time_unit ) pct_pv_charge = pv_potential / ( capacity / time_unit ) pct_grid_charge = 1 - pct_pv_charge score = ( solar_factor * pct_pv_charge ) + ( grid_factor * pct_grid_charge ) return score def calculate_score_s ( discharge_y_pred , charge_y_pred , s_demand , s_pv , solar_factor = 3 , grid_factor = 1 ): charge_score = score_charge ( charge_y_pred , s_pv , solar_factor , grid_factor ) discharge_score = score_discharge ( discharge_y_pred , s_demand ) s_score = discharge_score * charge_score return s_score , charge_score , discharge_score def evaluate_submission ( submission , intermediate_data_dir ): if isinstance ( submission , str ): df_solution = pd . read_csv ( submission ) df_solution = df_solution . set_index ( pd . to_datetime ( df_solution . datetime , utc = True )) else : assert isinstance ( submission , pd . DataFrame ), '`submission` must either be a valid submission dataframe or a filepath to the submission' df_solution = submission df_real = clean . combine_training_datasets ( intermediate_data_dir ) df_real = df_real [ df_real . index . isin ( df_solution . index )] df_solution_charge = df_solution . between_time ( '00:00' , '15:00' ) df_solution_discharge = df_solution . between_time ( '15:30' , '20:30' ) df_real_charge = df_real . between_time ( '00:00' , '15:00' ) df_real_discharge = df_real . between_time ( '15:30' , '20:30' ) total_score , charge_score , discharge_score = calculate_score_s ( df_solution_discharge . charge_MW , df_solution_charge . charge_MW , df_real_discharge . demand_MW , df_real_charge . pv_power_mw ) df_results = pd . DataFrame ({ 'total_score' : total_score , 'charge_score' : charge_score , 'discharge_score' : discharge_score , 'max_charge_score' : max_charge_score ( df_real_charge . pv_power_mw ) }) return df_results submission_fp = '../data/output/ESAIL_set1.csv' df_results = evaluate_submission ( submission_fp , intermediate_data_dir ) df_results We can then calculate our average score over this period df_results [ 'total_score' ] . mean ()","title":"Evaluation Metrics"},{"location":"06-tuning/#discharge-model-tuning","text":"We'll begin by carrying out some feature selection #exports def feature_selection ( x_train , y_train , groups = None , model = RandomForestRegressor (), min_num_features = 1 , max_num_features = None , ** sfs_kwargs ): if max_num_features is None : max_num_features = 1 + x_train . shape [ 1 ] result_features = dict () result_scores = dict () for num_features in track ( range ( min_num_features , max_num_features )): sfs = SFS ( model , k_features = num_features , ** sfs_kwargs ) sfs . fit ( x_train , y_train , groups = groups ) result_features [ num_features ] = sfs . k_feature_names_ result_scores [ num_features ] = sfs . k_score_ return result_features , result_scores peak_reduction_scorer = discharge . construct_peak_reduction_calculator ( s_demand = s_demand . loc [ discharge_x_train . index ], scorer = True ) week_groups = discharge_x_train . index . year + discharge_x_train . index . isocalendar () . week / 52 rerun_feature_selection = False feature_selection_filename = f 'feature_selection.json' if ( rerun_feature_selection == True ) or ( feature_selection_filename not in os . listdir ( cache_data_dir )): result_features , result_scores = feature_selection ( discharge_x_train , discharge_y_train , groups = week_groups , n_jobs =- 1 ) with open ( f ' { cache_data_dir } / { feature_selection_filename } ' , 'w' ) as fp : json . dump ( dict ( zip ([ 'features' , 'scores' ], [ result_features , result_scores ])), fp ) else : with open ( f ' { cache_data_dir } / { feature_selection_filename } ' , 'r' ) as fp : results = json . load ( fp ) result_features , result_scores = results [ 'features' ], results [ 'scores' ] We can visualise how the model accuracy changes with the number of features included pd . Series ( result_scores ) . plot () We'll also calculate the relative importance of each feature by counting how many times they were included in the optimal feature subset flatten_iterables = lambda iterable : [ item for sublist in list ( iterable ) for item in sublist ] s_feature_importance = pd . Series ( flatten_iterables ( result_features . values ())) . value_counts () . divide ( len ( result_features )) s_feature_importance We'll now do some hyper-parameter tuning using the skopt library features = s_feature_importance . index [: 11 ] evening_datetimes = discharge . extract_evening_datetimes ( discharge_x_train ) week_groups = discharge_x_train . index . year + discharge_x_train . index . isocalendar () . week / 52 peak_reduction_scorer = discharge . construct_peak_reduction_calculator ( s_demand = s_demand , scorer = True ) pipeline = Pipeline ([ # Add in oversampling of more recent/similar dates ( 'pandas_RF' , utils . PandasRandomForestRegressor ()) ]) search_spaces = { 'pandas_RF__min_samples_leaf' : Integer ( 1 , 20 , 'uniform' ), 'pandas_RF__criterion' : Categorical ([ 'mse' , 'mae' ]), 'pandas_RF__n_estimators' : Integer ( 50 , 250 , 'uniform' ), 'pandas_RF__max_features' : Categorical ([ 'auto' , 'sqrt' ]), 'pandas_RF__max_depth' : Integer ( 10 , 50 , 'uniform' ), 'pandas_RF__min_samples_split' : Integer ( 2 , 10 , 'uniform' ), 'pandas_RF__min_samples_leaf' : Integer ( 1 , 4 , 'uniform' ), 'pandas_RF__bootstrap' : Categorical ([ True , False ]) } opt = utils . BayesSearchCV ( pipeline , search_spaces , n_iter = 15 , verbose = 1 , cv = 8 , # 8 works well for me as that's how many concurrent workers I can use scoring = peak_reduction_scorer , n_jobs =- 1 ) fit_BayesSearchCV = False if fit_BayesSearchCV == True : opt . fit ( discharge_x_train [ features ], discharge_y_train , groups = evening_datetimes . date ) print ( f 'Cross-validation score: { opt . best_score_ : .2f } ' ) print ( f 'Hold-out score: { opt . score ( discharge_x_test [ features ], discharge_y_test ) : .2f } ' ) print ( f ' \\n Best params: \\n { opt . best_params_ } ' ) # want to be saving model runs # could include as part of a callback?","title":"Discharge Model Tuning"},{"location":"06-tuning/#model-comparisons","text":"Here we'll compare our discharge v pv-forecast modelling approaches index = pd . date_range ( '2019-03-02' , '2019-03-09 23:30' , freq = '30T' , tz = 'UTC' ) %% time discharge_opt_model_fp = '../models/discharge_opt.sav' X , y = discharge . prepare_training_input_data ( intermediate_data_dir ) idxs_to_keep = sorted ( list ( set ( X . index ) - set ( index ))) X , y = X . loc [ idxs_to_keep ], y . loc [ idxs_to_keep ] discharge . fit_and_save_model ( X , y , discharge_opt_model_fp ) s_discharge_profile = discharge . optimise_test_discharge_profile ( raw_data_dir , intermediate_data_dir , discharge_opt_model_fp , index = index ) s_discharge_profile . plot () charge_opt_model_fp = '../models/charge_opt.sav' X , y = charge . prepare_training_input_data ( intermediate_data_dir , start_hour = 5 ) idxs_to_keep = sorted ( list ( set ( X . index ) - set ( index ))) X , y = X . loc [ idxs_to_keep ], y . loc [ idxs_to_keep ] charge . fit_and_save_charging_model ( X , y , charge_opt_model_fp ) s_charge_profile = charge . optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , charge_opt_model_fp , index = index ) s_charge_profile . plot () We'll now create the charging profile using the PV forecast, in this instance we'll use a linear model for the solar forecast pv_model_fp = '../models/pv_model.sav' X , y = pv . prepare_training_input_data ( intermediate_data_dir , start_hour = 5 ) idxs_to_keep = sorted ( list ( set ( X . index ) - set ( index ))) X , y = X . loc [ idxs_to_keep ], y . loc [ idxs_to_keep ] pv . fit_and_save_pv_model ( X , y , pv_model_fp ) s_charge_profile = pv . optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , pv_model_fp , index = index ) s_charge_profile . plot () In this example we repeat the same procedure using a random forest instead of linear model pv_model_fp = '../models/pv_model.sav' X , y = pv . prepare_training_input_data ( intermediate_data_dir , start_hour = 5 ) idxs_to_keep = sorted ( list ( set ( X . index ) - set ( index ))) X , y = X . loc [ idxs_to_keep ], y . loc [ idxs_to_keep ] pv . fit_and_save_pv_model ( X , y , pv_model_fp , model_class = RandomForestRegressor ) s_charge_profile = pv . optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , pv_model_fp , index = index ) s_charge_profile . plot () submission = ( s_discharge_profile + s_charge_profile ) . to_frame ( name = 'charge_MW' ) df_results = evaluate_submission ( submission , intermediate_data_dir ) df_results df_results [ 'total_score' ] . mean () submission = ( s_discharge_profile + s_charge_profile ) . to_frame ( name = 'charge_MW' ) df_results = evaluate_submission ( submission , intermediate_data_dir ) df_results [ 'total_score' ] . mean () Finally we'll export the relevant code to our batopt module","title":"Model Comparisons"},{"location":"07-pv-forecast/","text":"Charging with PV Forecast \u00b6 Imports \u00b6 #exports import numpy as np import pandas as pd import os import matplotlib.pyplot as plt import seaborn as sns import joblib from moepy.lowess import quantile_model from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression , Lasso , Ridge from sklearn.metrics import make_scorer , r2_score , mean_absolute_error , mean_squared_error from sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor from sklearn.model_selection import GroupKFold from mlxtend.feature_selection import SequentialFeatureSelector as SFS from skopt.plots import plot_objective from skopt.space import Real , Categorical , Integer from batopt import clean , discharge , utils , charge import FEAutils as hlp from ipypb import track User inputs \u00b6 raw_data_dir = '../data/raw' intermediate_data_dir = '../data/intermediate' cache_data_dir = '../data/nb-cache' charge_opt_model_fp = '../models/charge_opt.sav' Fitting forecast model \u00b6 We'll first load the input data #exports def construct_df_charge_features ( df , dt_rng = None ): if dt_rng is None : dt_rng = pd . date_range ( df . index . min (), df . index . max (), freq = '30T' ) df_features = pd . DataFrame ( index = dt_rng ) # Adding temperature data temp_loc_cols = df . columns [ df . columns . str . contains ( 'temp_location' )] df_features . loc [ df . index , temp_loc_cols ] = df [ temp_loc_cols ] . copy () df_features = df_features . ffill ( limit = 1 ) # Adding solar irradiance data solar_loc_cols = df . columns [ df . columns . str . contains ( 'solar_location' )] df_features . loc [ df . index , solar_loc_cols ] = df [ solar_loc_cols ] . copy () df_features = df_features . ffill ( limit = 1 ) # Adding avg solar from previous week df_features [ 'pv_7d_lag' ] = df [ 'pv_power_mw' ] . rolling ( 48 * 7 ) . mean () . shift ( 48 * 7 ) # Adding datetime features dts = df_features . index df_features [ 'hour' ] = dts . hour + dts . minute / 60 df_features [ 'doy' ] = dts . dayofyear # Removing some extraneous features - found not be particularly useful cols = [ c for c in df_features . columns if 'solar_location4' not in c and 'solar_location1' not in c ] df_features = df_features . filter ( cols ) # Removing NaN values df_features = df_features . dropna () return df_features def prepare_training_input_data ( intermediate_data_dir , start_hour = 5 ): # Loading input data df = clean . combine_training_datasets ( intermediate_data_dir ) . interpolate ( limit = 1 ) df_features = construct_df_charge_features ( df ) # Filtering for overlapping feature and target data dt_idx = pd . date_range ( df_features . index . min (), df [ 'pv_power_mw' ] . dropna () . index . max () - pd . Timedelta ( minutes = 30 ), freq = '30T' ) s_pv = df . loc [ dt_idx , 'pv_power_mw' ] df_features = df_features . loc [ dt_idx ] # Filtering for evening datetimes charging_datetimes = charge . extract_charging_datetimes ( df_features , start_hour = start_hour ) X = df_features . loc [ charging_datetimes ] y = s_pv . loc [ charging_datetimes ] return X , y X , y = prepare_training_input_data ( intermediate_data_dir ) X . head () temp_location1 temp_location2 temp_location3 temp_location4 temp_location5 temp_location6 solar_location2 solar_location3 solar_location5 solar_location6 pv_7d_lag weekend dow hour doy 2017-11-17 05:00:00+00:00 3.79 4.28 2.97 1.25 9.3 9.77 0 0 0 0 0.334792 0 4 5 321 2017-11-17 05:30:00+00:00 3.555 3.95 2.82 0.98 9.31 9.755 0 0 0 0 0.334792 0 4 5.5 321 2017-11-17 06:00:00+00:00 3.32 3.62 2.67 0.71 9.32 9.74 0 0 0 0 0.334792 0 4 6 321 2017-11-17 06:30:00+00:00 3.255 3.535 2.675 0.78 9.34 9.74 0.155 0.075 0.12 0.255 0.334792 0 4 6.5 321 2017-11-17 07:00:00+00:00 3.19 3.45 2.68 0.85 9.36 9.74 0.31 0.15 0.24 0.51 0.334732 0 4 7 321 We'll create a quick baseline PV forecast df_pred_LR = clean . generate_kfold_preds ( X . values , y . values , RandomForestRegressor (), index = X . index ) df_pred_LR . head () pred true 2017-11-17 05:00:00+00:00 0 0 2017-11-17 05:30:00+00:00 0 0 2017-11-17 06:00:00+00:00 0 0 2017-11-17 06:30:00+00:00 0 0 2017-11-17 07:00:00+00:00 0 0 Analysing the predictions plt . scatter ( df_pred . true , df_pred . pred , s = 0.1 ) plt . xlabel ( 'Observation' ) plt . ylabel ( 'Prediction' ) Text(0, 0.5, 'Prediction') We'll also visualise what the prediction looks like for randomly selected days #exports def plot_random_day ( df_pred , ax = None ): \"\"\" View predicted and observed PV profiles \"\"\" if ax is None : ax = plt . gca () random_day_idx = pd . to_datetime ( np . random . choice ( df_pred . index . date )) df_random_day = df_pred [ df_pred . index . date == random_day_idx ] df_random_day [ 'true' ] . plot ( ax = ax ) df_random_day [ 'pred' ] . plot ( ax = ax ) return ax plot_random_day ( df_pred ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x2c0e0225730> We'll then carry out the second-stage for our model, the solar peak flattening random_solar_profile = discharge . sample_random_day ( df_pred . pred ) . pipe ( charge . extract_solar_profile ) adj_random_solar_profile = discharge . flatten_peak ( random_solar_profile ) charge_profile = charge . construct_charge_profile ( random_solar_profile , adj_random_solar_profile ) plt . plot ( charge_profile ) [<matplotlib.lines.Line2D at 0x1227af520>] Predicting charge based on PV forecast \u00b6 Now we will begin developing a unified approach for predicting PV and then optimising the battery charge schedule. We will also group by week. This should make the problem a bit harder, and help encourage the final model to generalise to lengthy unseen periods. #exports def generate_kfold_preds_weeks ( X , y , model , groups , kfold_kwargs , index = None ): \"\"\" Generate kfold preds, grouping by week \"\"\" group_kfold = GroupKFold ( ** kfold_kwargs ) df_pred = pd . DataFrame ( columns = [ 'pred' , 'true' ], index = np . arange ( X . shape [ 0 ])) for train_index , test_index in group_kfold . split ( X , y , groups ): X_train , X_test = X [ train_index ], X [ test_index ] y_train , y_test = y [ train_index ], y [ test_index ] model . fit ( X_train , y_train ) df_pred . loc [ test_index , 'true' ] = y_test df_pred . loc [ test_index , 'pred' ] = model . predict ( X_test ) df_pred . sort_index () if index is not None : assert len ( index ) == df_pred . shape [ 0 ], 'The passed index must be the same length as X and y' df_pred . index = index return df_pred def generate_kfold_charge_preds ( X , y , model , groups , kfold_kwargs = { 'n_splits' : 5 }): \"\"\" Fit the PV forecasting model and calculate the optimal charge profile for predictions. \"\"\" df_pred = generate_kfold_preds_weeks ( X . values , y . values , model , groups , kfold_kwargs = kfold_kwargs , index = X . index ) charge_pred = charge . construct_charge_s ( df_pred . pred ) charge_pred = charge . post_pred_charge_proc_func ( charge_pred ) return pd . DataFrame ({ 'charge_pred' : charge_pred , 'pv_actual' : df_pred . true , 'pv_pred' : df_pred . pred }) def predict_charge ( X , model ): \"\"\" Given a fitted PV forecast model and feature array X, get the optimal charge profile. \"\"\" pv_pred = pd . Series ( model . predict ( X ), index = X . index ) charge_pred = charge . construct_charge_s ( pv_pred ) charge_pred = charge . post_pred_charge_proc_func ( charge_pred ) return pd . Series ( charge_pred , index = X . index ) We'll also create a helper function for our test/train split based on time #exports def get_train_test_arr ( arr , start_of_test_period ): train_arr = arr [: pd . to_datetime ( start_of_test_period , utc = True )] test_arr = arr [ pd . to_datetime ( start_of_test_period , utc = True ):] return train_arr , test_arr def get_train_test_Xy ( X , y , start_of_test_period ): x_train , x_test = get_train_test_arr ( X , start_of_test_period ) y_train , y_test = get_train_test_arr ( y , start_of_test_period ) return x_train , x_test , y_train , y_test start_of_test_period = '2019-02-04' X_train , X_test , y_train , y_test = get_train_test_Xy ( X , y , start_of_test_period ) Now let's try executing this unified approach using k-fold CV, for 3 default models on the training data: models = { 'std_linear' : LinearRegression (), 'lasso' : Lasso (), 'ridge' : Ridge (), 'boosted' : GradientBoostingRegressor (), 'random_forest' : RandomForestRegressor (), } # Define the week groups week_groups = X_train . index . year + X_train . index . isocalendar () . week / 52 for key in models : charge_pred_df = generate_kfold_charge_preds ( X_train , y_train , models [ key ], week_groups ) score = charge . score_charging ( charge_pred_df . charge_pred , charge_pred_df . pv_actual ) pv_mse = np . mean ( np . square ( charge_pred_df . pv_actual - charge_pred_df . pv_pred )) solar_exploit_pct = 100 * charge . prop_max_solar ( charge_pred_df . charge_pred , charge_pred_df . pv_actual ) print ( \" {} : PV MSE: {:.2f} , score: {:.2f} , solar exploit: {:.2f} %\" . format ( key , pv_mse , score , solar_exploit_pct )) std_linear: PV MSE: 0.35, score: 0.77, solar exploit: 94.86% lasso: PV MSE: 0.39, score: 0.76, solar exploit: 93.87% ridge: PV MSE: 0.35, score: 0.77, solar exploit: 94.86% boosted: PV MSE: 0.36, score: 0.77, solar exploit: 94.48% random_forest: PV MSE: 0.35, score: 0.76, solar exploit: 94.35% Interestingly, there is little difference between the models in terms of solar exploit, even though there are some differences in the MSE of the PV forecasts. For our previous attempt at the charging task, the linear model was much worse than the boosted model and RF in terms of solar exploit. This suggests that a weak (or under-fitted) estimator of solar PV actually performs quite well when it comes to best_model = RandomForestRegressor () best_model . fit ( X_train , y_train ) preds = predict_charge ( X_test , best_model ) 100 * charge . prop_max_solar ( preds , y_test ) 95.64718850989254 pd . Series ( dict ( zip ( X_test . columns , best_model . feature_importances_ ))) . sort_values ( ascending = False ) solar_location2 0.656734 solar_location6 0.057420 solar_location5 0.048603 hour 0.036628 pv_7d_lag 0.031805 solar_location3 0.031244 doy 0.030441 temp_location4 0.019506 temp_location6 0.018710 temp_location5 0.016729 temp_location1 0.014523 temp_location3 0.013219 dow 0.012169 temp_location2 0.011155 weekend 0.001112 dtype: float64 Running the above analysis it seems like solar_locations 1 and 4 do not contribute much at all to the regression models: both are over an order of magnitude smaller than the others. Best to remove these when processing the data (for the moment this is charge.py features = [ c for c in X_train . columns if 'solar_location4' not in c and 'solar_location1' not in c ] X_train_reduced , X_test_reduced = X_train . filter ( features ), X_test . filter ( features ) charge_pred_df = generate_kfold_charge_preds ( X_train_reduced , y_train , LinearRegression (), week_groups ) score = charge . score_charging ( charge_pred_df . charge_pred , charge_pred_df . pv_actual ) pv_mse = np . mean ( np . square ( charge_pred_df . pv_actual - charge_pred_df . pv_pred )) solar_exploit_pct = 100 * charge . prop_max_solar ( charge_pred_df . charge_pred , charge_pred_df . pv_actual ) print ( \"PV MSE: {:.2f} , score: {:.2f} , solar exploit: {:.2f} %\" . format ( pv_mse , score , solar_exploit_pct )) Tuned RF model \u00b6 rf_params = { 'criterion' : 'mae' , 'min_samples_leaf' : 46 , 'min_samples_split' : 2 , 'n_estimators' : 150 } best_model = RandomForestRegressor ( ** rf_params ) best_model . fit ( X_train , y_train ) preds = predict_charge ( X_test , best_model ) 100 * charge . prop_max_solar ( preds , y_test ) def construct_solar_exploit_calculator ( solar_profile , charging_datetimes = None , scorer = False ): if charging_datetimes is None : charging_datetimes = charge . extract_charging_datetimes ( solar_profile ) def calc_solar_exploitation ( y , y_pred ): # Checking evening datetimes if hasattr ( y_pred , 'index' ) == True : charging_datetimes = charge . extract_charging_datetimes ( y_pred ) assert y_pred . shape [ 0 ] == solar_profile . loc [ charging_datetimes ] . shape [ 0 ], f 'The prediction series must be the same length as the number of charging datetimes in the main dataframe, { y_pred . shape [ 0 ] } { s_demand . loc [ evening_datetimes ] . shape [ 0 ] } ' charge_pred = charge . construct_charge_s ( y_pred ) charge_pred = charge . post_pred_charge_proc_func ( charge_pred ) exploitation_pct = 100 * charge . prop_max_solar ( charge_pred , solar_profile . loc [ charging_datetimes ]) return exploitation_pct if scorer == True : return make_scorer ( calc_solar_exploitation ) else : return calc_solar_exploitation Feature Selection \u00b6 It seems like overfitting could be a substantial issue for charging. Trying some feature selection: model = Lasso ( alpha = 0.5 ) model . fit ( X_train , y_train ) coefs_df = pd . DataFrame ({ 'feature' : X_train . columns , 'coefs' : model . coef_ }) features = coefs_df [ abs ( coefs_df . coefs ) > 0 ] . feature features 9 solar_location5 15 solar_location2_rolling 16 solar_location3_rolling 17 solar_location5_rolling 18 solar_location6_rolling 22 temp_location4_rolling Name: feature, dtype: object # features = [f for f in X_train.columns if 'temp' not in f] def prop_max_solar_df ( charge_pred_df ): return charge . prop_max_solar ( charge_pred_df . charge_pred , charge_pred_df . pv_actual ) X_train_reduced = X_train . filter ( features ) models = { 'std_linear' : LinearRegression (), 'lasso' : Lasso (), 'boosted' : GradientBoostingRegressor (), 'random_forest' : RandomForestRegressor (), } # Define the week groups week_groups = X_train . index . year + X_train . index . isocalendar () . week / 52 for key in models : charge_pred_df = generate_kfold_charge_preds ( X_train , y_train , models [ key ], week_groups ) score = charge . score_charging ( charge_pred_df . charge_pred , charge_pred_df . pv_actual ) pv_mse = np . mean ( np . square ( charge_pred_df . pv_actual - charge_pred_df . pv_pred )) solar_exploit_pct = 100 * charge_pred_df . groupby ( charge_pred_df . index . date ) . apply ( prop_max_solar_df ) print ( \" {} : PV MSE: {:.2f} , score: {:.2f} , solar exploit: {:.2f} %, std. solar exploit: {:.2f} \" . format ( key , pv_mse , score , solar_exploit_pct . mean (), solar_exploit_pct . std ())) std_linear: PV MSE: 0.34, score: 0.77, solar exploit: 94.98%, std. solar exploit: 7.88 lasso: PV MSE: 0.37, score: 0.76, solar exploit: 94.52%, std. solar exploit: 8.43 boosted: PV MSE: 0.35, score: 0.77, solar exploit: 94.85%, std. solar exploit: 8.26 random_forest: PV MSE: 0.35, score: 0.76, solar exploit: 94.74%, std. solar exploit: 8.35 model = LinearRegression () charge_pred_df = generate_kfold_charge_preds ( X_train , y_train , model , week_groups ) fig , axs = plt . subplots ( 1 , 5 , figsize = ( 15 , 3 ), dpi = 125 ) for i in range ( 5 ): random_day = pd . to_datetime ( np . random . choice ( charge_pred_df . index . date )) random_df = charge_pred_df [ charge_pred_df . index . date == random_day ] random_df [ 'pv_actual' ] . plot ( ax = axs [ i ]) random_df [ 'charge_pred' ] . plot ( ax = axs [ i ]) #exports def predict_charge ( X , model ): \"\"\" Given a fitted PV forecast model and feature array X, get the optimal charge profile. \"\"\" pv_pred = pd . Series ( model . predict ( X ), index = X . index ) charge_pred = charge . construct_charge_s ( pv_pred ) charge_pred = charge . post_pred_charge_proc_func ( charge_pred ) return pd . Series ( charge_pred , index = X . index ) model = LinearRegression () model . fit ( X_train , y_train ) pv_pred = model . predict ( X_test ) charge_pred = predict_charge ( X_test , model ) charge_pred_df = pd . DataFrame ({ 'charge_pred' : charge_pred , 'pv_pred' : pv_pred , 'pv_actual' : y_test }) solar_exploit_pct = 100 * charge_pred_df . groupby ( charge_pred_df . index . date ) . apply ( prop_max_solar_df ) print ( \"Held out solar exploit: {:.2f} %\" . format ( solar_exploit_pct . mean (), )) print ( \"Held out solar exploit (std): {:.2f} \" . format ( solar_exploit_pct . std (), )) Held out solar exploit: 93.98% Held out solar exploit (std): 8.19 N = 8 fig , axs = plt . subplots ( 1 , N , figsize = ( 15 , 3 ), dpi = 125 , sharey = True ) for i in range ( N ): random_day = pd . to_datetime ( np . random . choice ( charge_pred_df . index . date , replace = True )) random_df = charge_pred_df [ charge_pred_df . index . date == random_day ] axs [ i ] . plot ( random_df . pv_actual , c = 'b' , alpha = 1 ) axs [ i ] . plot ( random_df . pv_pred , c = 'b' , linestyle = '--' , alpha = 0.2 ) axs [ i ] . plot ( random_df . charge_pred , c = 'g' ) axs [ i ] . axhline ( 2.5 , c = 'r' , linestyle = ':' ) axs [ i ] . set_xticks ([]) ## X, y = prepare_training_input_data(intermediate_data_dir) start_of_test_period = '2019-02-04' X_train , X_test = get_train_test ( X , start_of_test_period ) y_train , y_test = get_train_test ( y , start_of_test_period ) charging_datetimes = charge . extract_charging_datetimes ( X_train ) solar_exploit_scorer = construct_solar_exploit_calculator ( solar_profile = y , charging_datetimes = charging_datetimes , scorer = True ) groups = charging_datetimes . date pipeline = Pipeline ([ ( 'pandas_RF' , utils . PandasRandomForestRegressor ()) ]) search_spaces = { 'pandas_RF__min_samples_leaf' : Integer ( 1 , 20 , 'uniform' ), 'pandas_RF__criterion' : Categorical ([ 'mse' , 'mae' ]), 'pandas_RF__n_estimators' : Integer ( 10 , 150 , 'uniform' ), 'pandas_RF__max_features' : Categorical ([ 'auto' , 'sqrt' ]), 'pandas_RF__max_depth' : Integer ( 5 , 200 , 'uniform' ), 'pandas_RF__min_samples_split' : Integer ( 2 , 10 , 'uniform' ), 'pandas_RF__min_samples_leaf' : Integer ( 1 , 4 , 'uniform' ), 'pandas_RF__bootstrap' : Categorical ([ True , False ]) } opt = utils . BayesSearchCV ( pipeline , search_spaces , n_iter = 10 , verbose = 1 , cv = 4 , scoring = solar_exploit_scorer , n_jobs =- 1 ) fit_BayesSearchCV = False if fit_BayesSearchCV == True : opt . fit ( X_train , y_train , groups = groups ) print ( f 'validation score: { opt . best_score_ } ' ) print ( f 'held out score: { opt . score ( X_test , y_test ) } ' ) print ( f 'best params: { opt . best_params_ } ' ) _ = plot_objective ( opt . optimizer_results_ [ 0 ]) plt . show () #exports def fit_and_save_pv_model ( X , y , pv_model_fp , model_class = LinearRegression , ** model_params ): model = model_class ( ** model_params ) model . fit ( X , y ) with open ( pv_model_fp , 'wb' ) as fp : joblib . dump ( model , fp ) return #exports #exports def prepare_test_feature_data ( raw_data_dir , intermediate_data_dir , test_start_date = None , test_end_date = None , start_time = '08:00' , end_time = '23:59' ): # Loading input data df = clean . combine_training_datasets ( intermediate_data_dir ) . interpolate ( limit = 1 ) df_features = construct_df_charge_features ( df ) # Loading default index (latest submission) if test_end_date is None or test_start_date is None : index = discharge . load_latest_submission_template ( raw_data_dir ) . index else : index = df_features [ test_start_date : test_end_date ] . index # Filtering feature data on submission datetimes df_features = df_features . loc [ index ] . between_time ( start_time , end_time ) return df_features def optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , pv_model_fp , test_start_date = None , test_end_date = None , start_time = '08:00' , end_time = '23:59' ): df_features = prepare_test_feature_data ( raw_data_dir , intermediate_data_dir , test_start_date = test_start_date , test_end_date = test_end_date , start_time = start_time , end_time = end_time ) charging_datetimes = charge . extract_charging_datetimes ( df_features ) X_test = df_features . loc [ charging_datetimes ] model = discharge . load_trained_model ( pv_model_fp ) charge_profile = predict_charge ( X_test , model ) s_charge_profile = pd . Series ( charge_profile , index = charging_datetimes ) s_charge_profile = s_charge_profile . reindex ( df_features . index ) . fillna ( 0 ) s_charge_profile = charge . post_pred_charge_proc_func ( s_charge_profile ) assert charge . charge_is_valid ( s_charge_profile ), \"Charging profile is invalid\" return s_charge_profile pv_model_fp = '../models/pv_model.sav' s_charge_profile = optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , pv_model_fp ) s_charge_profile . plot () --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-6-8db658a0274e> in <module> 1 pv_model_fp = '../models/pv_model.sav' ----> 2 s_charge_profile = optimise_test_charge_profile(intermediate_data_dir, pv_model_fp) 3 4 s_charge_profile.plot() NameError: name 'intermediate_data_dir' is not defined Finally we'll export the relevant code to our batopt module","title":"PV"},{"location":"07-pv-forecast/#charging-with-pv-forecast","text":"","title":"Charging with PV Forecast"},{"location":"07-pv-forecast/#imports","text":"#exports import numpy as np import pandas as pd import os import matplotlib.pyplot as plt import seaborn as sns import joblib from moepy.lowess import quantile_model from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression , Lasso , Ridge from sklearn.metrics import make_scorer , r2_score , mean_absolute_error , mean_squared_error from sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor from sklearn.model_selection import GroupKFold from mlxtend.feature_selection import SequentialFeatureSelector as SFS from skopt.plots import plot_objective from skopt.space import Real , Categorical , Integer from batopt import clean , discharge , utils , charge import FEAutils as hlp from ipypb import track","title":"Imports"},{"location":"07-pv-forecast/#user-inputs","text":"raw_data_dir = '../data/raw' intermediate_data_dir = '../data/intermediate' cache_data_dir = '../data/nb-cache' charge_opt_model_fp = '../models/charge_opt.sav'","title":"User inputs"},{"location":"07-pv-forecast/#fitting-forecast-model","text":"We'll first load the input data #exports def construct_df_charge_features ( df , dt_rng = None ): if dt_rng is None : dt_rng = pd . date_range ( df . index . min (), df . index . max (), freq = '30T' ) df_features = pd . DataFrame ( index = dt_rng ) # Adding temperature data temp_loc_cols = df . columns [ df . columns . str . contains ( 'temp_location' )] df_features . loc [ df . index , temp_loc_cols ] = df [ temp_loc_cols ] . copy () df_features = df_features . ffill ( limit = 1 ) # Adding solar irradiance data solar_loc_cols = df . columns [ df . columns . str . contains ( 'solar_location' )] df_features . loc [ df . index , solar_loc_cols ] = df [ solar_loc_cols ] . copy () df_features = df_features . ffill ( limit = 1 ) # Adding avg solar from previous week df_features [ 'pv_7d_lag' ] = df [ 'pv_power_mw' ] . rolling ( 48 * 7 ) . mean () . shift ( 48 * 7 ) # Adding datetime features dts = df_features . index df_features [ 'hour' ] = dts . hour + dts . minute / 60 df_features [ 'doy' ] = dts . dayofyear # Removing some extraneous features - found not be particularly useful cols = [ c for c in df_features . columns if 'solar_location4' not in c and 'solar_location1' not in c ] df_features = df_features . filter ( cols ) # Removing NaN values df_features = df_features . dropna () return df_features def prepare_training_input_data ( intermediate_data_dir , start_hour = 5 ): # Loading input data df = clean . combine_training_datasets ( intermediate_data_dir ) . interpolate ( limit = 1 ) df_features = construct_df_charge_features ( df ) # Filtering for overlapping feature and target data dt_idx = pd . date_range ( df_features . index . min (), df [ 'pv_power_mw' ] . dropna () . index . max () - pd . Timedelta ( minutes = 30 ), freq = '30T' ) s_pv = df . loc [ dt_idx , 'pv_power_mw' ] df_features = df_features . loc [ dt_idx ] # Filtering for evening datetimes charging_datetimes = charge . extract_charging_datetimes ( df_features , start_hour = start_hour ) X = df_features . loc [ charging_datetimes ] y = s_pv . loc [ charging_datetimes ] return X , y X , y = prepare_training_input_data ( intermediate_data_dir ) X . head () temp_location1 temp_location2 temp_location3 temp_location4 temp_location5 temp_location6 solar_location2 solar_location3 solar_location5 solar_location6 pv_7d_lag weekend dow hour doy 2017-11-17 05:00:00+00:00 3.79 4.28 2.97 1.25 9.3 9.77 0 0 0 0 0.334792 0 4 5 321 2017-11-17 05:30:00+00:00 3.555 3.95 2.82 0.98 9.31 9.755 0 0 0 0 0.334792 0 4 5.5 321 2017-11-17 06:00:00+00:00 3.32 3.62 2.67 0.71 9.32 9.74 0 0 0 0 0.334792 0 4 6 321 2017-11-17 06:30:00+00:00 3.255 3.535 2.675 0.78 9.34 9.74 0.155 0.075 0.12 0.255 0.334792 0 4 6.5 321 2017-11-17 07:00:00+00:00 3.19 3.45 2.68 0.85 9.36 9.74 0.31 0.15 0.24 0.51 0.334732 0 4 7 321 We'll create a quick baseline PV forecast df_pred_LR = clean . generate_kfold_preds ( X . values , y . values , RandomForestRegressor (), index = X . index ) df_pred_LR . head () pred true 2017-11-17 05:00:00+00:00 0 0 2017-11-17 05:30:00+00:00 0 0 2017-11-17 06:00:00+00:00 0 0 2017-11-17 06:30:00+00:00 0 0 2017-11-17 07:00:00+00:00 0 0 Analysing the predictions plt . scatter ( df_pred . true , df_pred . pred , s = 0.1 ) plt . xlabel ( 'Observation' ) plt . ylabel ( 'Prediction' ) Text(0, 0.5, 'Prediction') We'll also visualise what the prediction looks like for randomly selected days #exports def plot_random_day ( df_pred , ax = None ): \"\"\" View predicted and observed PV profiles \"\"\" if ax is None : ax = plt . gca () random_day_idx = pd . to_datetime ( np . random . choice ( df_pred . index . date )) df_random_day = df_pred [ df_pred . index . date == random_day_idx ] df_random_day [ 'true' ] . plot ( ax = ax ) df_random_day [ 'pred' ] . plot ( ax = ax ) return ax plot_random_day ( df_pred ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x2c0e0225730> We'll then carry out the second-stage for our model, the solar peak flattening random_solar_profile = discharge . sample_random_day ( df_pred . pred ) . pipe ( charge . extract_solar_profile ) adj_random_solar_profile = discharge . flatten_peak ( random_solar_profile ) charge_profile = charge . construct_charge_profile ( random_solar_profile , adj_random_solar_profile ) plt . plot ( charge_profile ) [<matplotlib.lines.Line2D at 0x1227af520>]","title":"Fitting forecast model"},{"location":"07-pv-forecast/#predicting-charge-based-on-pv-forecast","text":"Now we will begin developing a unified approach for predicting PV and then optimising the battery charge schedule. We will also group by week. This should make the problem a bit harder, and help encourage the final model to generalise to lengthy unseen periods. #exports def generate_kfold_preds_weeks ( X , y , model , groups , kfold_kwargs , index = None ): \"\"\" Generate kfold preds, grouping by week \"\"\" group_kfold = GroupKFold ( ** kfold_kwargs ) df_pred = pd . DataFrame ( columns = [ 'pred' , 'true' ], index = np . arange ( X . shape [ 0 ])) for train_index , test_index in group_kfold . split ( X , y , groups ): X_train , X_test = X [ train_index ], X [ test_index ] y_train , y_test = y [ train_index ], y [ test_index ] model . fit ( X_train , y_train ) df_pred . loc [ test_index , 'true' ] = y_test df_pred . loc [ test_index , 'pred' ] = model . predict ( X_test ) df_pred . sort_index () if index is not None : assert len ( index ) == df_pred . shape [ 0 ], 'The passed index must be the same length as X and y' df_pred . index = index return df_pred def generate_kfold_charge_preds ( X , y , model , groups , kfold_kwargs = { 'n_splits' : 5 }): \"\"\" Fit the PV forecasting model and calculate the optimal charge profile for predictions. \"\"\" df_pred = generate_kfold_preds_weeks ( X . values , y . values , model , groups , kfold_kwargs = kfold_kwargs , index = X . index ) charge_pred = charge . construct_charge_s ( df_pred . pred ) charge_pred = charge . post_pred_charge_proc_func ( charge_pred ) return pd . DataFrame ({ 'charge_pred' : charge_pred , 'pv_actual' : df_pred . true , 'pv_pred' : df_pred . pred }) def predict_charge ( X , model ): \"\"\" Given a fitted PV forecast model and feature array X, get the optimal charge profile. \"\"\" pv_pred = pd . Series ( model . predict ( X ), index = X . index ) charge_pred = charge . construct_charge_s ( pv_pred ) charge_pred = charge . post_pred_charge_proc_func ( charge_pred ) return pd . Series ( charge_pred , index = X . index ) We'll also create a helper function for our test/train split based on time #exports def get_train_test_arr ( arr , start_of_test_period ): train_arr = arr [: pd . to_datetime ( start_of_test_period , utc = True )] test_arr = arr [ pd . to_datetime ( start_of_test_period , utc = True ):] return train_arr , test_arr def get_train_test_Xy ( X , y , start_of_test_period ): x_train , x_test = get_train_test_arr ( X , start_of_test_period ) y_train , y_test = get_train_test_arr ( y , start_of_test_period ) return x_train , x_test , y_train , y_test start_of_test_period = '2019-02-04' X_train , X_test , y_train , y_test = get_train_test_Xy ( X , y , start_of_test_period ) Now let's try executing this unified approach using k-fold CV, for 3 default models on the training data: models = { 'std_linear' : LinearRegression (), 'lasso' : Lasso (), 'ridge' : Ridge (), 'boosted' : GradientBoostingRegressor (), 'random_forest' : RandomForestRegressor (), } # Define the week groups week_groups = X_train . index . year + X_train . index . isocalendar () . week / 52 for key in models : charge_pred_df = generate_kfold_charge_preds ( X_train , y_train , models [ key ], week_groups ) score = charge . score_charging ( charge_pred_df . charge_pred , charge_pred_df . pv_actual ) pv_mse = np . mean ( np . square ( charge_pred_df . pv_actual - charge_pred_df . pv_pred )) solar_exploit_pct = 100 * charge . prop_max_solar ( charge_pred_df . charge_pred , charge_pred_df . pv_actual ) print ( \" {} : PV MSE: {:.2f} , score: {:.2f} , solar exploit: {:.2f} %\" . format ( key , pv_mse , score , solar_exploit_pct )) std_linear: PV MSE: 0.35, score: 0.77, solar exploit: 94.86% lasso: PV MSE: 0.39, score: 0.76, solar exploit: 93.87% ridge: PV MSE: 0.35, score: 0.77, solar exploit: 94.86% boosted: PV MSE: 0.36, score: 0.77, solar exploit: 94.48% random_forest: PV MSE: 0.35, score: 0.76, solar exploit: 94.35% Interestingly, there is little difference between the models in terms of solar exploit, even though there are some differences in the MSE of the PV forecasts. For our previous attempt at the charging task, the linear model was much worse than the boosted model and RF in terms of solar exploit. This suggests that a weak (or under-fitted) estimator of solar PV actually performs quite well when it comes to best_model = RandomForestRegressor () best_model . fit ( X_train , y_train ) preds = predict_charge ( X_test , best_model ) 100 * charge . prop_max_solar ( preds , y_test ) 95.64718850989254 pd . Series ( dict ( zip ( X_test . columns , best_model . feature_importances_ ))) . sort_values ( ascending = False ) solar_location2 0.656734 solar_location6 0.057420 solar_location5 0.048603 hour 0.036628 pv_7d_lag 0.031805 solar_location3 0.031244 doy 0.030441 temp_location4 0.019506 temp_location6 0.018710 temp_location5 0.016729 temp_location1 0.014523 temp_location3 0.013219 dow 0.012169 temp_location2 0.011155 weekend 0.001112 dtype: float64 Running the above analysis it seems like solar_locations 1 and 4 do not contribute much at all to the regression models: both are over an order of magnitude smaller than the others. Best to remove these when processing the data (for the moment this is charge.py features = [ c for c in X_train . columns if 'solar_location4' not in c and 'solar_location1' not in c ] X_train_reduced , X_test_reduced = X_train . filter ( features ), X_test . filter ( features ) charge_pred_df = generate_kfold_charge_preds ( X_train_reduced , y_train , LinearRegression (), week_groups ) score = charge . score_charging ( charge_pred_df . charge_pred , charge_pred_df . pv_actual ) pv_mse = np . mean ( np . square ( charge_pred_df . pv_actual - charge_pred_df . pv_pred )) solar_exploit_pct = 100 * charge . prop_max_solar ( charge_pred_df . charge_pred , charge_pred_df . pv_actual ) print ( \"PV MSE: {:.2f} , score: {:.2f} , solar exploit: {:.2f} %\" . format ( pv_mse , score , solar_exploit_pct ))","title":"Predicting charge based on PV forecast"},{"location":"07-pv-forecast/#tuned-rf-model","text":"rf_params = { 'criterion' : 'mae' , 'min_samples_leaf' : 46 , 'min_samples_split' : 2 , 'n_estimators' : 150 } best_model = RandomForestRegressor ( ** rf_params ) best_model . fit ( X_train , y_train ) preds = predict_charge ( X_test , best_model ) 100 * charge . prop_max_solar ( preds , y_test ) def construct_solar_exploit_calculator ( solar_profile , charging_datetimes = None , scorer = False ): if charging_datetimes is None : charging_datetimes = charge . extract_charging_datetimes ( solar_profile ) def calc_solar_exploitation ( y , y_pred ): # Checking evening datetimes if hasattr ( y_pred , 'index' ) == True : charging_datetimes = charge . extract_charging_datetimes ( y_pred ) assert y_pred . shape [ 0 ] == solar_profile . loc [ charging_datetimes ] . shape [ 0 ], f 'The prediction series must be the same length as the number of charging datetimes in the main dataframe, { y_pred . shape [ 0 ] } { s_demand . loc [ evening_datetimes ] . shape [ 0 ] } ' charge_pred = charge . construct_charge_s ( y_pred ) charge_pred = charge . post_pred_charge_proc_func ( charge_pred ) exploitation_pct = 100 * charge . prop_max_solar ( charge_pred , solar_profile . loc [ charging_datetimes ]) return exploitation_pct if scorer == True : return make_scorer ( calc_solar_exploitation ) else : return calc_solar_exploitation","title":"Tuned RF model"},{"location":"07-pv-forecast/#feature-selection","text":"It seems like overfitting could be a substantial issue for charging. Trying some feature selection: model = Lasso ( alpha = 0.5 ) model . fit ( X_train , y_train ) coefs_df = pd . DataFrame ({ 'feature' : X_train . columns , 'coefs' : model . coef_ }) features = coefs_df [ abs ( coefs_df . coefs ) > 0 ] . feature features 9 solar_location5 15 solar_location2_rolling 16 solar_location3_rolling 17 solar_location5_rolling 18 solar_location6_rolling 22 temp_location4_rolling Name: feature, dtype: object # features = [f for f in X_train.columns if 'temp' not in f] def prop_max_solar_df ( charge_pred_df ): return charge . prop_max_solar ( charge_pred_df . charge_pred , charge_pred_df . pv_actual ) X_train_reduced = X_train . filter ( features ) models = { 'std_linear' : LinearRegression (), 'lasso' : Lasso (), 'boosted' : GradientBoostingRegressor (), 'random_forest' : RandomForestRegressor (), } # Define the week groups week_groups = X_train . index . year + X_train . index . isocalendar () . week / 52 for key in models : charge_pred_df = generate_kfold_charge_preds ( X_train , y_train , models [ key ], week_groups ) score = charge . score_charging ( charge_pred_df . charge_pred , charge_pred_df . pv_actual ) pv_mse = np . mean ( np . square ( charge_pred_df . pv_actual - charge_pred_df . pv_pred )) solar_exploit_pct = 100 * charge_pred_df . groupby ( charge_pred_df . index . date ) . apply ( prop_max_solar_df ) print ( \" {} : PV MSE: {:.2f} , score: {:.2f} , solar exploit: {:.2f} %, std. solar exploit: {:.2f} \" . format ( key , pv_mse , score , solar_exploit_pct . mean (), solar_exploit_pct . std ())) std_linear: PV MSE: 0.34, score: 0.77, solar exploit: 94.98%, std. solar exploit: 7.88 lasso: PV MSE: 0.37, score: 0.76, solar exploit: 94.52%, std. solar exploit: 8.43 boosted: PV MSE: 0.35, score: 0.77, solar exploit: 94.85%, std. solar exploit: 8.26 random_forest: PV MSE: 0.35, score: 0.76, solar exploit: 94.74%, std. solar exploit: 8.35 model = LinearRegression () charge_pred_df = generate_kfold_charge_preds ( X_train , y_train , model , week_groups ) fig , axs = plt . subplots ( 1 , 5 , figsize = ( 15 , 3 ), dpi = 125 ) for i in range ( 5 ): random_day = pd . to_datetime ( np . random . choice ( charge_pred_df . index . date )) random_df = charge_pred_df [ charge_pred_df . index . date == random_day ] random_df [ 'pv_actual' ] . plot ( ax = axs [ i ]) random_df [ 'charge_pred' ] . plot ( ax = axs [ i ]) #exports def predict_charge ( X , model ): \"\"\" Given a fitted PV forecast model and feature array X, get the optimal charge profile. \"\"\" pv_pred = pd . Series ( model . predict ( X ), index = X . index ) charge_pred = charge . construct_charge_s ( pv_pred ) charge_pred = charge . post_pred_charge_proc_func ( charge_pred ) return pd . Series ( charge_pred , index = X . index ) model = LinearRegression () model . fit ( X_train , y_train ) pv_pred = model . predict ( X_test ) charge_pred = predict_charge ( X_test , model ) charge_pred_df = pd . DataFrame ({ 'charge_pred' : charge_pred , 'pv_pred' : pv_pred , 'pv_actual' : y_test }) solar_exploit_pct = 100 * charge_pred_df . groupby ( charge_pred_df . index . date ) . apply ( prop_max_solar_df ) print ( \"Held out solar exploit: {:.2f} %\" . format ( solar_exploit_pct . mean (), )) print ( \"Held out solar exploit (std): {:.2f} \" . format ( solar_exploit_pct . std (), )) Held out solar exploit: 93.98% Held out solar exploit (std): 8.19 N = 8 fig , axs = plt . subplots ( 1 , N , figsize = ( 15 , 3 ), dpi = 125 , sharey = True ) for i in range ( N ): random_day = pd . to_datetime ( np . random . choice ( charge_pred_df . index . date , replace = True )) random_df = charge_pred_df [ charge_pred_df . index . date == random_day ] axs [ i ] . plot ( random_df . pv_actual , c = 'b' , alpha = 1 ) axs [ i ] . plot ( random_df . pv_pred , c = 'b' , linestyle = '--' , alpha = 0.2 ) axs [ i ] . plot ( random_df . charge_pred , c = 'g' ) axs [ i ] . axhline ( 2.5 , c = 'r' , linestyle = ':' ) axs [ i ] . set_xticks ([]) ## X, y = prepare_training_input_data(intermediate_data_dir) start_of_test_period = '2019-02-04' X_train , X_test = get_train_test ( X , start_of_test_period ) y_train , y_test = get_train_test ( y , start_of_test_period ) charging_datetimes = charge . extract_charging_datetimes ( X_train ) solar_exploit_scorer = construct_solar_exploit_calculator ( solar_profile = y , charging_datetimes = charging_datetimes , scorer = True ) groups = charging_datetimes . date pipeline = Pipeline ([ ( 'pandas_RF' , utils . PandasRandomForestRegressor ()) ]) search_spaces = { 'pandas_RF__min_samples_leaf' : Integer ( 1 , 20 , 'uniform' ), 'pandas_RF__criterion' : Categorical ([ 'mse' , 'mae' ]), 'pandas_RF__n_estimators' : Integer ( 10 , 150 , 'uniform' ), 'pandas_RF__max_features' : Categorical ([ 'auto' , 'sqrt' ]), 'pandas_RF__max_depth' : Integer ( 5 , 200 , 'uniform' ), 'pandas_RF__min_samples_split' : Integer ( 2 , 10 , 'uniform' ), 'pandas_RF__min_samples_leaf' : Integer ( 1 , 4 , 'uniform' ), 'pandas_RF__bootstrap' : Categorical ([ True , False ]) } opt = utils . BayesSearchCV ( pipeline , search_spaces , n_iter = 10 , verbose = 1 , cv = 4 , scoring = solar_exploit_scorer , n_jobs =- 1 ) fit_BayesSearchCV = False if fit_BayesSearchCV == True : opt . fit ( X_train , y_train , groups = groups ) print ( f 'validation score: { opt . best_score_ } ' ) print ( f 'held out score: { opt . score ( X_test , y_test ) } ' ) print ( f 'best params: { opt . best_params_ } ' ) _ = plot_objective ( opt . optimizer_results_ [ 0 ]) plt . show () #exports def fit_and_save_pv_model ( X , y , pv_model_fp , model_class = LinearRegression , ** model_params ): model = model_class ( ** model_params ) model . fit ( X , y ) with open ( pv_model_fp , 'wb' ) as fp : joblib . dump ( model , fp ) return #exports #exports def prepare_test_feature_data ( raw_data_dir , intermediate_data_dir , test_start_date = None , test_end_date = None , start_time = '08:00' , end_time = '23:59' ): # Loading input data df = clean . combine_training_datasets ( intermediate_data_dir ) . interpolate ( limit = 1 ) df_features = construct_df_charge_features ( df ) # Loading default index (latest submission) if test_end_date is None or test_start_date is None : index = discharge . load_latest_submission_template ( raw_data_dir ) . index else : index = df_features [ test_start_date : test_end_date ] . index # Filtering feature data on submission datetimes df_features = df_features . loc [ index ] . between_time ( start_time , end_time ) return df_features def optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , pv_model_fp , test_start_date = None , test_end_date = None , start_time = '08:00' , end_time = '23:59' ): df_features = prepare_test_feature_data ( raw_data_dir , intermediate_data_dir , test_start_date = test_start_date , test_end_date = test_end_date , start_time = start_time , end_time = end_time ) charging_datetimes = charge . extract_charging_datetimes ( df_features ) X_test = df_features . loc [ charging_datetimes ] model = discharge . load_trained_model ( pv_model_fp ) charge_profile = predict_charge ( X_test , model ) s_charge_profile = pd . Series ( charge_profile , index = charging_datetimes ) s_charge_profile = s_charge_profile . reindex ( df_features . index ) . fillna ( 0 ) s_charge_profile = charge . post_pred_charge_proc_func ( s_charge_profile ) assert charge . charge_is_valid ( s_charge_profile ), \"Charging profile is invalid\" return s_charge_profile pv_model_fp = '../models/pv_model.sav' s_charge_profile = optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , pv_model_fp ) s_charge_profile . plot () --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-6-8db658a0274e> in <module> 1 pv_model_fp = '../models/pv_model.sav' ----> 2 s_charge_profile = optimise_test_charge_profile(intermediate_data_dir, pv_model_fp) 3 4 s_charge_profile.plot() NameError: name 'intermediate_data_dir' is not defined Finally we'll export the relevant code to our batopt module","title":"Feature Selection"},{"location":"08-christmas/","text":"Christmas \u00b6 #exports import numpy as np import pandas as pd import os from sklearn.ensemble import RandomForestRegressor from batopt import clean , discharge , charge , constraints , pv import FEAutils as hlp import matplotlib.pyplot as plt User Inputs \u00b6 raw_data_dir = '../data/raw' intermediate_data_dir = '../data/intermediate' Christmas Model EDA \u00b6 We'll start by loading in the combined training dataset df = clean . combine_training_datasets ( intermediate_data_dir ) . interpolate ( limit = 1 ) df . head () demand pv weather demand_MW irradiance_Wm-2 panel_temp_C pv_power_mw solar_location1 solar_location2 solar_location3 solar_location4 solar_location5 solar_location6 temp_location1 temp_location2 temp_location3 temp_location4 temp_location5 temp_location6 holidays 2015-01-01 00:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.75 9.65 8.83 7.58 11.62 11.22 nan 2015-01-01 00:30:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.83 9.705 8.865 7.6 11.635 11.27 nan 2015-01-01 01:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.91 9.76 8.9 7.62 11.65 11.32 nan 2015-01-01 01:30:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.95 9.78 9 7.615 11.65 11.31 nan 2015-01-01 02:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.99 9.8 9.1 7.61 11.65 11.3 nan We'll now create our charge/discharge baseline for 2018 test_start_date = '2018-12-18' test_end_date = '2018-12-24 23:59' discharge_opt_model_fp = '../models/discharge_opt.sav' pv_model_fp = '../models/pv_model.sav' model_params = { 'criterion' : 'mse' , 'bootstrap' : True , 'max_depth' : 32 , 'max_features' : 'auto' , 'min_samples_leaf' : 1 , 'min_samples_split' : 4 , 'n_estimators' : 74 } X , y = pv . prepare_training_input_data ( intermediate_data_dir ) if test_start_date is not None and test_end_date is not None : pred_index = X [ test_start_date : test_end_date ] . index X = X . drop ( pred_index ) y = y . drop ( pred_index ) pv . fit_and_save_pv_model ( X , y , pv_model_fp , model_class = RandomForestRegressor , ** model_params ) s_charge_profile = pv . optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , pv_model_fp , test_start_date = test_start_date , test_end_date = test_end_date ) s_discharge_profile = discharge . optimise_test_discharge_profile ( raw_data_dir , intermediate_data_dir , discharge_opt_model_fp , test_start_date = test_start_date , test_end_date = test_end_date ) s_battery_profile = ( s_charge_profile + s_discharge_profile ) . fillna ( 0 ) s_battery_profile . name = 'charge_MW' s_battery_profile . plot () <AxesSubplot:> As well as the current year we're meant to be forecasting test_start_date = None test_end_date = None discharge_opt_model_fp = '../models/discharge_opt.sav' pv_model_fp = '../models/pv_model.sav' model_params = { 'criterion' : 'mse' , 'bootstrap' : True , 'max_depth' : 32 , 'max_features' : 'auto' , 'min_samples_leaf' : 1 , 'min_samples_split' : 4 , 'n_estimators' : 74 } X , y = pv . prepare_training_input_data ( intermediate_data_dir ) if test_start_date is not None and test_end_date is not None : pred_index = X [ test_start_date : test_end_date ] . index X = X . drop ( pred_index ) y = y . drop ( pred_index ) pv . fit_and_save_pv_model ( X , y , pv_model_fp , model_class = RandomForestRegressor , ** model_params ) s_charge_profile = pv . optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , pv_model_fp , test_start_date = test_start_date , test_end_date = test_end_date ) s_discharge_profile = discharge . optimise_test_discharge_profile ( raw_data_dir , intermediate_data_dir , discharge_opt_model_fp , test_start_date = test_start_date , test_end_date = test_end_date ) s_battery_profile = ( s_charge_profile + s_discharge_profile ) . fillna ( 0 ) s_battery_profile . name = 'charge_MW' s_battery_profile . plot () --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-9-81823d82d537> in <module> 24 pv.fit_and_save_pv_model(X, y, pv_model_fp, model_class=RandomForestRegressor, **model_params) 25 ---> 26 s_charge_profile = pv.optimise_test_charge_profile(raw_data_dir, intermediate_data_dir, pv_model_fp, test_start_date=test_start_date, test_end_date=test_end_date) 27 s_discharge_profile = discharge.optimise_test_discharge_profile(raw_data_dir, intermediate_data_dir, discharge_opt_model_fp, test_start_date=test_start_date, test_end_date=test_end_date) 28 c:\\users\\ayrto\\desktop\\hackathons\\wpd-ds-challenge\\batopt\\pv.py in optimise_test_charge_profile(raw_data_dir, intermediate_data_dir, pv_model_fp, test_start_date, test_end_date, start_time, end_time) 142 # Cell 143 def optimise_test_charge_profile(raw_data_dir, intermediate_data_dir, pv_model_fp, test_start_date=None, test_end_date=None, start_time='08:00', end_time='23:59'): --> 144 df_features = charge.prepare_test_feature_data(raw_data_dir, intermediate_data_dir, test_start_date=test_start_date, test_end_date=test_end_date, start_time=start_time, end_time=end_time) 145 charging_datetimes = charge.extract_charging_datetimes(df_features) 146 X_test = df_features.loc[charging_datetimes] c:\\users\\ayrto\\desktop\\hackathons\\wpd-ds-challenge\\batopt\\charge.py in prepare_test_feature_data(raw_data_dir, intermediate_data_dir, test_start_date, test_end_date, start_time, end_time) 280 281 # Filtering feature data on submission datetimes --> 282 df_features = df_features.loc[index].between_time(start_time, end_time) 283 284 return df_features ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in __getitem__(self, key) 892 893 maybe_callable = com.apply_if_callable(key, self.obj) --> 894 return self._getitem_axis(maybe_callable, axis=axis) 895 896 def _is_scalar_access(self, key: Tuple): ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in _getitem_axis(self, key, axis) 1110 raise ValueError(\"Cannot index with multidimensional key\") 1111 -> 1112 return self._getitem_iterable(key, axis=axis) 1113 1114 # nested tuple slicing ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in _getitem_iterable(self, key, axis) 1050 1051 # A collection of keys -> 1052 keyarr, indexer = self._get_listlike_indexer(key, axis, raise_missing=False) 1053 return self.obj._reindex_with_indexers( 1054 {axis: [keyarr, indexer]}, copy=True, allow_dups=True ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in _get_listlike_indexer(self, key, axis, raise_missing) 1263 keyarr, indexer, new_indexer = ax._reindex_non_unique(keyarr) 1264 -> 1265 self._validate_read_indexer(keyarr, indexer, axis, raise_missing=raise_missing) 1266 return keyarr, indexer 1267 ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in _validate_read_indexer(self, key, indexer, axis, raise_missing) 1305 if missing == len(indexer): 1306 axis_name = self.obj._get_axis_name(axis) -> 1307 raise KeyError(f\"None of [{key}] are in the [{axis_name}]\") 1308 1309 ax = self.obj._get_axis(axis) KeyError: \"None of [DatetimeIndex(['2020-07-03 00:00:00+00:00', '2020-07-03 00:30:00+00:00',\\n '2020-07-03 01:00:00+00:00', '2020-07-03 01:30:00+00:00',\\n '2020-07-03 02:00:00+00:00', '2020-07-03 02:30:00+00:00',\\n '2020-07-03 03:00:00+00:00', '2020-07-03 03:30:00+00:00',\\n '2020-07-03 04:00:00+00:00', '2020-07-03 04:30:00+00:00',\\n ...\\n '2020-07-09 19:00:00+00:00', '2020-07-09 19:30:00+00:00',\\n '2020-07-09 20:00:00+00:00', '2020-07-09 20:30:00+00:00',\\n '2020-07-09 21:00:00+00:00', '2020-07-09 21:30:00+00:00',\\n '2020-07-09 22:00:00+00:00', '2020-07-09 22:30:00+00:00',\\n '2020-07-09 23:00:00+00:00', '2020-07-09 23:30:00+00:00'],\\n dtype='datetime64[ns, UTC]', name='datetime', length=336, freq=None)] are in the [index]\" fig , ax = plt . subplots ( dpi = 150 ) for year in [ 2017 , 2018 ]: start_date = f ' { year } -12-18' end_date = f ' { year } -12-24 23:59' s_discharge = discharge . construct_discharge_s ( df . loc [ start_date : end_date , 'demand_MW' ]) plt . plot ( s_discharge . iloc [: 48 * 7 ] . values , label = f ' { year } ' ) plt . plot ( s_discharge_profile . iloc [: 48 * 7 ] . values , linestyle = '--' , label = '2019 Prediction' ) plt . legend ( frameon = False , bbox_to_anchor = ( 1 , 1 )) hlp . hide_spines ( ax ) for year in [ 2017 , 2018 ]: start_date = f ' { year } -12-18' end_date = f ' { year } -12-24 23:59' s_discharge = discharge . construct_discharge_s ( df . loc [ start_date : end_date , 'demand_MW' ]) plt . plot ( s_discharge . iloc [: 48 * 7 ] . values )","title":"Christmas"},{"location":"08-christmas/#christmas","text":"#exports import numpy as np import pandas as pd import os from sklearn.ensemble import RandomForestRegressor from batopt import clean , discharge , charge , constraints , pv import FEAutils as hlp import matplotlib.pyplot as plt","title":"Christmas"},{"location":"08-christmas/#user-inputs","text":"raw_data_dir = '../data/raw' intermediate_data_dir = '../data/intermediate'","title":"User Inputs"},{"location":"08-christmas/#christmas-model-eda","text":"We'll start by loading in the combined training dataset df = clean . combine_training_datasets ( intermediate_data_dir ) . interpolate ( limit = 1 ) df . head () demand pv weather demand_MW irradiance_Wm-2 panel_temp_C pv_power_mw solar_location1 solar_location2 solar_location3 solar_location4 solar_location5 solar_location6 temp_location1 temp_location2 temp_location3 temp_location4 temp_location5 temp_location6 holidays 2015-01-01 00:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.75 9.65 8.83 7.58 11.62 11.22 nan 2015-01-01 00:30:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.83 9.705 8.865 7.6 11.635 11.27 nan 2015-01-01 01:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.91 9.76 8.9 7.62 11.65 11.32 nan 2015-01-01 01:30:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.95 9.78 9 7.615 11.65 11.31 nan 2015-01-01 02:00:00+00:00 nan nan nan nan nan nan nan 0 0 0 0 0 0 9.99 9.8 9.1 7.61 11.65 11.3 nan We'll now create our charge/discharge baseline for 2018 test_start_date = '2018-12-18' test_end_date = '2018-12-24 23:59' discharge_opt_model_fp = '../models/discharge_opt.sav' pv_model_fp = '../models/pv_model.sav' model_params = { 'criterion' : 'mse' , 'bootstrap' : True , 'max_depth' : 32 , 'max_features' : 'auto' , 'min_samples_leaf' : 1 , 'min_samples_split' : 4 , 'n_estimators' : 74 } X , y = pv . prepare_training_input_data ( intermediate_data_dir ) if test_start_date is not None and test_end_date is not None : pred_index = X [ test_start_date : test_end_date ] . index X = X . drop ( pred_index ) y = y . drop ( pred_index ) pv . fit_and_save_pv_model ( X , y , pv_model_fp , model_class = RandomForestRegressor , ** model_params ) s_charge_profile = pv . optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , pv_model_fp , test_start_date = test_start_date , test_end_date = test_end_date ) s_discharge_profile = discharge . optimise_test_discharge_profile ( raw_data_dir , intermediate_data_dir , discharge_opt_model_fp , test_start_date = test_start_date , test_end_date = test_end_date ) s_battery_profile = ( s_charge_profile + s_discharge_profile ) . fillna ( 0 ) s_battery_profile . name = 'charge_MW' s_battery_profile . plot () <AxesSubplot:> As well as the current year we're meant to be forecasting test_start_date = None test_end_date = None discharge_opt_model_fp = '../models/discharge_opt.sav' pv_model_fp = '../models/pv_model.sav' model_params = { 'criterion' : 'mse' , 'bootstrap' : True , 'max_depth' : 32 , 'max_features' : 'auto' , 'min_samples_leaf' : 1 , 'min_samples_split' : 4 , 'n_estimators' : 74 } X , y = pv . prepare_training_input_data ( intermediate_data_dir ) if test_start_date is not None and test_end_date is not None : pred_index = X [ test_start_date : test_end_date ] . index X = X . drop ( pred_index ) y = y . drop ( pred_index ) pv . fit_and_save_pv_model ( X , y , pv_model_fp , model_class = RandomForestRegressor , ** model_params ) s_charge_profile = pv . optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , pv_model_fp , test_start_date = test_start_date , test_end_date = test_end_date ) s_discharge_profile = discharge . optimise_test_discharge_profile ( raw_data_dir , intermediate_data_dir , discharge_opt_model_fp , test_start_date = test_start_date , test_end_date = test_end_date ) s_battery_profile = ( s_charge_profile + s_discharge_profile ) . fillna ( 0 ) s_battery_profile . name = 'charge_MW' s_battery_profile . plot () --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-9-81823d82d537> in <module> 24 pv.fit_and_save_pv_model(X, y, pv_model_fp, model_class=RandomForestRegressor, **model_params) 25 ---> 26 s_charge_profile = pv.optimise_test_charge_profile(raw_data_dir, intermediate_data_dir, pv_model_fp, test_start_date=test_start_date, test_end_date=test_end_date) 27 s_discharge_profile = discharge.optimise_test_discharge_profile(raw_data_dir, intermediate_data_dir, discharge_opt_model_fp, test_start_date=test_start_date, test_end_date=test_end_date) 28 c:\\users\\ayrto\\desktop\\hackathons\\wpd-ds-challenge\\batopt\\pv.py in optimise_test_charge_profile(raw_data_dir, intermediate_data_dir, pv_model_fp, test_start_date, test_end_date, start_time, end_time) 142 # Cell 143 def optimise_test_charge_profile(raw_data_dir, intermediate_data_dir, pv_model_fp, test_start_date=None, test_end_date=None, start_time='08:00', end_time='23:59'): --> 144 df_features = charge.prepare_test_feature_data(raw_data_dir, intermediate_data_dir, test_start_date=test_start_date, test_end_date=test_end_date, start_time=start_time, end_time=end_time) 145 charging_datetimes = charge.extract_charging_datetimes(df_features) 146 X_test = df_features.loc[charging_datetimes] c:\\users\\ayrto\\desktop\\hackathons\\wpd-ds-challenge\\batopt\\charge.py in prepare_test_feature_data(raw_data_dir, intermediate_data_dir, test_start_date, test_end_date, start_time, end_time) 280 281 # Filtering feature data on submission datetimes --> 282 df_features = df_features.loc[index].between_time(start_time, end_time) 283 284 return df_features ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in __getitem__(self, key) 892 893 maybe_callable = com.apply_if_callable(key, self.obj) --> 894 return self._getitem_axis(maybe_callable, axis=axis) 895 896 def _is_scalar_access(self, key: Tuple): ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in _getitem_axis(self, key, axis) 1110 raise ValueError(\"Cannot index with multidimensional key\") 1111 -> 1112 return self._getitem_iterable(key, axis=axis) 1113 1114 # nested tuple slicing ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in _getitem_iterable(self, key, axis) 1050 1051 # A collection of keys -> 1052 keyarr, indexer = self._get_listlike_indexer(key, axis, raise_missing=False) 1053 return self.obj._reindex_with_indexers( 1054 {axis: [keyarr, indexer]}, copy=True, allow_dups=True ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in _get_listlike_indexer(self, key, axis, raise_missing) 1263 keyarr, indexer, new_indexer = ax._reindex_non_unique(keyarr) 1264 -> 1265 self._validate_read_indexer(keyarr, indexer, axis, raise_missing=raise_missing) 1266 return keyarr, indexer 1267 ~\\anaconda3\\envs\\batopt\\lib\\site-packages\\pandas\\core\\indexing.py in _validate_read_indexer(self, key, indexer, axis, raise_missing) 1305 if missing == len(indexer): 1306 axis_name = self.obj._get_axis_name(axis) -> 1307 raise KeyError(f\"None of [{key}] are in the [{axis_name}]\") 1308 1309 ax = self.obj._get_axis(axis) KeyError: \"None of [DatetimeIndex(['2020-07-03 00:00:00+00:00', '2020-07-03 00:30:00+00:00',\\n '2020-07-03 01:00:00+00:00', '2020-07-03 01:30:00+00:00',\\n '2020-07-03 02:00:00+00:00', '2020-07-03 02:30:00+00:00',\\n '2020-07-03 03:00:00+00:00', '2020-07-03 03:30:00+00:00',\\n '2020-07-03 04:00:00+00:00', '2020-07-03 04:30:00+00:00',\\n ...\\n '2020-07-09 19:00:00+00:00', '2020-07-09 19:30:00+00:00',\\n '2020-07-09 20:00:00+00:00', '2020-07-09 20:30:00+00:00',\\n '2020-07-09 21:00:00+00:00', '2020-07-09 21:30:00+00:00',\\n '2020-07-09 22:00:00+00:00', '2020-07-09 22:30:00+00:00',\\n '2020-07-09 23:00:00+00:00', '2020-07-09 23:30:00+00:00'],\\n dtype='datetime64[ns, UTC]', name='datetime', length=336, freq=None)] are in the [index]\" fig , ax = plt . subplots ( dpi = 150 ) for year in [ 2017 , 2018 ]: start_date = f ' { year } -12-18' end_date = f ' { year } -12-24 23:59' s_discharge = discharge . construct_discharge_s ( df . loc [ start_date : end_date , 'demand_MW' ]) plt . plot ( s_discharge . iloc [: 48 * 7 ] . values , label = f ' { year } ' ) plt . plot ( s_discharge_profile . iloc [: 48 * 7 ] . values , linestyle = '--' , label = '2019 Prediction' ) plt . legend ( frameon = False , bbox_to_anchor = ( 1 , 1 )) hlp . hide_spines ( ax ) for year in [ 2017 , 2018 ]: start_date = f ' { year } -12-18' end_date = f ' { year } -12-24 23:59' s_discharge = discharge . construct_discharge_s ( df . loc [ start_date : end_date , 'demand_MW' ]) plt . plot ( s_discharge . iloc [: 48 * 7 ] . values )","title":"Christmas Model EDA"},{"location":"09-pipeline/","text":"Pipeline \u00b6 #exports import numpy as np import pandas as pd import os from sklearn.ensemble import RandomForestRegressor from dagster import execute_pipeline , pipeline , solid , Field from batopt import clean , discharge , charge , constraints , pv import FEAutils as hlp import matplotlib.pyplot as plt End-to-End \u00b6 We're now going to combine these steps into a pipeline using dagster, first we'll create the individual components. @solid () def load_data ( _ , raw_data_dir : str ): loaded_data = dict () loaded_data [ 'pv' ] = clean . load_training_dataset ( raw_data_dir , 'pv' ) loaded_data [ 'demand' ] = clean . load_training_dataset ( raw_data_dir , 'demand' ) loaded_data [ 'weather' ] = clean . load_training_dataset ( raw_data_dir , 'weather' , dt_idx_freq = 'H' ) return loaded_data @solid () def clean_data ( _ , loaded_data , raw_data_dir : str , intermediate_data_dir : str ): # Cleaning cleaned_data = dict () cleaned_data [ 'pv' ] = ( loaded_data [ 'pv' ] . pipe ( clean . pv_anomalies_to_nan ) . pipe ( clean . interpolate_missing_panel_temps , loaded_data [ 'weather' ]) . pipe ( clean . interpolate_missing_site_irradiance , loaded_data [ 'weather' ]) . pipe ( clean . interpolate_missing_site_power ) ) cleaned_data [ 'weather' ] = clean . interpolate_missing_weather_solar ( loaded_data [ 'pv' ], loaded_data [ 'weather' ]) cleaned_data [ 'weather' ] = clean . interpolate_missing_temps ( cleaned_data [ 'weather' ], 'temp_location4' ) cleaned_data [ 'demand' ] = loaded_data [ 'demand' ] # Saving if os . path . exists ( intermediate_data_dir ) == False : os . mkdir ( intermediate_data_dir ) set_num = clean . identify_latest_set_num ( raw_data_dir ) cleaned_data [ 'pv' ] . to_csv ( f ' { intermediate_data_dir } /pv_set { set_num } .csv' ) cleaned_data [ 'demand' ] . to_csv ( f ' { intermediate_data_dir } /demand_set { set_num } .csv' ) cleaned_data [ 'weather' ] . to_csv ( f ' { intermediate_data_dir } /weather_set { set_num } .csv' ) return intermediate_data_dir @solid () def fit_and_save_pv_model ( _ , intermediate_data_dir : str , pv_model_fp : str , model_params : dict ): X , y = pv . prepare_training_input_data ( intermediate_data_dir ) pv . fit_and_save_pv_model ( X , y , pv_model_fp , model_class = RandomForestRegressor , ** model_params ) return True @solid () def fit_and_save_discharge_model ( _ , intermediate_data_dir : str , discharge_opt_model_fp : str , model_params : dict ): X , y = discharge . prepare_training_input_data ( intermediate_data_dir ) discharge . fit_and_save_model ( X , y , discharge_opt_model_fp , ** model_params ) return True @solid () def construct_battery_profile ( _ , charge_model_success : bool , discharge_model_success : bool , intermediate_data_dir : str , raw_data_dir : str , discharge_opt_model_fp : str , pv_model_fp : str , start_time : str ): assert charge_model_success and discharge_model_success , 'Model training was unsuccessful' s_discharge_profile = discharge . optimise_test_discharge_profile ( raw_data_dir , intermediate_data_dir , discharge_opt_model_fp ) s_charge_profile = pv . optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , pv_model_fp , start_time = start_time ) s_battery_profile = ( s_charge_profile + s_discharge_profile ) . fillna ( 0 ) s_battery_profile . name = 'charge_MW' return s_battery_profile @solid () def check_and_save_battery_profile ( _ , s_battery_profile , output_data_dir : str ): # Check that solution meets battery constraints assert constraints . schedule_is_legal ( s_battery_profile ), 'Solution violates constraints' # Saving if os . path . exists ( output_data_dir ) == False : os . mkdir ( output_data_dir ) s_battery_profile . index = s_battery_profile . index . tz_convert ( 'UTC' ) . tz_convert ( None ) s_battery_profile . to_csv ( f ' { output_data_dir } /latest_submission.csv' ) return Then we'll combine them in a pipeline @pipeline def end_to_end_pipeline (): # loading and cleaning loaded_data = load_data () intermediate_data_dir = clean_data ( loaded_data ) # charging charge_model_success = fit_and_save_pv_model ( intermediate_data_dir ) # discharing discharge_model_success = fit_and_save_discharge_model ( intermediate_data_dir ) # combining and saving s_battery_profile = construct_battery_profile ( charge_model_success , discharge_model_success , intermediate_data_dir ) check_and_save_battery_profile ( s_battery_profile ) Which we'll now run a test run_config = { 'solids' : { 'load_data' : { 'inputs' : { 'raw_data_dir' : '../data/raw' , }, }, 'clean_data' : { 'inputs' : { 'raw_data_dir' : '../data/raw' , 'intermediate_data_dir' : '../data/intermediate' , }, }, 'fit_and_save_discharge_model' : { 'inputs' : { 'discharge_opt_model_fp' : '../models/discharge_opt.sav' , 'model_params' : { 'criterion' : 'mse' , 'bootstrap' : True , 'max_depth' : 32 , 'max_features' : 'auto' , 'min_samples_leaf' : 1 , 'min_samples_split' : 4 , 'n_estimators' : 74 } }, }, 'fit_and_save_pv_model' : { 'inputs' : { 'pv_model_fp' : '../models/pv_model.sav' , 'model_params' : { 'bootstrap' : True , 'criterion' : 'mse' , 'max_depth' : 5 , 'max_features' : 'sqrt' , 'min_samples_leaf' : 1 , 'min_samples_split' : 2 , 'n_estimators' : 150 } }, }, 'construct_battery_profile' : { 'inputs' : { 'raw_data_dir' : '../data/raw' , 'discharge_opt_model_fp' : '../models/discharge_opt.sav' , 'pv_model_fp' : '../models/pv_model.sav' , 'start_time' : '08:00' , }, }, 'check_and_save_battery_profile' : { 'inputs' : { 'output_data_dir' : '../data/output' }, }, } } execute_pipeline ( end_to_end_pipeline , run_config = run_config ) \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - ENGINE_EVENT - Starting initialization of resources [asset_store]. \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - ENGINE_EVENT - Finished initialization of resources [asset_store]. \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - PIPELINE_START - Started execution of pipeline \"end_to_end_pipeline\". \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - ENGINE_EVENT - Executing steps in process (pid: 17436) \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - load_data.compute - STEP_START - Started execution of step \"load_data.compute\". \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - load_data.compute - STEP_INPUT - Got input \"raw_data_dir\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - load_data.compute - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed). \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - load_data.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle. \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - load_data.compute - STEP_SUCCESS - Finished execution of step \"load_data.compute\" in 253ms. \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - clean_data.compute - STEP_START - Started execution of step \"clean_data.compute\". \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - clean_data.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input loaded_data in memory object store using pickle. \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - clean_data.compute - STEP_INPUT - Got input \"loaded_data\" of type \"Any\". (Type check passed). \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - clean_data.compute - STEP_INPUT - Got input \"raw_data_dir\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - clean_data.compute - STEP_INPUT - Got input \"intermediate_data_dir\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - clean_data.compute - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - clean_data.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - clean_data.compute - STEP_SUCCESS - Finished execution of step \"clean_data.compute\" in 2m1s. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_discharge_model.compute - STEP_START - Started execution of step \"fit_and_save_discharge_model.compute\". \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_discharge_model.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input intermediate_data_dir in memory object store using pickle. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_discharge_model.compute - STEP_INPUT - Got input \"intermediate_data_dir\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_discharge_model.compute - STEP_INPUT - Got input \"discharge_opt_model_fp\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_discharge_model.compute - STEP_INPUT - Got input \"model_params\" of type \"dict\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_discharge_model.compute - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_discharge_model.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_discharge_model.compute - STEP_SUCCESS - Finished execution of step \"fit_and_save_discharge_model.compute\" in 5.44ms. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_pv_model.compute - STEP_START - Started execution of step \"fit_and_save_pv_model.compute\". \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_pv_model.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input intermediate_data_dir in memory object store using pickle. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_pv_model.compute - STEP_INPUT - Got input \"intermediate_data_dir\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_pv_model.compute - STEP_INPUT - Got input \"pv_model_fp\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_pv_model.compute - STEP_INPUT - Got input \"model_params\" of type \"dict\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_pv_model.compute - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_pv_model.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_pv_model.compute - STEP_SUCCESS - Finished execution of step \"fit_and_save_pv_model.compute\" in 6.29ms. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_START - Started execution of step \"construct_battery_profile.compute\". \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input charge_model_success in memory object store using pickle. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input discharge_model_success in memory object store using pickle. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input intermediate_data_dir in memory object store using pickle. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_INPUT - Got input \"charge_model_success\" of type \"Bool\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_INPUT - Got input \"discharge_model_success\" of type \"Bool\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_INPUT - Got input \"intermediate_data_dir\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_INPUT - Got input \"raw_data_dir\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_INPUT - Got input \"discharge_opt_model_fp\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_INPUT - Got input \"pv_model_fp\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_INPUT - Got input \"start_time\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed). \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle. \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_SUCCESS - Finished execution of step \"construct_battery_profile.compute\" in 2.58s. \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - check_and_save_battery_profile.compute - STEP_START - Started execution of step \"check_and_save_battery_profile.compute\". \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - check_and_save_battery_profile.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input s_battery_profile in memory object store using pickle. \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - check_and_save_battery_profile.compute - STEP_INPUT - Got input \"s_battery_profile\" of type \"Any\". (Type check passed). \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - check_and_save_battery_profile.compute - STEP_INPUT - Got input \"output_data_dir\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - check_and_save_battery_profile.compute - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed). \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - check_and_save_battery_profile.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle. \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - check_and_save_battery_profile.compute - STEP_SUCCESS - Finished execution of step \"check_and_save_battery_profile.compute\" in 15ms. \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - ENGINE_EVENT - Finished steps in process (pid: 17436) in 2m4s \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - PIPELINE_SUCCESS - Finished execution of pipeline \"end_to_end_pipeline\". <dagster.core.execution.results.PipelineExecutionResult at 0x1d1fa542ac0> We'll then visualise the latest charging profile df_latest_submission = pd . read_csv ( '../data/output/latest_submission.csv' ) s_latest_submission = df_latest_submission . set_index ( 'datetime' )[ 'charge_MW' ] s_latest_submission . index = pd . to_datetime ( s_latest_submission . index ) # Plotting fig , ax = plt . subplots ( dpi = 250 ) s_latest_submission . plot ( ax = ax ) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Charge (MW)' ) hlp . hide_spines ( ax ) fig . tight_layout () fig . savefig ( '../img/latest_submission.png' , dpi = 250 ) Finally we'll export the relevant code to our batopt module","title":"Pipeline"},{"location":"09-pipeline/#pipeline","text":"#exports import numpy as np import pandas as pd import os from sklearn.ensemble import RandomForestRegressor from dagster import execute_pipeline , pipeline , solid , Field from batopt import clean , discharge , charge , constraints , pv import FEAutils as hlp import matplotlib.pyplot as plt","title":"Pipeline"},{"location":"09-pipeline/#end-to-end","text":"We're now going to combine these steps into a pipeline using dagster, first we'll create the individual components. @solid () def load_data ( _ , raw_data_dir : str ): loaded_data = dict () loaded_data [ 'pv' ] = clean . load_training_dataset ( raw_data_dir , 'pv' ) loaded_data [ 'demand' ] = clean . load_training_dataset ( raw_data_dir , 'demand' ) loaded_data [ 'weather' ] = clean . load_training_dataset ( raw_data_dir , 'weather' , dt_idx_freq = 'H' ) return loaded_data @solid () def clean_data ( _ , loaded_data , raw_data_dir : str , intermediate_data_dir : str ): # Cleaning cleaned_data = dict () cleaned_data [ 'pv' ] = ( loaded_data [ 'pv' ] . pipe ( clean . pv_anomalies_to_nan ) . pipe ( clean . interpolate_missing_panel_temps , loaded_data [ 'weather' ]) . pipe ( clean . interpolate_missing_site_irradiance , loaded_data [ 'weather' ]) . pipe ( clean . interpolate_missing_site_power ) ) cleaned_data [ 'weather' ] = clean . interpolate_missing_weather_solar ( loaded_data [ 'pv' ], loaded_data [ 'weather' ]) cleaned_data [ 'weather' ] = clean . interpolate_missing_temps ( cleaned_data [ 'weather' ], 'temp_location4' ) cleaned_data [ 'demand' ] = loaded_data [ 'demand' ] # Saving if os . path . exists ( intermediate_data_dir ) == False : os . mkdir ( intermediate_data_dir ) set_num = clean . identify_latest_set_num ( raw_data_dir ) cleaned_data [ 'pv' ] . to_csv ( f ' { intermediate_data_dir } /pv_set { set_num } .csv' ) cleaned_data [ 'demand' ] . to_csv ( f ' { intermediate_data_dir } /demand_set { set_num } .csv' ) cleaned_data [ 'weather' ] . to_csv ( f ' { intermediate_data_dir } /weather_set { set_num } .csv' ) return intermediate_data_dir @solid () def fit_and_save_pv_model ( _ , intermediate_data_dir : str , pv_model_fp : str , model_params : dict ): X , y = pv . prepare_training_input_data ( intermediate_data_dir ) pv . fit_and_save_pv_model ( X , y , pv_model_fp , model_class = RandomForestRegressor , ** model_params ) return True @solid () def fit_and_save_discharge_model ( _ , intermediate_data_dir : str , discharge_opt_model_fp : str , model_params : dict ): X , y = discharge . prepare_training_input_data ( intermediate_data_dir ) discharge . fit_and_save_model ( X , y , discharge_opt_model_fp , ** model_params ) return True @solid () def construct_battery_profile ( _ , charge_model_success : bool , discharge_model_success : bool , intermediate_data_dir : str , raw_data_dir : str , discharge_opt_model_fp : str , pv_model_fp : str , start_time : str ): assert charge_model_success and discharge_model_success , 'Model training was unsuccessful' s_discharge_profile = discharge . optimise_test_discharge_profile ( raw_data_dir , intermediate_data_dir , discharge_opt_model_fp ) s_charge_profile = pv . optimise_test_charge_profile ( raw_data_dir , intermediate_data_dir , pv_model_fp , start_time = start_time ) s_battery_profile = ( s_charge_profile + s_discharge_profile ) . fillna ( 0 ) s_battery_profile . name = 'charge_MW' return s_battery_profile @solid () def check_and_save_battery_profile ( _ , s_battery_profile , output_data_dir : str ): # Check that solution meets battery constraints assert constraints . schedule_is_legal ( s_battery_profile ), 'Solution violates constraints' # Saving if os . path . exists ( output_data_dir ) == False : os . mkdir ( output_data_dir ) s_battery_profile . index = s_battery_profile . index . tz_convert ( 'UTC' ) . tz_convert ( None ) s_battery_profile . to_csv ( f ' { output_data_dir } /latest_submission.csv' ) return Then we'll combine them in a pipeline @pipeline def end_to_end_pipeline (): # loading and cleaning loaded_data = load_data () intermediate_data_dir = clean_data ( loaded_data ) # charging charge_model_success = fit_and_save_pv_model ( intermediate_data_dir ) # discharing discharge_model_success = fit_and_save_discharge_model ( intermediate_data_dir ) # combining and saving s_battery_profile = construct_battery_profile ( charge_model_success , discharge_model_success , intermediate_data_dir ) check_and_save_battery_profile ( s_battery_profile ) Which we'll now run a test run_config = { 'solids' : { 'load_data' : { 'inputs' : { 'raw_data_dir' : '../data/raw' , }, }, 'clean_data' : { 'inputs' : { 'raw_data_dir' : '../data/raw' , 'intermediate_data_dir' : '../data/intermediate' , }, }, 'fit_and_save_discharge_model' : { 'inputs' : { 'discharge_opt_model_fp' : '../models/discharge_opt.sav' , 'model_params' : { 'criterion' : 'mse' , 'bootstrap' : True , 'max_depth' : 32 , 'max_features' : 'auto' , 'min_samples_leaf' : 1 , 'min_samples_split' : 4 , 'n_estimators' : 74 } }, }, 'fit_and_save_pv_model' : { 'inputs' : { 'pv_model_fp' : '../models/pv_model.sav' , 'model_params' : { 'bootstrap' : True , 'criterion' : 'mse' , 'max_depth' : 5 , 'max_features' : 'sqrt' , 'min_samples_leaf' : 1 , 'min_samples_split' : 2 , 'n_estimators' : 150 } }, }, 'construct_battery_profile' : { 'inputs' : { 'raw_data_dir' : '../data/raw' , 'discharge_opt_model_fp' : '../models/discharge_opt.sav' , 'pv_model_fp' : '../models/pv_model.sav' , 'start_time' : '08:00' , }, }, 'check_and_save_battery_profile' : { 'inputs' : { 'output_data_dir' : '../data/output' }, }, } } execute_pipeline ( end_to_end_pipeline , run_config = run_config ) \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - ENGINE_EVENT - Starting initialization of resources [asset_store]. \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - ENGINE_EVENT - Finished initialization of resources [asset_store]. \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - PIPELINE_START - Started execution of pipeline \"end_to_end_pipeline\". \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - ENGINE_EVENT - Executing steps in process (pid: 17436) \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - load_data.compute - STEP_START - Started execution of step \"load_data.compute\". \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - load_data.compute - STEP_INPUT - Got input \"raw_data_dir\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - load_data.compute - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed). \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - load_data.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle. \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - load_data.compute - STEP_SUCCESS - Finished execution of step \"load_data.compute\" in 253ms. \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - clean_data.compute - STEP_START - Started execution of step \"clean_data.compute\". \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - clean_data.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input loaded_data in memory object store using pickle. \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - clean_data.compute - STEP_INPUT - Got input \"loaded_data\" of type \"Any\". (Type check passed). \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - clean_data.compute - STEP_INPUT - Got input \"raw_data_dir\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:22:42\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - clean_data.compute - STEP_INPUT - Got input \"intermediate_data_dir\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - clean_data.compute - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - clean_data.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - clean_data.compute - STEP_SUCCESS - Finished execution of step \"clean_data.compute\" in 2m1s. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_discharge_model.compute - STEP_START - Started execution of step \"fit_and_save_discharge_model.compute\". \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_discharge_model.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input intermediate_data_dir in memory object store using pickle. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_discharge_model.compute - STEP_INPUT - Got input \"intermediate_data_dir\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_discharge_model.compute - STEP_INPUT - Got input \"discharge_opt_model_fp\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_discharge_model.compute - STEP_INPUT - Got input \"model_params\" of type \"dict\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_discharge_model.compute - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_discharge_model.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_discharge_model.compute - STEP_SUCCESS - Finished execution of step \"fit_and_save_discharge_model.compute\" in 5.44ms. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_pv_model.compute - STEP_START - Started execution of step \"fit_and_save_pv_model.compute\". \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_pv_model.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input intermediate_data_dir in memory object store using pickle. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_pv_model.compute - STEP_INPUT - Got input \"intermediate_data_dir\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_pv_model.compute - STEP_INPUT - Got input \"pv_model_fp\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_pv_model.compute - STEP_INPUT - Got input \"model_params\" of type \"dict\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_pv_model.compute - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_pv_model.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - fit_and_save_pv_model.compute - STEP_SUCCESS - Finished execution of step \"fit_and_save_pv_model.compute\" in 6.29ms. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_START - Started execution of step \"construct_battery_profile.compute\". \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input charge_model_success in memory object store using pickle. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input discharge_model_success in memory object store using pickle. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input intermediate_data_dir in memory object store using pickle. \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_INPUT - Got input \"charge_model_success\" of type \"Bool\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_INPUT - Got input \"discharge_model_success\" of type \"Bool\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_INPUT - Got input \"intermediate_data_dir\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_INPUT - Got input \"raw_data_dir\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_INPUT - Got input \"discharge_opt_model_fp\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_INPUT - Got input \"pv_model_fp\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:43\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_INPUT - Got input \"start_time\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed). \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle. \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - construct_battery_profile.compute - STEP_SUCCESS - Finished execution of step \"construct_battery_profile.compute\" in 2.58s. \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - check_and_save_battery_profile.compute - STEP_START - Started execution of step \"check_and_save_battery_profile.compute\". \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - check_and_save_battery_profile.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input s_battery_profile in memory object store using pickle. \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - check_and_save_battery_profile.compute - STEP_INPUT - Got input \"s_battery_profile\" of type \"Any\". (Type check passed). \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - check_and_save_battery_profile.compute - STEP_INPUT - Got input \"output_data_dir\" of type \"String\". (Type check passed). \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - check_and_save_battery_profile.compute - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed). \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - check_and_save_battery_profile.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle. \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - check_and_save_battery_profile.compute - STEP_SUCCESS - Finished execution of step \"check_and_save_battery_profile.compute\" in 15ms. \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - ENGINE_EVENT - Finished steps in process (pid: 17436) in 2m4s \u001b[32m2021-03-19 01:24:46\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - 01f88437-2b8c-4581-9982-3f76523c2874 - 17436 - PIPELINE_SUCCESS - Finished execution of pipeline \"end_to_end_pipeline\". <dagster.core.execution.results.PipelineExecutionResult at 0x1d1fa542ac0> We'll then visualise the latest charging profile df_latest_submission = pd . read_csv ( '../data/output/latest_submission.csv' ) s_latest_submission = df_latest_submission . set_index ( 'datetime' )[ 'charge_MW' ] s_latest_submission . index = pd . to_datetime ( s_latest_submission . index ) # Plotting fig , ax = plt . subplots ( dpi = 250 ) s_latest_submission . plot ( ax = ax ) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Charge (MW)' ) hlp . hide_spines ( ax ) fig . tight_layout () fig . savefig ( '../img/latest_submission.png' , dpi = 250 ) Finally we'll export the relevant code to our batopt module","title":"End-to-End"},{"location":"methodology/","text":"Method Outline \u00b6 The following questions will help us understand the different approaches which were taken during the challenge as well as the effect of different inputs/features. Please limit total responses to no more than 2 pages. High-Level Overview \u00b6 Please describe a high-level overview of the methods you applied to solve the challenge tasks? If you solved the problem into different components (for example were the discharging and charging components solved separately?) what methods were applied to solve each part? Given the competition assumptions, as long as the battery is fully charged and discharged each day there are no additional benefits to modelling the charge/discharge periods in combination (whereas this does increase the complexity of the task). For this reason the discharging and charging profiles were optimised for separately in this work. There was also a practical element as the two team members were able to split the work more easily in this way. In the first submission both the charge and discharge optimisations were treated as a supervised learning problem, with the \"perfect\" charge/discharge profile calculated for the historical data and then used as the dependant variable. Due to the higher uncertainty of the solar time-series (relative to demand) we discovered that it was beneficial to forecast the solar output first, then apply our peak flattening algorithm to the forecasted output. For the discharge model the supervised learning approach (with discharge as the dependent variable) was found to out-perform the two-stage forecast and flatten method. We used Gaussian Process Regression to model the hyper-parameter optimisation surface in order to carry out a more 'intelligent' search of the parameter space. We used sequential feature selection to select the variables used as inputs. For both the hyper-parameter optimisation and feature selection the metric used was the same as that in the submission evaluation. Software \u00b6 What is the name of the software (proprietary or open-source) which were primarily used to solve the challenge? We used Python, making extensive use of the pandas , sklearn , and skopt libraries. dagster was used to generate an end-to-end pipeline from data retrieval to charge/discharge scheduling. Model Inputs \u00b6 What inputs did you include within the models? Please also include whether you used lagged versions of the variables, any interacting variables, or any features generated/engineered from the data. Discharge Model (in order of importance): hour (integer for time of day) doy (integer for day of year) temp_location4 weekend (dummy variable) SP_demand_7d_lag (demand from a week prior) evening_demand_max_7d_lag (max evening demand from a week prior) evening_demand_avg_7d_lag (mean evening demand from a week prior) temp_location3 daily_avg_temp (average over the full day) temp_location2 spatial_avg_temp temp_location1 temp_location6 temp_location5 dow (integer for day of week) As the main model used was a Random Forest (which can handle discrete changes) the temporal variables did not need further feature engineering. PV Model (no specific order): temp_location1 temp_location2 temp_location3 temp_location4 temp_location5 temp_location6 solar_location1 solar_location2 solar_location3 solar_location5 solar_location6 pv_7d_lag hour doy Model Evolution \u00b6 What changes did you make to your method(s) as the tasks progressed? What were some of the reasonings behind these changes? The key change was the previously discussed move from a supervised learning model for the charge time-series to instead forecasting solar and then flattening the forecast peak. Beyond this we also adjusted our data cleaning method due to the varying issues as new data was released. For each submission we also re-tuned the hyper-parameters and explored the effect of adjusting the coverage of the training data. Christmas Quirks \u00b6 Did you treat the Christmas period task (Task 3) differently compared to the other tasks? If so, what changes did you make? We carried out EDA to determine whether to switch to a similar days/week method for the Christmas submission instead of the previous supervised learning approach. The resulting output was almost identical to the supervised learning approach so we opted to keep the same methodology as used in other weeks for consistency. Covid Quirks \u00b6 Did you treat the task occurring during the COVID lock-down period (Task 4) differently to the other tasks? If so, what changes did you make? The Covid variables we included didn't have a significant impact but included: a covid indicator (0 before 26th march (first lockdown date), 1 afterwards) and the number of days since 26th March 2020. The latter variable was capped at 15th June, when there was a significant easing of restrictions. Data Coverage \u00b6 How much of the data did you use for training, testing and (where applicable) model selection/validation? Prior to the Covid period submission all data was used, for this final submission though the discharge model was trained only on data generated since March 2020. For model selection/validation we used k-fold validation where all days in a given week were grouped together (i.e. not split over multiple batches). General Comments \u00b6 Any other comments or suggestions about the challenge? Our team really enjoyed the challenge, in particular the focus on real-world constraints. It would have been great if the carbon emissions of the grid were based on real-world data from somewhere like www.carbonintensity.org.uk. Although difficult to address we did wonder if there were potential options to normalise the weekly scores to counter-act their varying contribution in the combined submissions score.","title":"Methodology"},{"location":"methodology/#method-outline","text":"The following questions will help us understand the different approaches which were taken during the challenge as well as the effect of different inputs/features. Please limit total responses to no more than 2 pages.","title":"Method Outline"},{"location":"methodology/#high-level-overview","text":"Please describe a high-level overview of the methods you applied to solve the challenge tasks? If you solved the problem into different components (for example were the discharging and charging components solved separately?) what methods were applied to solve each part? Given the competition assumptions, as long as the battery is fully charged and discharged each day there are no additional benefits to modelling the charge/discharge periods in combination (whereas this does increase the complexity of the task). For this reason the discharging and charging profiles were optimised for separately in this work. There was also a practical element as the two team members were able to split the work more easily in this way. In the first submission both the charge and discharge optimisations were treated as a supervised learning problem, with the \"perfect\" charge/discharge profile calculated for the historical data and then used as the dependant variable. Due to the higher uncertainty of the solar time-series (relative to demand) we discovered that it was beneficial to forecast the solar output first, then apply our peak flattening algorithm to the forecasted output. For the discharge model the supervised learning approach (with discharge as the dependent variable) was found to out-perform the two-stage forecast and flatten method. We used Gaussian Process Regression to model the hyper-parameter optimisation surface in order to carry out a more 'intelligent' search of the parameter space. We used sequential feature selection to select the variables used as inputs. For both the hyper-parameter optimisation and feature selection the metric used was the same as that in the submission evaluation.","title":"High-Level Overview"},{"location":"methodology/#software","text":"What is the name of the software (proprietary or open-source) which were primarily used to solve the challenge? We used Python, making extensive use of the pandas , sklearn , and skopt libraries. dagster was used to generate an end-to-end pipeline from data retrieval to charge/discharge scheduling.","title":"Software"},{"location":"methodology/#model-inputs","text":"What inputs did you include within the models? Please also include whether you used lagged versions of the variables, any interacting variables, or any features generated/engineered from the data. Discharge Model (in order of importance): hour (integer for time of day) doy (integer for day of year) temp_location4 weekend (dummy variable) SP_demand_7d_lag (demand from a week prior) evening_demand_max_7d_lag (max evening demand from a week prior) evening_demand_avg_7d_lag (mean evening demand from a week prior) temp_location3 daily_avg_temp (average over the full day) temp_location2 spatial_avg_temp temp_location1 temp_location6 temp_location5 dow (integer for day of week) As the main model used was a Random Forest (which can handle discrete changes) the temporal variables did not need further feature engineering. PV Model (no specific order): temp_location1 temp_location2 temp_location3 temp_location4 temp_location5 temp_location6 solar_location1 solar_location2 solar_location3 solar_location5 solar_location6 pv_7d_lag hour doy","title":"Model Inputs"},{"location":"methodology/#model-evolution","text":"What changes did you make to your method(s) as the tasks progressed? What were some of the reasonings behind these changes? The key change was the previously discussed move from a supervised learning model for the charge time-series to instead forecasting solar and then flattening the forecast peak. Beyond this we also adjusted our data cleaning method due to the varying issues as new data was released. For each submission we also re-tuned the hyper-parameters and explored the effect of adjusting the coverage of the training data.","title":"Model Evolution"},{"location":"methodology/#christmas-quirks","text":"Did you treat the Christmas period task (Task 3) differently compared to the other tasks? If so, what changes did you make? We carried out EDA to determine whether to switch to a similar days/week method for the Christmas submission instead of the previous supervised learning approach. The resulting output was almost identical to the supervised learning approach so we opted to keep the same methodology as used in other weeks for consistency.","title":"Christmas Quirks"},{"location":"methodology/#covid-quirks","text":"Did you treat the task occurring during the COVID lock-down period (Task 4) differently to the other tasks? If so, what changes did you make? The Covid variables we included didn't have a significant impact but included: a covid indicator (0 before 26th march (first lockdown date), 1 afterwards) and the number of days since 26th March 2020. The latter variable was capped at 15th June, when there was a significant easing of restrictions.","title":"Covid Quirks"},{"location":"methodology/#data-coverage","text":"How much of the data did you use for training, testing and (where applicable) model selection/validation? Prior to the Covid period submission all data was used, for this final submission though the discharge model was trained only on data generated since March 2020. For model selection/validation we used k-fold validation where all days in a given week were grouped together (i.e. not split over multiple batches).","title":"Data Coverage"},{"location":"methodology/#general-comments","text":"Any other comments or suggestions about the challenge? Our team really enjoyed the challenge, in particular the focus on real-world constraints. It would have been great if the carbon emissions of the grid were based on real-world data from somewhere like www.carbonintensity.org.uk. Although difficult to address we did wonder if there were potential options to normalise the weekly scores to counter-act their varying contribution in the combined submissions score.","title":"General Comments"},{"location":"notes/00-brief/","text":"Presumed Open Data: Data Science Challenge \u00b6 Presumed Open Data is an exciting Western Power Distribution (WPD) NIA project, in partnership with Energy Systems Catapult and Centre for Sustainable Energy, which is demonstrating the value created by increasing the visibility and availability of energy data. Taking forward the recommendations of the Energy Data Taskforce published in June 2019, WPD is creating an innovation data platform to make many valuable datasets open to the public. To demonstrate the value from making these energy data sets available, we are running a data science challenge using the data sets which will be available on the innovation data platform. The challenge will require participants to utilise sub-hourly data over several years to design an optimal schedule for a battery storage device to simultaneously reduce peak demands and maximise the use of solar photovoltaic generation. The datasets will consist of distribution network demand, solar PV generation outputs and weather data from several sites which can be utilised by participants to learn the best schedule. The challenge represents an important real-world problem whose solution could play an important role in the move to a zero-carbon world. Renewable generation is intermittent and far more uncertain than traditional sources which means storage control will be essential for decarbonising the energy supply. Participants are welcome from all backgrounds, and especially those with keen interests in Energy and/or data science and machine learning. Teams of limited size will be allowed to participate and therefore a mix of different expertise is encouraged. For the winners of the challenger there will be the opportunity to publish their winning entry in a special issue of the Energies journal and pitch their work and solutions to senior representatives from Western Power Distribution, Centre for Sustainable Energy and Energy Systems Catapult as well as other major stakeholders.","title":"Challenge"},{"location":"notes/00-brief/#presumed-open-data-data-science-challenge","text":"Presumed Open Data is an exciting Western Power Distribution (WPD) NIA project, in partnership with Energy Systems Catapult and Centre for Sustainable Energy, which is demonstrating the value created by increasing the visibility and availability of energy data. Taking forward the recommendations of the Energy Data Taskforce published in June 2019, WPD is creating an innovation data platform to make many valuable datasets open to the public. To demonstrate the value from making these energy data sets available, we are running a data science challenge using the data sets which will be available on the innovation data platform. The challenge will require participants to utilise sub-hourly data over several years to design an optimal schedule for a battery storage device to simultaneously reduce peak demands and maximise the use of solar photovoltaic generation. The datasets will consist of distribution network demand, solar PV generation outputs and weather data from several sites which can be utilised by participants to learn the best schedule. The challenge represents an important real-world problem whose solution could play an important role in the move to a zero-carbon world. Renewable generation is intermittent and far more uncertain than traditional sources which means storage control will be essential for decarbonising the energy supply. Participants are welcome from all backgrounds, and especially those with keen interests in Energy and/or data science and machine learning. Teams of limited size will be allowed to participate and therefore a mix of different expertise is encouraged. For the winners of the challenger there will be the opportunity to publish their winning entry in a special issue of the Energies journal and pitch their work and solutions to senior representatives from Western Power Distribution, Centre for Sustainable Energy and Energy Systems Catapult as well as other major stakeholders.","title":"Presumed Open Data: Data Science Challenge"},{"location":"notes/01-kickoff-meeting/","text":"Kickoff Meeting - 28th January 2021 \u00b6 We get three datasets: demand at a single substation; solar PV data from a site near the substation; weather reanalysis. Each task is 1 week long, where we are given the weather reanalysis data and no demand or solar. The is to produce a battery schedule that maximises a score function that promotes peak demand reduction (between 3.30\u20139pm each day) and the proportion of the batteries charge that came from solar PV (vs. the grid) First challenge is a practice that is due in 2 weeks i think, then there is one every week after that for 4 weeks","title":"Kickoff Meeting - 28th January 2021"},{"location":"notes/01-kickoff-meeting/#kickoff-meeting-28th-january-2021","text":"We get three datasets: demand at a single substation; solar PV data from a site near the substation; weather reanalysis. Each task is 1 week long, where we are given the weather reanalysis data and no demand or solar. The is to produce a battery schedule that maximises a score function that promotes peak demand reduction (between 3.30\u20139pm each day) and the proportion of the batteries charge that came from solar PV (vs. the grid) First challenge is a practice that is due in 2 weeks i think, then there is one every week after that for 4 weeks","title":"Kickoff Meeting - 28th January 2021"}]}