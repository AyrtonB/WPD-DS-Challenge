{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "strategic-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-palestine",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "<br>\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "miniature-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dagster import execute_pipeline, pipeline, solid, Field\n",
    "\n",
    "from batopt import clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-effect",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### End-to-End\n",
    "\n",
    "We're now going to combine these steps into a pipeline using dagster, first we'll create the individual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "molecular-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "@solid()\n",
    "def load_data(_, raw_data_dir: str):\n",
    "    loaded_data = dict()\n",
    "    \n",
    "    loaded_data['pv'] = clean.load_training_dataset(raw_data_dir, 'pv')\n",
    "    loaded_data['demand'] = clean.load_training_dataset(raw_data_dir, 'demand')\n",
    "    loaded_data['weather'] = clean.load_training_dataset(raw_data_dir, 'weather', dt_idx_freq='H')\n",
    "    \n",
    "    return loaded_data\n",
    "\n",
    "@solid()\n",
    "def clean_data(_, loaded_data, intermediate_data_dir: str):\n",
    "    # Cleaning\n",
    "    cleaned_data = dict()\n",
    "\n",
    "    cleaned_data['pv'] = (loaded_data['pv']\n",
    "                          .pipe(clean.interpolate_missing_panel_temps, loaded_data['weather'])\n",
    "                          .pipe(clean.interpolate_missing_site_irradiance, loaded_data['weather'])\n",
    "                          .pipe(clean.interpolate_missing_site_power)\n",
    "                         )\n",
    "    cleaned_data['weather'] = clean.interpolate_missing_weather_solar(loaded_data['pv'], loaded_data['weather'])\n",
    "    cleaned_data['demand'] = loaded_data['demand']\n",
    "    \n",
    "    \n",
    "    # Saving\n",
    "    cleaned_data['pv'].to_csv(f'{intermediate_data_dir}/cleaned_pv.csv')\n",
    "    cleaned_data['demand'].to_csv(f'{intermediate_data_dir}/cleaned_demand.csv')\n",
    "    cleaned_data['weather'].to_csv(f'{intermediate_data_dir}/cleaned_weather.csv')\n",
    "            \n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-ethernet",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Then we'll combine them in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "important-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline\n",
    "def end_to_end_pipeline(): \n",
    "    loaded_data = load_data()\n",
    "    cleaned_data = clean_data(loaded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-pressure",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Which we'll now run a test with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sustained-salvation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2021-01-29 01:28:26\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - dcacb44f-a4a2-4759-be05-a750516d08a5 - 912 - ENGINE_EVENT - Starting initialization of resources [asset_store].\n",
      "\u001b[32m2021-01-29 01:28:26\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - dcacb44f-a4a2-4759-be05-a750516d08a5 - 912 - ENGINE_EVENT - Finished initialization of resources [asset_store].\n",
      "\u001b[32m2021-01-29 01:28:26\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - dcacb44f-a4a2-4759-be05-a750516d08a5 - 912 - PIPELINE_START - Started execution of pipeline \"end_to_end_pipeline\".\n",
      "\u001b[32m2021-01-29 01:28:26\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - dcacb44f-a4a2-4759-be05-a750516d08a5 - 912 - ENGINE_EVENT - Executing steps in process (pid: 912)\n",
      "\u001b[32m2021-01-29 01:28:26\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - dcacb44f-a4a2-4759-be05-a750516d08a5 - 912 - load_data.compute - STEP_START - Started execution of step \"load_data.compute\".\n",
      "\u001b[32m2021-01-29 01:28:26\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - dcacb44f-a4a2-4759-be05-a750516d08a5 - 912 - load_data.compute - STEP_INPUT - Got input \"raw_data_dir\" of type \"String\". (Type check passed).\n",
      "\u001b[32m2021-01-29 01:28:26\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - dcacb44f-a4a2-4759-be05-a750516d08a5 - 912 - load_data.compute - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed).\n",
      "\u001b[32m2021-01-29 01:28:26\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - dcacb44f-a4a2-4759-be05-a750516d08a5 - 912 - load_data.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle.\n",
      "\u001b[32m2021-01-29 01:28:26\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - dcacb44f-a4a2-4759-be05-a750516d08a5 - 912 - load_data.compute - STEP_SUCCESS - Finished execution of step \"load_data.compute\" in 175ms.\n",
      "\u001b[32m2021-01-29 01:28:26\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - dcacb44f-a4a2-4759-be05-a750516d08a5 - 912 - clean_data.compute - STEP_START - Started execution of step \"clean_data.compute\".\n",
      "\u001b[32m2021-01-29 01:28:26\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - dcacb44f-a4a2-4759-be05-a750516d08a5 - 912 - clean_data.compute - OBJECT_STORE_OPERATION - Retrieved intermediate object for input loaded_data in memory object store using pickle.\n",
      "\u001b[32m2021-01-29 01:28:26\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - dcacb44f-a4a2-4759-be05-a750516d08a5 - 912 - clean_data.compute - STEP_INPUT - Got input \"loaded_data\" of type \"Any\". (Type check passed).\n",
      "\u001b[32m2021-01-29 01:28:26\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - dcacb44f-a4a2-4759-be05-a750516d08a5 - 912 - clean_data.compute - STEP_INPUT - Got input \"intermediate_data_dir\" of type \"String\". (Type check passed).\n",
      "\u001b[32m2021-01-29 01:29:13\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - dcacb44f-a4a2-4759-be05-a750516d08a5 - 912 - clean_data.compute - STEP_OUTPUT - Yielded output \"result\" of type \"Any\". (Type check passed).\n",
      "\u001b[32m2021-01-29 01:29:13\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - dcacb44f-a4a2-4759-be05-a750516d08a5 - 912 - clean_data.compute - OBJECT_STORE_OPERATION - Stored intermediate object for output result in memory object store using pickle.\n",
      "\u001b[32m2021-01-29 01:29:13\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - dcacb44f-a4a2-4759-be05-a750516d08a5 - 912 - clean_data.compute - STEP_SUCCESS - Finished execution of step \"clean_data.compute\" in 46.8s.\n",
      "\u001b[32m2021-01-29 01:29:13\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - dcacb44f-a4a2-4759-be05-a750516d08a5 - 912 - ENGINE_EVENT - Finished steps in process (pid: 912) in 46.99s\n",
      "\u001b[32m2021-01-29 01:29:13\u001b[0m - dagster - \u001b[34mDEBUG\u001b[0m - end_to_end_pipeline - dcacb44f-a4a2-4759-be05-a750516d08a5 - 912 - PIPELINE_SUCCESS - Finished execution of pipeline \"end_to_end_pipeline\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dagster.core.execution.results.PipelineExecutionResult at 0x21b565a2ca0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_config = {\n",
    "    'solids': {\n",
    "        'load_data': {\n",
    "            'inputs': {\n",
    "                'raw_data_dir': '../data/raw',\n",
    "            },\n",
    "        },\n",
    "        'clean_data': {\n",
    "            'inputs': {\n",
    "                'intermediate_data_dir': '../data/intermediate',\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "execute_pipeline(end_to_end_pipeline, run_config=run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-native",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-frost",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "legislative-median",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Finally we'll export the relevant code to our `batopt` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "    \n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "batopt",
   "language": "python",
   "name": "batopt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
