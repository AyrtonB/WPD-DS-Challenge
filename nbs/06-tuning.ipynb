{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-opening",
   "metadata": {},
   "source": [
    "# Tuning\n",
    "\n",
    "<br>\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from skopt.plots import plot_objective\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from batopt import clean, discharge, charge, pv, utils\n",
    "\n",
    "import os\n",
    "from ipypb import track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "import FEAutils as hlp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-burke",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_data_dir = '../data/intermediate'\n",
    "raw_data_dir = '../data/raw'\n",
    "cache_data_dir = '../data/nb-cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-traffic",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Preparing Data\n",
    "\n",
    "First we'll load in the target and feature data for both the charging and discharging models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-monday",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_x, charge_y = pv.prepare_training_input_data(intermediate_data_dir)\n",
    "discharge_x, discharge_y = discharge.prepare_training_input_data(intermediate_data_dir)\n",
    "\n",
    "charge_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-seller",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_demand = clean.load_training_dataset(intermediate_data_dir, 'demand')['demand_MW']\n",
    "\n",
    "s_demand.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-output",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pv = clean.load_training_dataset(intermediate_data_dir, 'pv')['pv_power_mw']\n",
    "\n",
    "s_pv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-guide",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def get_train_test_arr(arr, start_of_test_period): \n",
    "    train_arr = arr[:pd.to_datetime(start_of_test_period, utc=True)]\n",
    "    test_arr = arr[pd.to_datetime(start_of_test_period, utc=True):]\n",
    "    \n",
    "    return train_arr, test_arr\n",
    "\n",
    "def get_train_test_Xy(X, y, start_of_test_period): \n",
    "    x_train, x_test = get_train_test_arr(X, start_of_test_period)\n",
    "    y_train, y_test = get_train_test_arr(y, start_of_test_period)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_of_test_period = '2020-06-15'\n",
    "\n",
    "charge_x_train, charge_x_test, charge_y_train, charge_y_test = pv.get_train_test_Xy(charge_x, charge_y, start_of_test_period)\n",
    "discharge_x_train, discharge_x_test, discharge_y_train, discharge_y_test = pv.get_train_test_Xy(discharge_x, discharge_y, start_of_test_period)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-western",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "We want to evaluate each of our models based on their contribution to the final scoring value, to do this we'll first create some predictions for our discharge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-bronze",
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge_rf = RandomForestRegressor()\n",
    "\n",
    "discharge_rf.fit(discharge_x_train, discharge_y_train)\n",
    "discharge_y_pred = pd.Series(discharge_rf.predict(discharge_x_test), index=discharge_x_test.index)\n",
    "\n",
    "discharge_y_pred.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-china",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We'll then create a time-series of the percentage peak reduction for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def calculate_pct_peak_reduction_s(discharge_y_pred, s_demand):\n",
    "    s_demand_test = s_demand.loc[discharge_y_pred.index]\n",
    "\n",
    "    s_old_peaks = s_demand_test.groupby(s_demand_test.index.date).max()\n",
    "    s_new_peaks = (s_demand_test+discharge_y_pred).groupby(s_demand_test.index.date).max()\n",
    "\n",
    "    s_pct_peak_reduction = 100*(s_old_peaks - s_new_peaks)/s_new_peaks\n",
    "    s_pct_peak_reduction.index = pd.to_datetime(s_pct_peak_reduction.index)\n",
    "\n",
    "    return s_pct_peak_reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-landing",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pct_peak_reduction = calculate_pct_peak_reduction_s(discharge_y_pred, s_demand)\n",
    "\n",
    "print(f'The average peak reduction was {s_pct_peak_reduction.mean():.2f}%')\n",
    "\n",
    "s_pct_peak_reduction.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-april",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We'll then repeat this with the charging model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_rf = RandomForestRegressor()\n",
    "\n",
    "charge_rf.fit(charge_x_train, charge_y_train)\n",
    "charge_y_pred = pd.Series(charge_rf.predict(charge_x_test), index=charge_x_test.index)\n",
    "\n",
    "charge_y_pred.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-soldier",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "For which we'll calculate the emissions factor series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def calculate_emissions_factor_s(charge_y_pred, s_pv, solar_factor=3, grid_factor=1):\n",
    "    s_solar_charge_pct = (charge_y_pred - s_pv.loc[charge_y_pred.index]).clip(0).groupby(charge_y_pred.index.date).sum()/charge_y_pred.groupby(charge_y_pred.index.date).sum()\n",
    "    s_grid_charge_pct = 1 - s_solar_charge_pct\n",
    "\n",
    "    s_emissions_factor = solar_factor*s_solar_charge_pct + grid_factor*s_grid_charge_pct\n",
    "    s_emissions_factor.index = pd.to_datetime(s_emissions_factor.index)\n",
    "\n",
    "    return s_emissions_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_emissions_factor = calculate_emissions_factor_s(charge_y_pred, s_pv)\n",
    "\n",
    "s_emissions_factor.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-performance",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We can then combine these two steps to determine our final score for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calculate_score_s(discharge_y_pred, charge_y_pred, s_demand, s_pv, solar_factor=3, grid_factor=1):\n",
    "    s_pct_peak_reduction = calculate_pct_peak_reduction_s(discharge_y_pred, s_demand)\n",
    "    s_emissions_factor = calculate_emissions_factor_s(charge_y_pred, s_pv, solar_factor=solar_factor, grid_factor=grid_factor)\n",
    "    \n",
    "    s_score = s_pct_peak_reduction*s_emissions_factor\n",
    "    \n",
    "    return s_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_score = calculate_score_s(discharge_y_pred, charge_y_pred, s_demand, s_pv)\n",
    "\n",
    "print(f'The average score was: {s_score.mean():.2f}')\n",
    "\n",
    "s_score.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-writing",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "For the charging we can also look at how much was sourced from PV relative to the potential maximum (capped at 6 MWh per day)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_charge = np.minimum(charge_y_pred, s_pv.loc[charge_y_pred.index])\n",
    "day_solar_charge = solar_charge.groupby(solar_charge.index.date).sum().clip(0,12)\n",
    "day_solar_charge.index = pd.to_datetime(day_solar_charge.index)\n",
    "\n",
    "solar_potential = np.clip(s_pv.loc[charge_y_pred.index], 0, 2.5)\n",
    "day_solar_potential = solar_potential.groupby(solar_potential.index.date).sum().clip(0,12)\n",
    "day_solar_potential.index = pd.to_datetime(day_solar_potential.index)\n",
    "\n",
    "day_solar_charge.plot()\n",
    "day_solar_potential.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-quarterly",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_exploit = 100 * day_solar_charge/day_solar_potential\n",
    "pct_exploit.plot()\n",
    "plt.ylabel('% exploited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-worship",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def score_charge(schedule, solar_profile, solar_factor=3, grid_factor=1):\n",
    "    # The actual pv charge is the minimum of the scheduled charge and the actual solar availability \n",
    "    actual_pv_charge = np.minimum(schedule.values, solar_profile.values)\n",
    "    actual_pv_charge = pd.Series(actual_pv_charge, index=schedule.index)\n",
    "        \n",
    "    pct_pv_charge = actual_pv_charge.groupby(actual_pv_charge.index.date).sum() / schedule.groupby(schedule.index.date).sum()\n",
    "    pct_grid_charge = 1 - pct_pv_charge\n",
    "    \n",
    "    score = (solar_factor * pct_pv_charge) + (grid_factor * pct_grid_charge)\n",
    "    \n",
    "    return score\n",
    "\n",
    "def score_discharge(schedule, demand):\n",
    "    \n",
    "    new_demand = schedule + demand\n",
    "    old_demand = demand\n",
    "        \n",
    "    new_peaks = new_demand.groupby(new_demand.index.date).max()\n",
    "    old_peaks = old_demand.groupby(old_demand.index.date).max()\n",
    "            \n",
    "    pct_reduction = 100*((old_peaks - new_peaks)/ old_peaks)\n",
    "    \n",
    "    return pct_reduction\n",
    "\n",
    "def max_charge_score(solar_profile, solar_factor=3, grid_factor=1, capacity=6, time_unit=0.5):\n",
    "    pv_potential = solar_profile.groupby(solar_profile.index.date).sum().clip(0, capacity/time_unit)\n",
    "    pct_pv_charge = pv_potential / (capacity/time_unit)\n",
    "    pct_grid_charge = 1 - pct_pv_charge\n",
    "    \n",
    "    score = (solar_factor * pct_pv_charge) + (grid_factor * pct_grid_charge)\n",
    "    \n",
    "    return score\n",
    "    \n",
    "       \n",
    "def calculate_score_s(discharge_y_pred, charge_y_pred, s_demand, s_pv, solar_factor=3, grid_factor=1):\n",
    "    \n",
    "    charge_score = score_charge(charge_y_pred, s_pv, solar_factor, grid_factor)\n",
    "    discharge_score = score_discharge(discharge_y_pred, s_demand)\n",
    "    \n",
    "    s_score = discharge_score*charge_score\n",
    "    \n",
    "    return s_score, charge_score, discharge_score\n",
    "\n",
    "def evaluate_submission(submission, intermediate_data_dir):\n",
    "    if isinstance(submission, str):\n",
    "        df_solution = pd.read_csv(submission)\n",
    "        df_solution = df_solution.set_index(pd.to_datetime(df_solution.datetime, utc=True))\n",
    "    else:\n",
    "        assert isinstance(submission, pd.DataFrame), '`submission` must either be a valid submission dataframe or a filepath to the submission'\n",
    "        df_solution = submission\n",
    "        \n",
    "    df_real = clean.combine_training_datasets(intermediate_data_dir)\n",
    "    df_real = df_real[df_real.index.isin(df_solution.index)]\n",
    "\n",
    "    df_solution_charge = df_solution.between_time('00:00', '15:00')\n",
    "    df_solution_discharge = df_solution.between_time('15:30', '20:30')\n",
    "\n",
    "    df_real_charge = df_real.between_time('00:00', '15:00')\n",
    "    df_real_discharge = df_real.between_time('15:30', '20:30')\n",
    "\n",
    "    total_score, charge_score, discharge_score = calculate_score_s(df_solution_discharge.charge_MW, df_solution_charge.charge_MW, df_real_discharge.demand_MW, df_real_charge.pv_power_mw)\n",
    "\n",
    "    df_results = pd.DataFrame({\n",
    "        'total_score': total_score,\n",
    "        'charge_score': charge_score,\n",
    "        'discharge_score': discharge_score, \n",
    "        'max_charge_score': max_charge_score(df_real_charge.pv_power_mw)\n",
    "    })\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_fp = '../data/output/ESAIL_set1.csv'\n",
    "\n",
    "df_results = evaluate_submission(submission_fp, intermediate_data_dir)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-infrared",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We can then calculate our average score over this period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-triangle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['total_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-model",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Discharge Model Tuning\n",
    "\n",
    "We'll begin by carrying out some feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def feature_selection(x_train, y_train, groups=None, model=RandomForestRegressor(), min_num_features=1, max_num_features=None, **sfs_kwargs):\n",
    "    if max_num_features is None:\n",
    "        max_num_features = 1 + x_train.shape[1]\n",
    "        \n",
    "    result_features = dict()\n",
    "    result_scores = dict()\n",
    "\n",
    "    for num_features in track(range(min_num_features, max_num_features)):\n",
    "        sfs = SFS(\n",
    "            model,\n",
    "            k_features=num_features, \n",
    "            **sfs_kwargs\n",
    "        )\n",
    "\n",
    "        sfs.fit(x_train, y_train, groups=groups)\n",
    "\n",
    "        result_features[num_features] = sfs.k_feature_names_\n",
    "        result_scores[num_features] = sfs.k_score_\n",
    "        \n",
    "    return result_features, result_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_reduction_scorer = discharge.construct_peak_reduction_calculator(s_demand=s_demand.loc[discharge_x_train.index], scorer=True)\n",
    "week_groups = discharge_x_train.index.year + discharge_x_train.index.isocalendar().week/52\n",
    "\n",
    "rerun_feature_selection = False\n",
    "feature_selection_filename = f'feature_selection.json'\n",
    "\n",
    "if (rerun_feature_selection == True) or (feature_selection_filename not in os.listdir(cache_data_dir)):\n",
    "    result_features, result_scores = feature_selection(discharge_x_train, discharge_y_train, groups=week_groups, n_jobs=-1)\n",
    "    \n",
    "    with open(f'{cache_data_dir}/{feature_selection_filename}', 'w') as fp:\n",
    "        json.dump(dict(zip(['features', 'scores'], [result_features, result_scores])), fp)\n",
    "        \n",
    "else:\n",
    "    with open(f'{cache_data_dir}/{feature_selection_filename}', 'r') as fp:\n",
    "        results = json.load(fp)\n",
    "        \n",
    "    result_features, result_scores = results['features'], results['scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-collar",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We can visualise how the model accuracy changes with the number of features included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(result_scores).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-portal",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We'll also calculate the relative importance of each feature by counting how many times they were included in the optimal feature subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_iterables = lambda iterable: [item for sublist in list(iterable) for item in sublist]\n",
    "\n",
    "s_feature_importance = pd.Series(flatten_iterables(result_features.values())).value_counts().divide(len(result_features))\n",
    "\n",
    "s_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-least",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We'll now do some hyper-parameter tuning using the `skopt` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-record",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = s_feature_importance.index[:11]\n",
    "evening_datetimes = discharge.extract_evening_datetimes(discharge_x_train)\n",
    "week_groups = discharge_x_train.index.year + discharge_x_train.index.isocalendar().week/52\n",
    "peak_reduction_scorer = discharge.construct_peak_reduction_calculator(s_demand=s_demand, scorer=True)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # Add in oversampling of more recent/similar dates\n",
    "    ('pandas_RF', utils.PandasRandomForestRegressor())\n",
    "])\n",
    "\n",
    "search_spaces = {\n",
    "        'pandas_RF__min_samples_leaf': Integer(1, 20, 'uniform'),\n",
    "        'pandas_RF__criterion': Categorical(['mse', 'mae']),\n",
    "        'pandas_RF__n_estimators': Integer(50, 250, 'uniform'),\n",
    "        'pandas_RF__max_features': Categorical(['auto', 'sqrt']),\n",
    "        'pandas_RF__max_depth': Integer(10, 50, 'uniform'),\n",
    "        'pandas_RF__min_samples_split': Integer(2, 10, 'uniform'),\n",
    "        'pandas_RF__min_samples_leaf': Integer(1, 4, 'uniform'),\n",
    "        'pandas_RF__bootstrap': Categorical([True, False])\n",
    "}\n",
    "\n",
    "opt = utils.BayesSearchCV(\n",
    "    pipeline,\n",
    "    search_spaces,\n",
    "    n_iter=15,\n",
    "    verbose=1,\n",
    "    cv=8, # 8 works well for me as that's how many concurrent workers I can use\n",
    "    scoring=peak_reduction_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "fit_BayesSearchCV = False\n",
    "\n",
    "if fit_BayesSearchCV == True:\n",
    "    opt.fit(discharge_x_train[features], discharge_y_train, groups=evening_datetimes.date)\n",
    "\n",
    "    print(f'Cross-validation score: {opt.best_score_:.2f}')\n",
    "    print(f'Hold-out score: {opt.score(discharge_x_test[features], discharge_y_test):.2f}')\n",
    "    print(f'\\nBest params: \\n{opt.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-capability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to be saving model runs\n",
    "# could include as part of a callback?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-stadium",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Model Comparisons\n",
    "\n",
    "Here we'll compare our discharge v pv-forecast modelling approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.date_range('2019-03-02', '2019-03-09 23:30', freq='30T', tz='UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-jamaica",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "discharge_opt_model_fp = '../models/discharge_opt.sav'\n",
    "\n",
    "X, y = discharge.prepare_training_input_data(intermediate_data_dir)\n",
    "idxs_to_keep = sorted(list(set(X.index) - set(index)))\n",
    "X, y = X.loc[idxs_to_keep], y.loc[idxs_to_keep]\n",
    "\n",
    "discharge.fit_and_save_model(X, y, discharge_opt_model_fp)\n",
    "s_discharge_profile = discharge.optimise_test_discharge_profile(raw_data_dir, intermediate_data_dir, discharge_opt_model_fp, index=index)\n",
    "\n",
    "s_discharge_profile.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-valuation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "charge_opt_model_fp = '../models/charge_opt.sav'\n",
    "\n",
    "X, y = charge.prepare_training_input_data(intermediate_data_dir, start_hour=5)\n",
    "idxs_to_keep = sorted(list(set(X.index) - set(index)))\n",
    "X, y = X.loc[idxs_to_keep], y.loc[idxs_to_keep]\n",
    "\n",
    "charge.fit_and_save_charging_model(X, y, charge_opt_model_fp)\n",
    "s_charge_profile = charge.optimise_test_charge_profile(raw_data_dir, intermediate_data_dir, charge_opt_model_fp, index=index)\n",
    "\n",
    "s_charge_profile.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-salem",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We'll now create the charging profile using the PV forecast, in this instance we'll use a linear model for the solar forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_model_fp = '../models/pv_model.sav'\n",
    "\n",
    "X, y = pv.prepare_training_input_data(intermediate_data_dir, start_hour=5)\n",
    "idxs_to_keep = sorted(list(set(X.index) - set(index)))\n",
    "X, y = X.loc[idxs_to_keep], y.loc[idxs_to_keep]\n",
    "\n",
    "pv.fit_and_save_pv_model(X, y, pv_model_fp)\n",
    "s_charge_profile = pv.optimise_test_charge_profile(raw_data_dir, intermediate_data_dir, pv_model_fp, index=index)\n",
    "\n",
    "s_charge_profile.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-purse",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "In this example we repeat the same procedure using a random forest instead of linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_model_fp = '../models/pv_model.sav'\n",
    "\n",
    "X, y = pv.prepare_training_input_data(intermediate_data_dir, start_hour=5)\n",
    "idxs_to_keep = sorted(list(set(X.index) - set(index)))\n",
    "X, y = X.loc[idxs_to_keep], y.loc[idxs_to_keep]\n",
    "\n",
    "pv.fit_and_save_pv_model(X, y, pv_model_fp, model_class=RandomForestRegressor)\n",
    "s_charge_profile = pv.optimise_test_charge_profile(raw_data_dir, intermediate_data_dir, pv_model_fp, index=index)\n",
    "\n",
    "s_charge_profile.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-crash",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-familiar",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = (s_discharge_profile+s_charge_profile).to_frame(name='charge_MW')\n",
    "\n",
    "df_results = evaluate_submission(submission, intermediate_data_dir)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-thing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['total_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-alberta",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = (s_discharge_profile+s_charge_profile).to_frame(name='charge_MW')\n",
    "\n",
    "df_results = evaluate_submission(submission, intermediate_data_dir)\n",
    "\n",
    "df_results['total_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-market",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "downtown-reading",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Finally we'll export the relevant code to our `batopt` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "    \n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-given",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "batopt",
   "language": "python",
   "name": "batopt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
