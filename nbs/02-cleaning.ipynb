{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-grace",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "<br>\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from ipypb import track\n",
    "\n",
    "from batopt import utils, retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-mapping",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = '../data/raw'\n",
    "cache_data_dir = '../data/nb-cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-smith",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Loading the Raw Data\n",
    "\n",
    "We'll start by loading in the demand data, first we have to determine the latest training set that is available for us to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def identify_latest_set_num(data_dir):\n",
    "    set_num = max([\n",
    "        int(f.split('_set')[1].replace('.csv', '')) \n",
    "        for f in os.listdir(data_dir) \n",
    "        if 'set' in f\n",
    "    ])\n",
    "    \n",
    "    return set_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_num = identify_latest_set_num(raw_data_dir)\n",
    "\n",
    "set_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-classroom",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We'll then load in and clean the datetime index of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def reindex_df_dt_idx(df, freq='30T'):\n",
    "    full_dt_idx = pd.date_range(df.index.min(), df.index.max(), freq=freq)\n",
    "    df = df.reindex(full_dt_idx)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_training_dataset(raw_data_dir: str, dataset_name: str='demand', set_num=None, parse_dt_idx: bool=True, dt_idx_freq: str='30T') -> pd.DataFrame:\n",
    "    if set_num is None:\n",
    "        set_num = identify_latest_set_num(raw_data_dir)\n",
    "        \n",
    "    allowed_datasets = ['demand', 'pv', 'weather']\n",
    "    assert dataset_name in allowed_datasets, f\"`dataset_name` must be one of: {', '.join(allowed_datasets)} - not {dataset_name}\"\n",
    "    \n",
    "    df = pd.read_csv(glob.glob(f'{raw_data_dir}/{dataset_name}*set{set_num}.csv')[0].replace('\\\\', '/'))\n",
    "    \n",
    "    if parse_dt_idx == True:\n",
    "        assert 'datetime' in df.columns, 'if `parse_dt_idx` is True then `datetime` must be a column in the dataset'\n",
    "        \n",
    "        df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "        df = df.set_index('datetime').pipe(reindex_df_dt_idx, freq=dt_idx_freq).sort_index(axis=1)\n",
    "        df.index.name = 'datetime'\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand = load_training_dataset(raw_data_dir, 'demand')\n",
    "\n",
    "df_demand.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-antenna",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Then the pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pv = load_training_dataset(raw_data_dir, 'pv')\n",
    "\n",
    "df_pv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-retail",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "And finally the weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = load_training_dataset(raw_data_dir, 'weather', dt_idx_freq='H')\n",
    "\n",
    "df_weather.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-hindu",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We'll also create a function that reads all of the datasets in at once and then combines them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def combine_training_datasets(raw_data_dir, set_num=None):\n",
    "    # Loading provided training datasets\n",
    "    single_datasets = dict()\n",
    "    dataset_names = ['demand', 'pv', 'weather']\n",
    "\n",
    "    for dataset_name in dataset_names:\n",
    "        single_datasets[dataset_name] = load_training_dataset(raw_data_dir, dataset_name, set_num=set_num)\n",
    "    \n",
    "    # Constructing date range\n",
    "    min_dt = min([df.index.min() for df in single_datasets.values()])\n",
    "    max_dt = max([df.index.max() for df in single_datasets.values()]) + pd.Timedelta(minutes=30)\n",
    "\n",
    "    dt_rng = pd.date_range(min_dt, max_dt, freq='30T')\n",
    "    \n",
    "    # Constructing combined dataframe\n",
    "    df_combined = pd.DataFrame(index=dt_rng, columns=dataset_names)\n",
    "    \n",
    "    for dataset_name in dataset_names:\n",
    "        df_single_dataset = single_datasets[dataset_name]\n",
    "        cols_to_be_overwritten = set(df_combined.columns) - (set(df_combined.columns) - set(df_single_dataset.columns))\n",
    "        assert len(cols_to_be_overwritten) == 0, f\"The following columns exist in multiple datasets meaning data would be overwritten: {', '.join(cols_to_be_overwritten)}\"\n",
    "\n",
    "        df_combined[df_single_dataset.columns] = df_single_dataset\n",
    "    \n",
    "    df_combined = df_combined.sort_index()\n",
    "    \n",
    "    # Adding holiday dates\n",
    "    s_holidays = retrieval.load_holidays_s(raw_data_dir)\n",
    "    \n",
    "    s_cropped_holidays = s_holidays[max(df_combined.index.min(), s_holidays.index.min()):\n",
    "                                    min(df_combined.index.max(), s_holidays.index.max())]\n",
    "    \n",
    "    df_combined.loc[s_cropped_holidays.index, 'holidays'] = s_cropped_holidays\n",
    "    \n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = combine_training_datasets(raw_data_dir)\n",
    "\n",
    "df_combined.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-graduate",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "### Identifying Missing Values\n",
    "\n",
    "We'll quickly inspect the datasets and check their coverage over the full date range when aggregated by dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def identify_df_dt_entries(df_demand, df_pv, df_weather):\n",
    "    min_dt = min(df_demand.index.min(), df_pv.index.min(), df_weather.index.min())\n",
    "    max_dt = max(df_demand.index.max(), df_pv.index.max(), df_weather.index.max())\n",
    "    \n",
    "    dt_rng = pd.date_range(min_dt, max_dt, freq='30T')\n",
    "    df_nulls = pd.DataFrame(index=dt_rng)\n",
    "    \n",
    "    df_nulls['demand'] = df_demand.reindex(dt_rng).isnull().mean(axis=1).astype(int)\n",
    "    df_nulls['pv'] = df_pv.reindex(dt_rng).isnull().mean(axis=1).astype(int)\n",
    "    df_nulls['weather'] = df_weather.reindex(dt_rng).ffill(limit=1).isnull().mean(axis=1).astype(int)\n",
    "    \n",
    "    df_entries = 1 - df_nulls\n",
    "    \n",
    "    return df_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entries = identify_df_dt_entries(df_demand, df_pv, df_weather)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "\n",
    "sns.heatmap(df_entries.T, ax=ax, cmap=plt.cm.binary)\n",
    "\n",
    "utils.set_date_ticks(ax, '2015-01-01', '2018-07-31', axis='x', freq='Qs', date_format='%b %y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-place",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We'll also determine the null percentage in each individual column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-monkey",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We can see that all of the PV data columns are missing some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pv.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-monthly",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Location 2 is also missing some solar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! We also need to handle the two days where demand is constantly 0 !!!\n",
    "# Separately it looks like there's also some anomalous 0 readings for the PV output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_demand==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand.query('demand_MW==0').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-comedy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "steady-carter",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Handling Missing Values\n",
    "\n",
    "We'll start by interpolating the missing PV data, first checking the number of variables that have null values for each time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_pv_num_null_vals = df_pv.isnull().sum(axis=1).replace(0, np.nan).dropna().astype(int)\n",
    "\n",
    "s_pv_num_null_vals.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-robin",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "`pv_power_mw` and `irradiance_Wm-2` have the same average number of null values, there are also no time-periods where only 2 variables have null values - it's therefore likely that power and irradiance always have null periods at the same time which makes it harder to interpolate their values. We'll quickly check this hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_pv['pv_power_mw'].isnull() == df_pv['irradiance_Wm-2'].isnull()).mean() == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-buffer",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "It appears as though the `pv_power_mw` and `irradiance_Wm-2` missing values are a single time-block that coincides with a larger set of missing values within `panel_temp_C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pv[df_pv['pv_power_mw'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-combination",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Looking at the `panel_temp_C` data we can see there are 3 time-blocks where obervations are missing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pv['panel_temp_C'].isnull().astype(int).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-parliament",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "One option might to be replace the missing temperature values with the temperatures observed at the surrounding weather grid locations, we'll start by constructing a dataframe that includes all of the temperature data as well as the average rolling temperature for each weather data location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def construct_df_temp_features(df_weather, df_pv):\n",
    "    df_weather = df_weather.reindex(pd.date_range(df_weather.index.min(), df_weather.index.max(), freq='30T')).ffill(limit=1)\n",
    "    temp_loc_cols = df_weather.columns[df_weather.columns.str.contains('temp')]\n",
    "    \n",
    "    df_temp_features = (df_weather\n",
    "                        .copy()\n",
    "                        [temp_loc_cols]\n",
    "                        .assign(site_temp=df_pv['panel_temp_C'])\n",
    "                       )\n",
    "\n",
    "    df_temp_features[[col+'_rolling' for col in temp_loc_cols]] = df_temp_features.rolling(3).mean()[temp_loc_cols]\n",
    "\n",
    "    df_temp_features = df_temp_features.sort_index(axis=1)\n",
    "\n",
    "    return df_temp_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_features = construct_df_temp_features(df_weather, df_pv).dropna()\n",
    "\n",
    "df_temp_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-captain",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We'll now check the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-despite",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_temp_features.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-convert",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The correlation drops off quickly when it gets to the site temperature, looking at the full distributions we can see that the site measurements get far higher. This is because the panel is absorbing heat that raises its temperature above that of the surrounding area, again making it more difficult to simply fill in with the nearby temperature measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_temp_features['site_temp'], color='C0', label='Panel')\n",
    "sns.distplot(df_temp_features.drop('site_temp', axis=1).min(axis=1), color='C1', label='MERRA Min')\n",
    "sns.distplot(df_temp_features.drop('site_temp', axis=1).max(axis=1), color='C2', label='MERRA Max')\n",
    "\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-backing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could use an RF to estimate the panel temp based on the weather grid temps?\n",
    "# Potential features: current average surrounding temp, average surrounding temp over the last 3 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def split_X_y_data(df, target_col='site_temp'):\n",
    "    df = df.dropna()\n",
    "    X_cols = df.drop(target_col, axis=1).columns\n",
    "\n",
    "    X = df[X_cols].values\n",
    "    y = df[target_col].values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def split_X_y_data_with_index(df, target_col='site_temp'):\n",
    "    df = df.dropna()\n",
    "    X_cols = df.drop(target_col, axis=1).columns\n",
    "\n",
    "    X = df[X_cols].values\n",
    "    y = df[target_col].values\n",
    "    index = df.index\n",
    "    \n",
    "    return X, y, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_X_y_data(df_temp_features)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-cheese",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def generate_kfold_preds(\n",
    "    X, \n",
    "    y, \n",
    "    model=LinearRegression(), \n",
    "    kfold_kwargs={'n_splits': 5, 'shuffle': True},\n",
    "    index=None\n",
    "):\n",
    "\n",
    "    kfold = KFold(**kfold_kwargs)\n",
    "    df_pred = pd.DataFrame(columns=['pred', 'true'], index=np.arange(X.shape[0]))\n",
    "\n",
    "    for train_idxs, test_idxs in kfold.split(X):\n",
    "        X_train, y_train = X[train_idxs], y[train_idxs]\n",
    "        X_test, y_test = X[test_idxs], y[test_idxs]\n",
    "    \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        df_pred.loc[test_idxs, 'true'] = y_test\n",
    "        df_pred.loc[test_idxs, 'pred'] = model.predict(X_test)\n",
    "        \n",
    "    df_pred = df_pred.sort_index()\n",
    "    \n",
    "    if index is not None:\n",
    "        assert len(index) == df_pred.shape[0], 'The passed index must be the same length as X and y'\n",
    "        df_pred.index = index\n",
    "        \n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = generate_kfold_preds(X, y)\n",
    "\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-daily",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def evaluate_models(X, y, models, post_pred_proc_func=None, index=None):\n",
    "    model_scores = dict()\n",
    "\n",
    "    for model_name, model in track(models.items()):\n",
    "        df_pred = generate_kfold_preds(X, y, model, index=index)\n",
    "        \n",
    "        if post_pred_proc_func is not None:\n",
    "            df_pred['pred'] = post_pred_proc_func(df_pred['pred'])\n",
    "\n",
    "        model_scores[model_name] = {\n",
    "            'mae': mean_absolute_error(df_pred['true'], df_pred['pred']),\n",
    "            'rmse': np.sqrt(mean_squared_error(df_pred['true'], df_pred['pred']))\n",
    "        }\n",
    "\n",
    "    df_model_scores = pd.DataFrame(model_scores)\n",
    "    \n",
    "    df_model_scores.index.name = 'metric'\n",
    "    df_model_scores.columns.name = 'model'\n",
    "\n",
    "    return df_model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'std_linear': LinearRegression(),\n",
    "    'random_forest': RandomForestRegressor(),\n",
    "    'boosted': GradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "rerun_panel_temp_model = True\n",
    "model_scores_filename = 'panel_temp_interp_model_results.csv'\n",
    "\n",
    "if (rerun_panel_temp_model == True) or (model_scores_filename not in os.listdir(cache_data_dir)):\n",
    "    df_model_scores = evaluate_models(X, y, models)\n",
    "    df_model_scores.to_csv(f'{cache_data_dir}/{model_scores_filename}')\n",
    "else:\n",
    "    df_model_scores = pd.read_csv(f'{cache_data_dir}/{model_scores_filename}', index_col='metric')\n",
    "\n",
    "df_model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-poverty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = df_model_scores.T['rmse'].idxmin()\n",
    "df_pred = generate_kfold_preds(X, y, models[top_model])\n",
    "\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-breach",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_residuals = df_pred.diff(1, axis=1).dropna(axis=1).iloc[:, 0]\n",
    "\n",
    "s_residuals.plot(linewidth=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-transportation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_pred['true'], df_pred['pred'], s=1)\n",
    "\n",
    "plt.xlabel('Obervation')\n",
    "plt.ylabel('Prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-cologne",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def interpolate_missing_panel_temps(df_pv, df_weather, model=RandomForestRegressor()):\n",
    "    missing_panel_temp_dts = df_pv.index[df_pv['panel_temp_C'].isnull()]\n",
    "    \n",
    "    if len(missing_panel_temp_dts) == 0: # i.e. no missing values\n",
    "        return df_pv\n",
    "\n",
    "    df_temp_features = construct_df_temp_features(df_weather, df_pv)\n",
    "    missing_dt_X = df_temp_features.loc[missing_panel_temp_dts].drop('site_temp', axis=1).values\n",
    "    X, y = split_X_y_data(df_temp_features, 'site_temp')\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    df_pv.loc[missing_panel_temp_dts, 'panel_temp_C'] = model.predict(missing_dt_X)\n",
    "    \n",
    "    assert df_pv['panel_temp_C'].isnull().sum() == 0, 'There are still null values for the PV panel temperature'\n",
    "    \n",
    "    return df_pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pv = interpolate_missing_panel_temps(df_pv, df_weather)\n",
    "\n",
    "df_pv.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-donor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def construct_df_irradiance_features(df_weather, df_pv):\n",
    "    df_weather = df_weather.reindex(pd.date_range(df_weather.index.min(), df_weather.index.max(), freq='30T')).ffill(limit=1)\n",
    "    temp_loc_cols = df_weather.columns[df_weather.columns.str.contains('solar')]\n",
    "    \n",
    "    df_irradiance_features = (df_weather\n",
    "                              .copy()\n",
    "                              [temp_loc_cols]\n",
    "                              .assign(site_solar=df_pv['irradiance_Wm-2'])\n",
    "                              .pipe(lambda df: df.assign(hour=df.index.hour + df.index.minute/60))\n",
    "                             )\n",
    "\n",
    "    df_irradiance_features = df_irradiance_features.sort_index(axis=1)\n",
    "\n",
    "    return df_irradiance_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_irradiance_features = construct_df_irradiance_features(df_weather, df_pv)\n",
    "\n",
    "df_irradiance_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-jimmy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'std_linear': LinearRegression(),\n",
    "    'random_forest': RandomForestRegressor(),\n",
    "    'boosted': GradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "rerun_site_irradiance_model = True\n",
    "model_scores_filename = 'site_irradiance_interp_model_results.csv'\n",
    "\n",
    "X, y = split_X_y_data(df_irradiance_features, 'site_solar')\n",
    "\n",
    "if (rerun_site_irradiance_model == True) or (model_scores_filename not in os.listdir(cache_data_dir)):\n",
    "    df_model_scores = evaluate_models(X, y, models)\n",
    "    df_model_scores.to_csv(f'{cache_data_dir}/{model_scores_filename}')\n",
    "else:\n",
    "    df_model_scores = pd.read_csv(f'{cache_data_dir}/{model_scores_filename}', index_col='metric')\n",
    "\n",
    "df_model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-scholar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = df_model_scores.T['rmse'].idxmin()\n",
    "df_pred = generate_kfold_preds(X, y, models[top_model])\n",
    "\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-nurse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_pred['true'], df_pred['pred'], s=1)\n",
    "\n",
    "plt.xlabel('Obervation')\n",
    "plt.ylabel('Prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-creature",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-occasions",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def interpolate_missing_site_irradiance(df_pv, df_weather, model=RandomForestRegressor()):\n",
    "    missing_site_irradiance_dts = df_pv.index[df_pv['irradiance_Wm-2'].isnull()]\n",
    "    \n",
    "    if len(missing_site_irradiance_dts) == 0: # i.e. no missing values\n",
    "        return df_pv\n",
    "\n",
    "    df_irradiance_features = construct_df_irradiance_features(df_weather, df_pv)\n",
    "    missing_dt_X = df_irradiance_features.loc[missing_site_irradiance_dts].drop('site_solar', axis=1).values\n",
    "    X, y = split_X_y_data(df_irradiance_features, 'site_solar')\n",
    " \n",
    "    model.fit(X, y)\n",
    "    df_pv.loc[missing_site_irradiance_dts, 'irradiance_Wm-2'] = model.predict(missing_dt_X)\n",
    "    \n",
    "    assert df_pv['irradiance_Wm-2'].isnull().sum() == 0, 'There are still null values for the solar site irradiance'\n",
    "    \n",
    "    return df_pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pv = interpolate_missing_site_irradiance(df_pv, df_weather)\n",
    "\n",
    "df_pv.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-newton",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now that we have the irradiance and temperature we're ready to start filling in the missing values for power output, again using the same regression interpolation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def construct_df_power_features(df_pv):\n",
    "    df_power_features = (df_pv\n",
    "                         .pipe(lambda df: df.assign(hour=df.index.hour + df.index.minute/60))\n",
    "                         .sort_index(axis=1)\n",
    "                        )\n",
    "\n",
    "    return df_power_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power_features = construct_df_power_features(df_pv)\n",
    "\n",
    "df_power_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'std_linear': LinearRegression(),\n",
    "    'random_forest': RandomForestRegressor(),\n",
    "    'boosted': GradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "rerun_site_power_model = True\n",
    "model_scores_filename = 'site_power_interp_model_results.csv'\n",
    "\n",
    "X, y, dates = split_X_y_data_with_index(df_power_features, 'pv_power_mw')\n",
    "\n",
    "if (rerun_site_power_model == True) or (model_scores_filename not in os.listdir(cache_data_dir)):\n",
    "    df_model_scores = evaluate_models(X, y, models)\n",
    "    df_model_scores.to_csv(f'{cache_data_dir}/{model_scores_filename}')\n",
    "else:\n",
    "    df_model_scores = pd.read_csv(f'{cache_data_dir}/{model_scores_filename}', index_col='metric')\n",
    "\n",
    "df_model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = df_model_scores.T['rmse'].idxmin()\n",
    "df_pred = generate_kfold_preds(X, y, models[top_model])\n",
    "\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_pred['true'], df_pred['pred'], s=1)\n",
    "\n",
    "plt.xlabel('Obervation')\n",
    "plt.ylabel('Prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-technician",
   "metadata": {},
   "source": [
    "##### Anomalous data points in PV data\n",
    "\n",
    "The PV data shows a number of points where the observed value is 0 but the prediction is much higher. \n",
    "\n",
    "First let's try and identify them (setting the tolerance to be lower will capture more values as anomalous). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_anomalies_pv(df_pred, tolerance=0.1):\n",
    "    foo = df_pred.copy()\n",
    "    foo['difference'] = foo.pred - foo.true\n",
    "    foo = foo[(foo.difference > tolerance) & (foo.true == 0)]\n",
    "    return foo.index\n",
    "\n",
    "anomalous_dates = dates[identify_anomalies_pv(df_pred)]\n",
    "anomalous_df = df_power_features[df_power_features.index.isin(anomalous_dates)]\n",
    "plt.hist(anomalous_df.hour) # Check this histogram to eyeball if any unreasonable anomalous values are caught by the tolerance (e.g. late at night)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-visit",
   "metadata": {},
   "source": [
    "Replace these values in `df_power_features`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power_features_clean = df_power_features.copy()\n",
    "df_power_features_clean.loc[df_power_features_clean.index.isin(anomalous_dates), 'pv_power_mw'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-shooting",
   "metadata": {},
   "source": [
    "Rerun the previous model fitting and check the pred vs. actual graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'std_linear': LinearRegression(),\n",
    "    'random_forest': RandomForestRegressor(),\n",
    "    'boosted': GradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "rerun_site_power_model = True\n",
    "model_scores_filename = 'site_power_interp_clean_model_results.csv'\n",
    "\n",
    "X, y, dates = split_X_y_data_with_index(df_power_features_clean, 'pv_power_mw')\n",
    "\n",
    "if (rerun_site_power_model == True) or (model_scores_filename not in os.listdir(cache_data_dir)):\n",
    "    df_model_scores = evaluate_models(X, y, models)\n",
    "    df_model_scores.to_csv(f'{cache_data_dir}/{model_scores_filename}')\n",
    "else:\n",
    "    df_model_scores = pd.read_csv(f'{cache_data_dir}/{model_scores_filename}', index_col='metric')\n",
    "\n",
    "top_model = df_model_scores.T['rmse'].idxmin()\n",
    "df_pred = generate_kfold_preds(X, y, models[top_model])\n",
    "\n",
    "plt.scatter(df_pred['true'], df_pred['pred'], s=1)\n",
    "\n",
    "plt.xlabel('Obervation')\n",
    "plt.ylabel('Prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-quebec",
   "metadata": {},
   "source": [
    "The above graph looks to be a cleaner with tolerance at 0.1. It looks like there might still be some which aren't though. Consider lowering the tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def pv_anomalies_to_nan(df_pv, model=GradientBoostingRegressor(), tolerance=0.1):\n",
    "    \"\"\"\n",
    "    Run this function to identify places where predicted values for pv_power_mw are much larger\n",
    "    than true values and where the true value is 0 (we expect these are anomalies) and make these values nan.\n",
    "\n",
    "    \"\"\"\n",
    "    df_power_features = construct_df_power_features(df_pv)\n",
    "    \n",
    "    X, y, dates = split_X_y_data_with_index(df_power_features, 'pv_power_mw')\n",
    "    df_pred = generate_kfold_preds(X, y, model)\n",
    "    df_pred['difference'] = df_pred.pred - df_pred.true\n",
    "    df_pred['datetime'] = dates\n",
    "    df_pred = df_pred.set_index('datetime')\n",
    "    \n",
    "    anomalous_idx = df_pred[(df_pred.difference > tolerance) & (df_pred.true == 0)].index    \n",
    "    \n",
    "    df_pv.loc[df_pv.index.isin(anomalous_idx), 'pv_power_mw'] = np.nan\n",
    "        \n",
    "    return df_pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pv = pv_anomalies_to_nan(df_pv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-plain",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def interpolate_missing_site_power(df_pv, model=RandomForestRegressor()):\n",
    "    missing_site_power_dts = df_pv.index[df_pv['pv_power_mw'].isnull()]\n",
    "    \n",
    "    if len(missing_site_power_dts) == 0: # i.e. no missing values\n",
    "        return df_pv\n",
    "\n",
    "    df_power_features = construct_df_power_features(df_pv)\n",
    "    missing_dt_X = df_power_features.loc[missing_site_power_dts].drop('pv_power_mw', axis=1).values\n",
    "    X, y = split_X_y_data(df_power_features, 'pv_power_mw')\n",
    " \n",
    "    model.fit(X, y)\n",
    "    df_pv.loc[missing_site_power_dts, 'pv_power_mw'] = model.predict(missing_dt_X)\n",
    "    \n",
    "    assert df_pv['pv_power_mw'].isnull().sum() == 0, 'There are still null values for the solar site power'\n",
    "    \n",
    "    return df_pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pv = interpolate_missing_site_power(df_pv)\n",
    "\n",
    "df_pv.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-thirty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def interpolate_missing_weather_solar(df_pv, df_weather, weather_col='solar_location2', model=RandomForestRegressor()):\n",
    "    missing_weather_solar_dts = df_weather.index[df_weather[weather_col].isnull()]\n",
    "    \n",
    "    if len(missing_weather_solar_dts) == 0: # i.e. no missing values\n",
    "        return df_pv\n",
    "\n",
    "    df_irradiance_features = construct_df_irradiance_features(df_weather, df_pv).drop('site_solar', axis=1)\n",
    "    missing_dt_X = df_irradiance_features.loc[missing_weather_solar_dts].drop(weather_col, axis=1).values\n",
    "    X, y = split_X_y_data(df_irradiance_features, weather_col)\n",
    " \n",
    "    model.fit(X, y)\n",
    "    df_weather.loc[missing_weather_solar_dts, weather_col] = model.predict(missing_dt_X)\n",
    "    \n",
    "    assert df_weather[weather_col].isnull().sum() == 0, 'There are still null values for the weather dataset solar observations'\n",
    "    \n",
    "    return df_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = interpolate_missing_weather_solar(df_pv, df_weather, model=LinearRegression())\n",
    "\n",
    "df_weather.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-graduation",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Interpolate Missing Temp Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def interpolate_missing_temps(df_weather, temp_variable, model=RandomForestRegressor()):\n",
    "    \"\"\"\n",
    "    Use the other temperature locations to predict missing values of `temp_variable`. \n",
    "    \n",
    "    For test_2: a full day is missing for temp_location4 on 2018-11-18\n",
    "    \"\"\"    \n",
    "    missing_temp_dts = df_weather.index[df_weather[temp_variable].isnull()]\n",
    "        \n",
    "    if len(missing_temp_dts) == 0: # i.e. no missing values\n",
    "        return df_weather\n",
    "    \n",
    "    temp_loc_cols = df_weather.columns[df_weather.columns.str.contains('temp')]\n",
    "\n",
    "    df_temp_features = df_weather.filter(temp_loc_cols)\n",
    "    missing_dt_X = df_temp_features.loc[missing_temp_dts].drop(temp_variable, axis=1).values\n",
    "        \n",
    "    X, y = split_X_y_data(df_temp_features, temp_variable)\n",
    "        \n",
    "    model.fit(X, y)\n",
    "    \n",
    "    df_weather.loc[missing_temp_dts, temp_variable] = model.predict(missing_dt_X)\n",
    "    df_weather = df_weather.ffill(limit=1)\n",
    "            \n",
    "    assert df_weather[temp_variable].isnull().sum() == 0, 'There are still null values for the PV panel temperature'\n",
    "    \n",
    "    return df_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = interpolate_missing_temps(df_weather, 'temp_location4', model=LinearRegression())\n",
    "foo.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-driver",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Finally we'll export the relevant code to our `batopt` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "    \n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-falls",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
